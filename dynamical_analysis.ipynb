{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0da45c-8aa7-4fe3-94f2-503b39dda3d6",
   "metadata": {},
   "source": [
    "# Identification of population events and core neurons\n",
    "\n",
    "Here we find first population events, we quantify them, and we extract their core neurons.   \n",
    "This analysis extends (from step 4.3 on) that performed by [Filipchuk et al. 2022](https://www.nature.com/articles/s41593-022-01168-5):    \n",
    "\n",
    "1. Compute population binned firing rate (bin)    \n",
    "1.1 firing rate     \n",
    "1.2 smooth it, and find the baseline\n",
    "\n",
    "2. Establish significance threshold for population events    \n",
    "2.1 compute Inter-Spike Intervals (ISI) of the original spiketrains    \n",
    "2.2 reshuffle ISI to create (100 or 1000) surrogates    \n",
    "2.3 compute the population instantaneous firing rate for each surrogate time-binned rasterplot    \n",
    "\n",
    "3. Find population events    \n",
    "3.1 instantaneous threshold is the 99% of the surrogate population instantaneous firing rate    \n",
    "3.2 smoothed firing rate    \n",
    "3.3 the peaks above intersections of smoothed fr and threshold mark population events    \n",
    "3.4 the minima before and after a peak are taken as start and end times of the population event    \n",
    "\n",
    "4. Find clusters of events    \n",
    "4.1 produce a cell id signature vector of each population event    \n",
    "4.2 perform clustering linkage by complete cross-correlation of event vectors    \n",
    "4.3 produce surrogates clusters to establish a cluster significance threshold (varying between 60% and 99%)    \n",
    "4.4 find the event reproducibility within each cluster (cluster events cross-correlation)     \n",
    "4.5 check stimulus specificity of reproducible events\n",
    "\n",
    "5. Find core neurons\n",
    "5.1 take all neurons participating to a cluster of events    \n",
    "5.2 use the a percentage (from 60 to 99%) of the cluster event reproducibility as core significance threshold    \n",
    "5.3 if the occurrence frequency of a neuron is beyond threshold, then the neuron is taken as core    \n",
    "5.4 remove core neurons if firing unspecifically within and outside their cluster    \n",
    "\n",
    "All analyses, surrogates, and figures will be saved in the corresponding experiment folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f75df-27fb-4cd6-a336-323fedaa6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123456)\n",
    "np.random.seed(123456)\n",
    "\n",
    "# defaults\n",
    "plt.rcParams['image.cmap'] = 'viridis' # works in old and newer versions of matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e90e58-70cc-45bd-b9e3-fdeb49f2b57f",
   "metadata": {},
   "source": [
    "## 1. Compute population binned firing rate\n",
    "##### 1.1 Firing rate as in the [Neuronal Dynamics book](https://neuronaldynamics.epfl.ch/online/Ch7.S2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a90aea-88ab-4e35-8df0-db374f6fe099",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = firinghist(start_time, stop_time, scan_spiketrains, bin_size=frame_duration) #\n",
    "print(\"    population firing: {:1.2f}±{:1.2f} sp/frame\".format(np.mean(fr),np.std(fr)) )\n",
    "# print(\"    frames:\",len(fr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095c798-c7ea-43fb-8bda-cef6962c1698",
   "metadata": {},
   "source": [
    "##### 1.2 smoothing the firing rate and finding the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbc9a9-7f9a-44d3-8a3c-c003edeaa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed firing rate: Savitzky-Golay 1D filter\n",
    "smoothed_fr = savgol_filter(fr, window_length=7, polyorder=3) # as in Filipchuk\n",
    "smoothed_fr[smoothed_fr<0.] = 0. # filtering can bring the firing fit below zero\n",
    "\n",
    "# Baseline: Eilers-Boelens 1D filter\n",
    "baseline_fr = baseline(smoothed_fr, l=10**8, p=0.01) # as in Filipchuk\n",
    "\n",
    "# print(len(smoothed_fr))\n",
    "# # print(video_timestamps[exp_istart:].shape) # (53046,)\n",
    "# # print(len(np.arange(exp_tstart, video_timestamps[-1], 0.025)))\n",
    "# # print((signal.decimate(video_timestamps[(np.abs(video_timestamps - exp_tstart)).argmin():], 2)).shape)\n",
    "# print(len(motSVD_1c))\n",
    "# motSVD_1clong = np.interp(np.linspace(0,len(motSVD_1c[exp_istart:]),len(smoothed_fr)), np.arange(len(motSVD_1c[exp_istart:])), motSVD_1c[exp_istart:])\n",
    "# print(motSVD_1clong.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2256e-4282-48b8-a089-34db3b51d4a1",
   "metadata": {},
   "source": [
    "plotting firing rate, smoothed, and baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082b2e5-b408-46e3-b8e0-b8f8376c27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(fr, linewidth=0.5, zorder=1)\n",
    "plt.plot(smoothed_fr, linewidth=0.5, zorder=1)\n",
    "plt.plot(baseline_fr, linewidth=0.5, zorder=2)\n",
    "plt.title(\"mean rate: %.2f (%.2f) sp/frame\" % (fr.mean(), fr.std()) )\n",
    "plt.ylabel('Firing (sp/frame)')\n",
    "plt.xlabel('Time (frames)')\n",
    "fig.savefig(exp_path+\"/results/firing_scan%s.svg\"%scan_id, transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ac1a3-acb0-43db-847a-0a8ec3d38fd4",
   "metadata": {},
   "source": [
    "## 2. Establish significance threshold for population events\n",
    "\n",
    "##### 2.1 compute Inter-Spike Intervals (ISI) of the original spiketrains \n",
    "and take the firing statistics of each cell (for later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb622239-6d82-4ccf-a206-51c45411406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spiketrainsISI = []\n",
    "cells_cv = []\n",
    "cells_firing_rate = []\n",
    "for st in scan_spiketrains:\n",
    "    cells_firing_rate.append( firinghist_one(st, np.arange(start_time, stop_time, frame_duration) ) ) # cell firing rate\n",
    "    spiketrainsISI.append( np.diff( st ) )\n",
    "print(\"    cells firing rate: {:1.2f}±{:1.2f} sp/s\".format(np.mean(cells_firing_rate),np.std(cells_firing_rate)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325cfae3-7f4b-47ed-b67d-015196cbdb2d",
   "metadata": {},
   "source": [
    "##### 2.2 reshuffle ISI to create (100 or 1000) surrogates    \n",
    "and\n",
    "##### 2.3 compute the population instantaneous firing rate for each surrogate time-binned rasterplot    \n",
    "Then we (load, if already done, or) generate a number of surrogate firing rates.    \n",
    "To respect the statistics of each experiment and population, this is done by taking the actual inter-spike-intervals of each cell and shuffle it. The number of spikes will be the same, the intervals will be the same, but not their order.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1140ee5-b45e-4ab9-ab26-c264acc4dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... generating surrogates to establish population event threshold\")\n",
    "if os.path.exists(exp_path+\"/results/surrogate_fr_scan%s.npy\"%scan_id):\n",
    "    surrogate_fr = np.load(exp_path+\"/results/surrogate_fr_scan%s.npy\"%scan_id, allow_pickle=True)\n",
    "    print(\"... loaded surrogates\")\n",
    "else:\n",
    "    print(\"... entering reshuffling\")\n",
    "    surrogate_fr = []\n",
    "    # for isur in tqdm(range(100), total=100, desc=\"creating surrogates\"):\n",
    "    for isur in range(100):\n",
    "        # build surrogate rasterplot\n",
    "        surrogate_spiketrains = []\n",
    "        for isi in spiketrainsISI:\n",
    "            random.shuffle(isi) # in-place function\n",
    "            if len(isi): # init of timing of first spike\n",
    "                isi[0] += start_time\n",
    "            surrogate_spiketrains.append( np.cumsum(isi) )\n",
    "        # compute the population instantaneous firing rate for each surrogate timebinned rasterplot\n",
    "        surrogate_fr.append( firinghist(start_time, stop_time, surrogate_spiketrains, bin_size=frame_duration) )\n",
    "        # print(\"    performed reshuffling\",isur)\n",
    "    np.save(exp_path+\"/results/surrogate_fr_scan%s.npy\"%scan_id, surrogate_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1422ee88-1dde-44f8-805a-65647272d71b",
   "metadata": {},
   "source": [
    "## 3. Find population events    \n",
    "\n",
    "##### 3.1 instantaneous threshold is the 99% of the surrogate population instantaneous firing rate    \n",
    "\n",
    "The significance threshold is made by the 99% of the surrogate population instantaneous firing rate, plus the baseline to respect slower fluctuations.    \n",
    "Either flattening the surrogates or averaging over thier instants (keeping the 0th dimension) leads to similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91fd13-6edd-4e7a-a9c1-a031ebcfad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_threshold = np.nanpercentile(np.array(surrogate_fr), q=99) +baseline_fr # flattened (single mean value)\n",
    "# event_threshold = np.nanpercentile(np.array(surrogate_fr), q=99, axis=0) +baseline_fr # keep dimension 0 (100 surrogates)\n",
    "print(\"    event size threshold (mean):\",np.mean(event_threshold))\n",
    "del surrogate_fr\n",
    "%reset_selective -f surrogate_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be660530-6601-4d4f-93e1-b7b9662a5c25",
   "metadata": {},
   "source": [
    "##### 3.2 smoothed firing rate    \n",
    "\n",
    "The peaks (maximal extrema), above threshold, are the peaks population events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771b22b-8089-473c-b279-67707c13e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... find peaks\")\n",
    "peaks = []\n",
    "fr_mean = np.mean(smoothed_fr)\n",
    "for peak in signal.argrelextrema(smoothed_fr, np.greater)[0]:\n",
    "    if smoothed_fr[peak] < event_threshold[peak]:\n",
    "        continue # ignore peaks below mean\n",
    "    peaks.append(peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0954e6c-75a3-44df-90bf-53016a9d39dc",
   "metadata": {},
   "source": [
    "##### 3.3 the peaks above intersections of smoothed fr and threshold mark population events    \n",
    "##### 3.4 the minima before and after a peak are taken as start and end times of the population event    \n",
    "\n",
    "The minimal extrema (above or below threshold) are possible limits (beginnigs and ends) of population events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be749419-7741-4bad-b0bb-831664ef0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... find minima\")\n",
    "minima = signal.argrelextrema(smoothed_fr, np.less, order=2)[0]\n",
    "# there can be periods when the firing rate is 0, to be considered minima as well\n",
    "zerominima = np.where(smoothed_fr == 0)[0]\n",
    "minima = np.concatenate((minima,zerominima))\n",
    "minima.sort(kind='mergesort')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d820dc-736b-490b-8e9f-e64fce99ed07",
   "metadata": {},
   "source": [
    "The minimum before and after a peak are taken as start and end times of a putative population event.    \n",
    "The dict list `events` will store beginning, and end (and stimulus, if applicable) of each population event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f29b48-2272-4645-967c-ba65554492df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... find population events\")\n",
    "events = []\n",
    "# each peak (beyond threshold) is part of an event\n",
    "for peak in peaks:\n",
    "    event = {'start':0, 'end':0, 'start_frame':0, 'end_frame':0, 'stimulus':None, 'color':'gray'} # init\n",
    "    # the minima (either below or above threshold) before and after the peak are the real limits of the event\n",
    "    # index of the minimum before the peak\n",
    "    minimum_pre = minima[ np.argwhere(minima<peak)[-1][0] if len(np.argwhere(minima<peak))>0 else 0 ]\n",
    "    # start\n",
    "    event['start'] = minimum_pre\n",
    "    event['start_frame'] = minimum_pre\n",
    "    # end\n",
    "    # index of the minimum after the peak\n",
    "    mininum_post = minima[ np.argwhere(minima>peak)[0][0] if len(np.argwhere(minima>peak))>0 else 0 ]\n",
    "    event['end'] = mininum_post\n",
    "    event['end_frame'] = mininum_post\n",
    "    if event['start']>=0 and event['end']>event['start']:\n",
    "        events.append(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce32a1-7571-4f5b-b166-af8a21f58313",
   "metadata": {},
   "source": [
    "##### Processing the behavioral indicator\n",
    "The data from Stringer et al. 2019 also contains face motion energy components. The first principal component captures motion everywhere on the face and strongly correlates with arousal measures (pupil, whisking), see their Fig. 2 and S4.    \n",
    "Here, we use this component to study the temporal order of population events with respect to motion energy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba1d19-ffe6-47b8-a619-4614104e78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'video_timestamps' in locals():\n",
    "    print(\"... processing behavioral data\")\n",
    "    smoothed_beh = np.ones(len(smoothed_fr)) * -np.inf\n",
    "    for ievent,event in enumerate(events):\n",
    "        event_start_time = exp_tstart + event['start'] * frame_duration\n",
    "        event_end_time = exp_tstart + event['end'] * frame_duration\n",
    "        # to have a measure of behavior, we take the mean of the first component value of each frame during the event\n",
    "        meanbeh1c = np.mean(motSVD_1c[np.where((video_timestamps>=event_start_time)&(video_timestamps<=event_end_time))]) / 1000. # A.U. just a factor to ease plotting\n",
    "        events[ievent]['stimulus'] = meanbeh1c -2. # A.U. to be all negative\n",
    "        smoothed_beh[event['start']] = meanbeh1c -2. # A.U. to be all negative\n",
    "        \n",
    "    # pre-processing the motSVD PC1\n",
    "    smoothed_beh[smoothed_beh==-np.inf] = np.max(smoothed_beh) # default\n",
    "    # smooth motion energy curve\n",
    "    smoothed_beh = savgol_filter(smoothed_beh, window_length=3, polyorder=2)    \n",
    "    # Find times of changes in motion energy\n",
    "    smoothed_diff = np.diff(smoothed_beh)\n",
    "    smoothed_beh_indices = list(np.where(smoothed_diff < np.percentile(smoothed_diff, q=1))[0])\n",
    "    \n",
    "    # Count the number of events before and after the change.\n",
    "    # cycle over series of intervals, to test dependence on interval\n",
    "    pre_beh_events = [[] for i in range(6)] # intervals event idx\n",
    "    post_beh_events = [[] for i in range(6)]\n",
    "    Npre_beh_events = [0 for i in range(6)] # intervals event count\n",
    "    Npost_beh_events = [0 for i in range(6)] \n",
    "    beh_intervals = [0.050, 0.100, 0.150, 0.200, 0.250, 0.300]\n",
    "    for iint,interval in enumerate(beh_intervals):\n",
    "        print(\"    interval\", interval)\n",
    "        for sni in smoothed_beh_indices:\n",
    "            snitime = exp_tstart + sni * frame_duration\n",
    "            snitime_pre = snitime - interval # s\n",
    "            snitime_post = snitime + interval # s\n",
    "            for ievent,event in enumerate(events):\n",
    "                event_start_time = exp_tstart + event['start'] * frame_duration\n",
    "                if snitime_pre < event_start_time and event_start_time < snitime:\n",
    "                    pre_beh_events[iint].append(ievent)\n",
    "                    Npre_beh_events[iint] += 1\n",
    "                if snitime < event_start_time and event_start_time < snitime_post:\n",
    "                    post_beh_events[iint].append(ievent)\n",
    "                    Npost_beh_events[iint] += 1\n",
    "        print(\"    pre %.3f (%d/%d)\"%(Npre_beh_events[iint]/len(events), Npre_beh_events[iint], len(events)) )\n",
    "        print(\"    post %.3f (%d/%d)\"%(Npost_beh_events[iint]/len(events), Npost_beh_events[iint], len(events)) )\n",
    "    Npre_beh_events = np.array(Npre_beh_events)/len(events)\n",
    "    Npost_beh_events = np.array(Npost_beh_events)/len(events)\n",
    "    # summary\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    plt.plot(Npre_beh_events, linewidth=2.5, color='red', zorder=5)\n",
    "    plt.plot(Npost_beh_events, linewidth=2.5, color='blue', zorder=5)\n",
    "    plt.ylabel('event occurrences ratio')\n",
    "    plt.xlabel('interval (sec)')\n",
    "    plt.ylim([0,1])\n",
    "    plt.xticks(range(6),beh_intervals)\n",
    "    fig.savefig(exp_path+\"/results/eventsXmovements_scan%s.png\"%scan_id, transparent=True, dpi=900)\n",
    "    plt.close()\n",
    "    fig.clear()\n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cf967-d88e-4053-bb02-e5ff71bdb3ff",
   "metadata": {},
   "source": [
    "#### plot everything so far\n",
    "original, threshold, maxima and minima, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aaced2-cffc-4bfa-9d9c-98e23bef32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "x = np.arange(0,len(smoothed_fr))\n",
    "\n",
    "for event in events:\n",
    "    ax.axvspan(event['start'], event['end'], alpha=0.1, color='green', zorder=1)\n",
    "    \n",
    "    if 'video_timestamps' in locals():\n",
    "        ax.plot(event['start'], event['stimulus'], 'co', markersize=1, markeredgewidth=0., zorder=6)\n",
    "        \n",
    "if 'video_timestamps' in locals():\n",
    "    plt.plot(smoothed_beh, linewidth=0.5, color='b', zorder=5)\n",
    "    for sni in smoothed_beh_indices:\n",
    "        ax.plot(sni, -1.5, 'yo', markersize=1, markeredgewidth=0., zorder=6)\n",
    "\n",
    "if 'orientation_interval_frames' in locals():\n",
    "    for stimframes in orientation_interval_frames[scan_id]:\n",
    "        for timeframes in stimframes:\n",
    "            ax.axvspan(timeframes[0], timeframes[1], alpha=0.1, linewidth=0, facecolor='grey', zorder=1)\n",
    "            \n",
    "ax.plot(smoothed_fr,linewidth=0.5, color='k', zorder=2)\n",
    "ax.plot(event_threshold, linewidth=0.5, color='magenta', zorder=5)\n",
    "plt.plot(baseline_fr, linewidth=0.5, color='orange', zorder=5)\n",
    "plt.plot(baseline_fr, linewidth=0.5, color='orange', zorder=5)\n",
    "ax.plot(x[minima], smoothed_fr[minima], 'wo', markersize=1, markeredgewidth=0., zorder=4)\n",
    "ax.plot(x[peaks], smoothed_fr[peaks], 'rs', markersize=.5, zorder=4)\n",
    "fig.savefig(exp_path+\"/results/Events_population_firing_scan%s.png\"%scan_id, transparent=True, dpi=900)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d06d2-9094-4cfb-830f-442248c54a48",
   "metadata": {},
   "source": [
    "## 4. Find clusters of events    \n",
    "\n",
    "with not enough events, no point in going further ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06719a1b-d7bb-4a14-8d1f-d90cc9ffa1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(events)<4:\n",
    "    print(\"... not enough events to continue analysis (<4)\")\n",
    "#     raise StopExecution\n",
    "#     # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901dbe2-d048-4dcf-9e30-91cf3c815201",
   "metadata": {},
   "source": [
    "##### 4.1 produce a cell id signature vector of each population event   \n",
    "We store both the ids and indexes of all cells firing during the population event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd9054-9fdc-4ad8-981a-eda55bada5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... signatures of population events\")\n",
    "# get the cell ids for each event\n",
    "events_signatures = [] # N x M, N events and all M idx for each event (0s and 1s)\n",
    "events_spectrums = [] # N x Z, N events and only Z cids for each event\n",
    "events_vectors = [] # event vectors for attractor analysis\n",
    "toBremoved = []\n",
    "\n",
    "if 'exp_tstart' not in vars() or 'exp_tstart' not in globals():\n",
    "    exp_tstart = 0\n",
    "\n",
    "for event in events:\n",
    "    # signature\n",
    "    signature = [0 for i in range(len(scan_spiketrains))] # init\n",
    "    spectrum = []\n",
    "    evector = {\"color\":\"gray\", \"variables\":np.zeros(len(ophys_cell_ids))}\n",
    "    # start end of event index are converted to ms in order to search cell ids being active in that window\n",
    "    tstart = exp_tstart + event['start'] * frame_duration\n",
    "    tend = exp_tstart + event['end'] * frame_duration\n",
    "    for idx,spiketrain in enumerate(scan_spiketrains):\n",
    "        # take the idx if there are spikes within event start and end\n",
    "        spiketrain = np.array(spiketrain)\n",
    "        # choose idx based on event limits\n",
    "        if np.any(np.logical_and(spiketrain>=tstart, spiketrain<tend)):\n",
    "            spectrum.append( ophys_cell_ids[idx] ) # to store the actual cid and not just the index\n",
    "            signature[idx] = 1 #\n",
    "        evector[\"variables\"][idx] = np.mean(cells_firing_rate[idx][event['start']:event['end']])\n",
    "    # check that the event signature has more than 1 cell\n",
    "    if np.count_nonzero(spectrum)>1:\n",
    "        events_spectrums.append( spectrum )\n",
    "        events_signatures.append( signature )\n",
    "        events_vectors.append(evector)\n",
    "    else:\n",
    "        toBremoved.append(events.index(event))\n",
    "\n",
    "# removing events with just one cell\n",
    "for index in sorted(toBremoved, reverse=True):\n",
    "    del events[index]\n",
    "\n",
    "events_signatures = np.array(events_signatures)\n",
    "events_spectrums = np.array(events_spectrums, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c13aee-342d-49b1-bbff-1bd57fb6faa6",
   "metadata": {},
   "source": [
    "#### Some basic population event statistics\n",
    "Number of events per second, event duration, and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f617e9-6ebe-4355-b0c2-35336e69e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"    number of events:\",len(events))\n",
    "events_sec = len(events_signatures)/stop_time\n",
    "print(\"    number of events per sec:\",events_sec)\n",
    "\n",
    "# events durations and intervals\n",
    "events_durations = []\n",
    "events_intervals = []\n",
    "last_event = None\n",
    "for event in events:\n",
    "    events_durations.append(event['end']-event['start'])\n",
    "    if last_event: # only from the second event on\n",
    "        events_intervals.append(event['start']-last_event['end'])\n",
    "    last_event = event\n",
    "\n",
    "# Population events mean+std Duration\n",
    "events_durations_f = np.array(events_durations, dtype=float)\n",
    "events_durations_f = events_durations_f*frame_duration\n",
    "np.save(exp_path+\"/results/events_durations%s.npy\"%scan_id, events_durations_f)\n",
    "print(\"    events duration: %.3f±%.3f\" % (np.median(events_durations_f), np.std(events_durations_f)))\n",
    "fig = plt.figure()\n",
    "plt.yscale('log')\n",
    "hn,hx,_ = plt.hist(events_durations_f, bins='auto')\n",
    "lims = plt.ylim()\n",
    "plt.vlines([np.median(events_durations_f)], ymin=lims[0], ymax=lims[1], linestyles='dashed', colors='k')\n",
    "plt.title(\"median events duration: %.3f±%.3f\" % (np.median(events_durations_f), np.std(events_durations_f)) )\n",
    "plt.ylabel('event occurrences')\n",
    "plt.xlabel('event duration (sec)')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "fig.savefig(exp_path+\"/results/Events_durations_scan%s.svg\"%scan_id, transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "\n",
    "# Population events size (number of cells per event)\n",
    "events_size = []\n",
    "for esignature in events_signatures:\n",
    "    events_size.append(len(np.nonzero(esignature)[0]))\n",
    "np.save(exp_path+\"/results/events_size%s.npy\"%scan_id, events_size)\n",
    "print(\"    events size: %.3f±%.3f\" % (np.median(events_size), np.std(events_size)))\n",
    "fig = plt.figure()\n",
    "hn,hx,_ = plt.hist(events_size, bins='auto')\n",
    "lims = plt.ylim()\n",
    "plt.vlines([np.median(events_size)], ymin=lims[0], ymax=lims[1], linestyles='dashed', colors='k')\n",
    "plt.title(\"median size: %.3f (%.3f)\" % (np.median(events_size), np.std(events_size)) )\n",
    "plt.ylabel('occurrences')\n",
    "plt.xlabel('number of cells per event')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "fig.savefig(exp_path+\"/results/Events_size_scan%s.svg\"%scan_id, transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016f351-e19e-4adb-b260-a76278a80d9e",
   "metadata": {},
   "source": [
    "#### 4.2 perform clustering linkage by complete cross-correlation of event vectors  \n",
    "\n",
    "We compute the Pearson's correlation matrix over events signatures (the patterns of firing cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d29b1-89a3-4d54-9ae5-17a5c1fc8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... Similarity of events matrix\")\n",
    "SimilarityMap = np.corrcoef(events_signatures)\n",
    "fig = plt.figure()\n",
    "plt.pcolormesh(SimilarityMap)\n",
    "cbar = plt.colorbar()\n",
    "fig.savefig(exp_path+\"/results/Events_CorrMatrix%s.png\"%scan_id, dpi=600, transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a494266-7823-4d57-b9cf-e411720f11e6",
   "metadata": {},
   "source": [
    "Then we perform the clustering linkage by complete cross-correlation of event signatures.    \n",
    "The cut-off is arbitrary, but different cut-offs lead to similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d1ba5-45dd-4869-a370-1bf8042c8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... clustering\")\n",
    "print(\"    linkage\")\n",
    "Z = linkage(events_signatures, method='complete', metric='correlation') #\n",
    "Z[ Z<0 ] = 0 # for very low correlations, negative values can result\n",
    "cut_off = 0.7*max(Z[:,2]) # generic cutoff as in matlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a765542-c70f-444a-a86a-0b3d7a850b87",
   "metadata": {},
   "source": [
    "#### 4.3 produce surrogates clusters to establish a cluster significance threshold    \n",
    "The threshold for cluster significance is established by generating 100 surrogate events_signatures, choosing at random signatures form the population of cells.   \n",
    "Then we correlate and cluster as above... there will be clusters, happening just by chance due to the finite number of cells, but their internal correlation should not be high.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2f4b6-d8ff-457f-8769-052bd6ce999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"    surrogate events signatures for clustering threshold\")\n",
    "if os.path.exists(exp_path+\"/results/surrogate_reproducibility_list%s.npy\"%scan_id):\n",
    "    surrogate_reproducibility_list = np.load(exp_path+\"/results/surrogate_reproducibility_list%s.npy\"%scan_id, allow_pickle=True)\n",
    "    print(\"... loaded surrogates\")\n",
    "else:\n",
    "    print(\"... entering creation of surrogate signatures\")    \n",
    "    surrogate_reproducibility_list = []\n",
    "    for csur in range(100):\n",
    "        surrogate_events_signatures = []\n",
    "        for evsig in events_signatures:\n",
    "            surrogate_signature = np.array([0 for i in ophys_cell_indexes]) # init\n",
    "            surrogate_signature[ np.random.choice(ophys_cell_indexes, size=np.count_nonzero(evsig), replace=False) ] = 1\n",
    "            surrogate_events_signatures.append(surrogate_signature.tolist())\n",
    "        # similarity\n",
    "        surrogate_similaritymap = np.corrcoef(surrogate_events_signatures)\n",
    "        # clustering\n",
    "        surrogate_Z = linkage(surrogate_events_signatures, method='complete', metric='correlation') #\n",
    "        surrogate_Z[ surrogate_Z<0 ] = 0 # for very low correlations, negative values can result\n",
    "        surrogate_events_assignments = fcluster(surrogate_Z, t=cut_off, criterion='distance')\n",
    "        # sorting by cluster based on the first element of zip\n",
    "        surrogate_permutation = [x for _, x in sorted(zip(surrogate_events_assignments, range(len(surrogate_events_signatures))))]\n",
    "        clustered_surrogate_similaritymap = surrogate_similaritymap[surrogate_permutation] # x\n",
    "        clustered_surrogate_similaritymap = clustered_surrogate_similaritymap[:,surrogate_permutation] # y\n",
    "        # cluster reproducibility\n",
    "        surrogate_events_cluster_sequence = sorted(surrogate_events_assignments) # [ 1 1 1 1 2 2 3 3 3 3 3 ...]\n",
    "        surrogate_cnt = collections.Counter()\n",
    "        for word in surrogate_events_cluster_sequence:\n",
    "            surrogate_cnt[word] += 1\n",
    "        surrogate_events_cluster_chunks = list(surrogate_cnt.values())\n",
    "        starti = 0\n",
    "        for iblock,nblock in enumerate(surrogate_events_cluster_chunks):\n",
    "            endi = starti+nblock\n",
    "            # get sub-array\n",
    "            surrogate_cluster_subarray = np.array(clustered_surrogate_similaritymap[ starti:endi-1, starti:endi-1 ])\n",
    "            if surrogate_cluster_subarray.size:\n",
    "                np.fill_diagonal(surrogate_cluster_subarray, 0.0)\n",
    "                # compute the subarray average\n",
    "                surrogate_reproducibility_list.append( np.nanmean(surrogate_cluster_subarray) )\n",
    "            starti = endi\n",
    "    np.save(exp_path+\"/results/surrogate_reproducibility_list%s.npy\"%scan_id, surrogate_reproducibility_list)\n",
    "    \n",
    "    del surrogate_events_signatures\n",
    "    %reset_selective -f surrogate_events_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133aa8d-cb53-4c12-8ee1-46e5210a278b",
   "metadata": {},
   "source": [
    "The 95% (or more) correlation of this random cluster will be the threshold for a cluster to be significantly correlated.     \n",
    "A minimum of 2 events is required for the size of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57725a84-bbc1-4bd5-becc-31458774cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_reproducibility_threshold = np.percentile(np.array(surrogate_reproducibility_list), 5)\n",
    "print(\"    cluster reproducibility threshold:\",cluster_reproducibility_threshold)\n",
    "# number of events in a cluster, even small clusters as long as they pass the reproducibility threshold\n",
    "cluster_size_threshold = 2 # minimum requirement\n",
    "print(\"    cluster size threshold:\",cluster_size_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72cd58-74c8-4d83-824a-9d62602e692a",
   "metadata": {},
   "source": [
    "#### 4.4 find the event reproducibility within each cluster (cluster events cross-correlation)    \n",
    "\n",
    "The events are rearranged in the correlation map to put together those that are similar.     \n",
    "The clusters are identified by a (randomly chosen) color that will be kept for the following analyses.    \n",
    "Gray is the color of clusters below reproducibility threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ae78e-a0a4-4826-b001-1f8720e00883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters\n",
    "events_assignments = fcluster(Z, t=cut_off, criterion='distance')\n",
    "# print(events_assignments)\n",
    "# print(len(events_assignments))\n",
    "# ex. [4 4 4 4 2 2 34 4 7 7 7 7 5 5 5 5 4 ... ]\n",
    "\n",
    "# count clusters\n",
    "nevents_clusters = np.unique(events_assignments, return_counts=True)[1]\n",
    "# print(\"    events/clusters:\", nevents_clusters)\n",
    "# ex. [ 39  19  11  70  15  74  45  13  10  45 ...]\n",
    "nclusters = len(nevents_clusters)\n",
    "print(\"    Total number of clusters:\",nclusters)\n",
    "\n",
    "# color map of the clustered events\n",
    "cmap = mpcm.get_cmap('rainbow')\n",
    "cluster_color_array = [mpl.colors.rgb2hex(rgba) for rgba in cmap(np.linspace(0.0, 1.0, nclusters))]\n",
    "random.shuffle(cluster_color_array) # to have different nearby colors\n",
    "cluster_color_array = np.array(cluster_color_array)\n",
    "# print(\"cluster colors:\",len(cluster_color_array))\n",
    "\n",
    "threshold_map = nevents_clusters < cluster_size_threshold\n",
    "# print(\"    removing below size threshold clusters:\", np.count_nonzero(threshold_map))\n",
    "cluster_color_array[threshold_map] = 'gray' # or 'none'\n",
    "\n",
    "color_array = []\n",
    "for cluidx in events_assignments:\n",
    "    color_array.append(cluster_color_array[cluidx-1]) # fcluster returns numeric labels from 0 to nclusters-1\n",
    "color_array = np.array(color_array)\n",
    "# ex. color_array = ['#304030', '#008c85', '#008c85', '#005955', '#2db3ac', ...]\n",
    "\n",
    "# sorting the index of events_signatures based on events_assignments (the first element of zip)\n",
    "permutation = [x for _,x in sorted(zip(events_assignments, range(len(events_signatures))))]\n",
    "# permuting all arrays used after\n",
    "clustered_signatures = events_signatures[permutation]\n",
    "clustered_spectrums = events_spectrums[permutation]\n",
    "clustered_event_colors = color_array[permutation]\n",
    "events = np.array(events)\n",
    "clustered_events = events[permutation]\n",
    "np.save(exp_path+\"/results/clustered_signatures%s.npy\"%scan_id, clustered_signatures)\n",
    "\n",
    "# reordering SimilarityMap\n",
    "clustered_SimilarityMap = SimilarityMap[permutation] # x\n",
    "clustered_SimilarityMap = clustered_SimilarityMap[:,permutation] # y\n",
    "\n",
    "# Pattern reproducibility - Cluster self-similarity\n",
    "remove_colors = [cola for cola in color_array]\n",
    "reproducibility_list = [ 0. for elc in cluster_color_array ]\n",
    "core_reproducibility = {elc:1. for elc in cluster_color_array}\n",
    "events_cluster_sequence = sorted(events_assignments) # [ 1 1 1 1 2 2 3 3 3 3 3 ...]\n",
    "# subarray iteration\n",
    "ec_cnt = collections.Counter()\n",
    "for word in events_cluster_sequence:\n",
    "    ec_cnt[word] += 1\n",
    "events_cluster_chunks = list(ec_cnt.values())\n",
    "starti = 0\n",
    "for iblock,nblock in enumerate(events_cluster_chunks):\n",
    "    endi = starti+nblock\n",
    "    cluster_subarray = np.array(clustered_SimilarityMap[ starti:endi-1, starti:endi-1 ])\n",
    "    if cluster_subarray.size:\n",
    "        np.fill_diagonal(cluster_subarray, 0.0)\n",
    "        # compute the subarray average\n",
    "        reproducibility_list[iblock] = np.nanmean(cluster_subarray)\n",
    "        # overwrite color\n",
    "        if np.nanmean(cluster_subarray) < cluster_reproducibility_threshold:\n",
    "            cluster_color_array[iblock] = \"gray\"\n",
    "            clustered_event_colors[starti:endi] = \"gray\"\n",
    "        else:\n",
    "            # Stimulus-free method to detect core neurons:\n",
    "            # within each cluster of events,\n",
    "            # cores are those participating to more than 99% of cluster events\n",
    "            core_reproducibility[cluster_color_array[iblock]] = np.percentile(cluster_subarray, core_reproducibility_perc)\n",
    "    starti = endi\n",
    "print(\"    # clusters (after removing those below reproducibility threshold):\", len(cluster_color_array) - collections.Counter(cluster_color_array)['gray'])\n",
    "\n",
    "# Remove colors to be gray-ed in the event color assignement\n",
    "events_color_assignments = [col if col in cluster_color_array else \"gray\" for col in color_array]\n",
    "# print(events_color_assignments)\n",
    "for evtidx,_ in enumerate(events):\n",
    "    events[evtidx]['color'] = events_color_assignments[evtidx]\n",
    "\n",
    "# save total clusters with their events\n",
    "clustercolors,eventcounts = np.unique(events_color_assignments, return_counts=True) \n",
    "if 'orientation_interval_frames' in locals():\n",
    "    global_tot_clusters[scan_id].append( clustercolors.tolist() )\n",
    "    global_tot_clusters[scan_id].append( eventcounts.tolist() )\n",
    "\n",
    "# plot all\n",
    "fig, ax = plt.subplots()\n",
    "plt.pcolormesh(clustered_SimilarityMap)\n",
    "# loop over cluster colors\n",
    "clcoord = 0\n",
    "for csize,ccolor,reproval in zip(nevents_clusters,cluster_color_array,reproducibility_list):\n",
    "    if ccolor!=\"gray\":\n",
    "        rect = patches.Rectangle((clcoord, clcoord), csize, csize, linewidth=1., edgecolor=ccolor, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        # ax.text(clcoord+1, clcoord+1, \"{:1.2f}\".format(reproval), color=ccolor, fontsize=2)\n",
    "    clcoord += csize\n",
    "cbar = plt.colorbar()\n",
    "cbar.outline.set_visible(False)\n",
    "cbar.set_ticks([]) # remove all ticks\n",
    "for spine in plt.gca().spines.values(): # get rid of the frame\n",
    "    spine.set_visible(False)\n",
    "plt.xticks([]) # remove all ticks\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "fig.savefig(exp_path+\"/results/Events_CorrClustered_scan%s.png\"%scan_id, transparent=True, dpi=600)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n",
    "# Pattern reproducibility by Cluster\n",
    "fig = plt.figure()\n",
    "for repri, (reprv, reprc) in enumerate(zip(reproducibility_list,cluster_color_array)):\n",
    "    plt.bar(repri, reprv, 1., facecolor=reprc)\n",
    "plt.title('Pattern reproducibility')\n",
    "plt.ylabel('Auto-correlation')\n",
    "plt.xlabel('Clusters')\n",
    "fig.savefig(exp_path+\"/results/Pattern_reproducibility_scan%s.png\"%scan_id, transparent=True, dpi=600)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "\n",
    "# Self-similarity index\n",
    "fig = plt.figure()\n",
    "hn,hx,_ = plt.hist(reproducibility_list, bins='auto')\n",
    "bin_centers = 0.5*(hx[1:]+hx[:-1])\n",
    "plt.plot(bin_centers,hn) ## using bin_centers rather than edges\n",
    "lims = plt.ylim()\n",
    "plt.vlines([np.mean(reproducibility_list)], ymin=lims[0], ymax=lims[1], linestyles='dashed', colors='k')\n",
    "plt.title(\"Self-Similarity Index: %.3f\" % (np.mean(reproducibility_list)))\n",
    "plt.xlabel('Auto-correlation')\n",
    "plt.ylabel('Cluster occurrences')\n",
    "fig.savefig(exp_path+'/results/Pattern_reproducibility_SelfSimilarityIndex%s.png'%scan_id, transparent=True, dpi=600)\n",
    "fig.savefig(exp_path+'/results/Pattern_reproducibility_SelfSimilarityIndex%s.svg'%scan_id, transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8e5c9-5c48-4516-9516-5d97ddff1e30",
   "metadata": {},
   "source": [
    "#### 4.5 Check stimulus specificity of reproducible events\n",
    "\n",
    "Replot spiketrains, coloring stimuli by orientation and events by cluster.     \n",
    "There are 16 orientations. Let's take 16 colors and color events by their (reproducible) cluster color.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c8649-1b1f-453d-bab9-7a126cd261e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orientation_interval_frames' in locals():\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    orientation_colors = cmap(np.arange(16))\n",
    "    # print(orientation_colors)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "\n",
    "    for oridx,stimframes in enumerate(orientation_interval_frames[scan_id]):\n",
    "        for timeframes in stimframes:\n",
    "            ax.axvspan(timeframes[0], timeframes[1], alpha=0.4, linewidth=0, facecolor=orientation_colors[oridx], zorder=1)\n",
    "\n",
    "    for evtidx,event in enumerate(events):\n",
    "        ax.axvspan(event['start'], event['end'], alpha=0.4, color=event['color'], zorder=1)\n",
    "\n",
    "    ax.plot(smoothed_fr,linewidth=0.5, color='k', zorder=2)\n",
    "    ax.plot(event_threshold, linewidth=0.5, color='magenta', zorder=3)\n",
    "    plt.plot(baseline_fr, linewidth=0.5, color='orange', zorder=3)\n",
    "    x = np.arange(0,len(smoothed_fr))\n",
    "    ax.plot(x[minima], smoothed_fr[minima], 'wo', markersize=1, markeredgewidth=0., zorder=4)\n",
    "    ax.plot(x[peaks], smoothed_fr[peaks], 'rs', markersize=.5, zorder=4)\n",
    "    fig.savefig(exp_path+\"/results/Events_population_firing_scan%s.svg\"%scan_id, transparent=True)\n",
    "    plt.close()\n",
    "    fig.clear()\n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41e577-a1ed-4bde-b16b-34fda9acc7ad",
   "metadata": {},
   "source": [
    "We need to check whether reproducible patterns are stimulus-orientation specific.   \n",
    "\n",
    "To establish a significance level, we want to know what is the chance of random events to be stimulus specific.     \n",
    "We create surrogate events (leaving the same cluster color) by jittering their timing. \n",
    "\n",
    "For each orientation (16), we store the color of the event that follows.    \n",
    "An event is considered to be triggered by a stimulus if it starts after presentation and no later than the 10 frames (~640ms) after presentation (a third of the inter-stimulus interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ff7fe-f3d6-45a8-8d5b-1bc21301fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orientation_interval_frames' in locals():\n",
    "    orientation_following_colors = [[] for i in range(16)]\n",
    "    surrogate_orientation_following_colors = [[] for i in range(16)]\n",
    "\n",
    "    for oridx,stimframes in enumerate(orientation_interval_frames[scan_id]):\n",
    "        for stimidx,sframes in enumerate(stimframes): # all but last\n",
    "\n",
    "            for event in events:\n",
    "                if event['start'] >= sframes[0] and event['start']<=sframes[0]+10:\n",
    "                    if event['color'] != \"gray\":\n",
    "                        orientation_following_colors[oridx].append(event['color'])\n",
    "\n",
    "        for isur in range(100):\n",
    "            surrogate_events = copy.deepcopy(events) \n",
    "            for evtidx,sampleevt in enumerate(surrogate_events):\n",
    "                surrogate_events[evtidx]['start'] = sampleevt['start'] + random.randint(-15, 15)\n",
    "                surrogate_events[evtidx]['end'] = sampleevt['end'] + random.randint(-15, 15)\n",
    "            for event in surrogate_events:\n",
    "                    if event['start'] >= sframes[0] and event['start']<=sframes[0]+10:\n",
    "                        if event['color'] != \"gray\":\n",
    "                            surrogate_orientation_following_colors[oridx].append(event['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d940fa-bb61-42be-9a67-0ec3ef0bdaca",
   "metadata": {},
   "source": [
    "We also store, for each orientation, which and how many reproducible patterns (clusters) happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2687d-e03d-4c5b-8ecf-01d88dfa3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orientation_interval_frames' in locals():\n",
    "    for oridx,followers in enumerate(orientation_following_colors):\n",
    "        # print(oridx)\n",
    "        # print(np.unique(followers, return_counts=True))\n",
    "        stimclusters,stimcounts = np.unique(followers, return_counts=True) # (array(['#80ffb4'], dtype='<U7'), array([2]))\n",
    "        if len(stimclusters):\n",
    "            for stcl,stcnt in zip(stimclusters,stimcounts):\n",
    "                if stcl in global_cluster_tuning[oridx].keys():\n",
    "                    global_cluster_tuning[oridx][\"%s,%s\"%(stcl,scan_id)] += stcnt\n",
    "                else:\n",
    "                    global_cluster_tuning[oridx][\"%s,%s\"%(stcl,scan_id)] = stcnt\n",
    "\n",
    "    for oridx,followers in enumerate(surrogate_orientation_following_colors):\n",
    "        # print(oridx)\n",
    "        # print(np.unique(followers, return_counts=True))\n",
    "        stimclusters,stimcounts = np.unique(followers, return_counts=True) # (array(['#80ffb4'], dtype='<U7'), array([2]))\n",
    "        if len(stimclusters):\n",
    "            for stcl,stcnt in zip(stimclusters,stimcounts):\n",
    "                if stcl in global_surrogate_cluster_tuning[oridx].keys():\n",
    "                    global_surrogate_cluster_tuning[oridx][\"%s,%s\"%(stcl,scan_id)] += stcnt\n",
    "                else:\n",
    "                    global_surrogate_cluster_tuning[oridx][\"%s,%s\"%(stcl,scan_id)] = stcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c5c88-e53b-4a40-8b2f-d45e727aed42",
   "metadata": {},
   "source": [
    "We also store the cell ids of events by cluster color for later comparison to orientation tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3db2a-ba3a-4601-adbf-d0890b239e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_clustered_spectrums = {}\n",
    "for cluster_evtcol,cluster_spectrum in zip(clustered_event_colors, clustered_spectrums):\n",
    "    if \"%s,%s\"%(cluster_evtcol,scan_id) in scan_clustered_spectrums.keys():\n",
    "        scan_clustered_spectrums[\"%s,%s\"%(cluster_evtcol,scan_id)].append( cluster_spectrum )\n",
    "    else:\n",
    "        scan_clustered_spectrums[\"%s,%s\"%(cluster_evtcol,scan_id)] = []\n",
    "        scan_clustered_spectrums[\"%s,%s\"%(cluster_evtcol,scan_id)].append( cluster_spectrum )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33d284-faa9-49b3-a94c-12cf195275bb",
   "metadata": {},
   "source": [
    "### 5. Find core neurons\n",
    "#### 5.1 take all neurons participating to a cluster of events    \n",
    "For each cluster above significance threshold, we count how many times each cell participated to the cluster's events.\n",
    "\n",
    "#### 5.2 use the a percentage of the cluster event reproducibility as core significance threshold    \n",
    "Cores identification is by frequence of occurrence (arbitrarily imposed from the outside, from 60% to 99%).\n",
    "\n",
    "#### 5.3 if the occurrence frequency of a neuron is beyond threshold, then the neuron is taken as core    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bff03f-533f-4e0f-ab8f-80bcb81c4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... finding cluster cores\")\n",
    "clusters_cores = []\n",
    "clusters_cores_by_color = {ecolor:list() for ecolor in clustered_event_colors}\n",
    "cluster_color_cores = [[] for clsp in clustered_spectrums]\n",
    "currentcl = clustered_event_colors[0]\n",
    "cluster_events_list = []\n",
    "for cl_idlist, cl_color in zip(clustered_spectrums, clustered_event_colors):\n",
    "    # when the color changes, plot the map and reset containers\n",
    "    if currentcl != cl_color:\n",
    "        # find common subset of cells in a clusters\n",
    "        cid_counter = {}\n",
    "        for event_cids in cluster_events_list:\n",
    "            for cidc in event_cids:\n",
    "                if cidc in cid_counter:\n",
    "                    cid_counter[cidc] += 1/len(cluster_events_list)\n",
    "                else: # create\n",
    "                    cid_counter[cidc] = 1/len(cluster_events_list)\n",
    "        cluster_core = []\n",
    "        for cidkey,cidprob in cid_counter.items():\n",
    "            # Cores identification, independent of stimuli\n",
    "            # A cell is considered 'core' of multiple events if it has\n",
    "            # a frequence of occurrence > core_reproducibility (%)\n",
    "            if currentcl in core_reproducibility and cidprob >= core_reproducibility[currentcl]:\n",
    "                cluster_core.append(cidkey)\n",
    "                clusters_cores_by_color[currentcl].append(cidkey)\n",
    "        if len(cluster_core)>0:\n",
    "            clusters_cores.append(cluster_core)\n",
    "        # reset containers\n",
    "        cluster_events_list = []\n",
    "        currentcl = cl_color\n",
    "    # while the color is the same, append the idx to the current ensemble\n",
    "    cluster_events_list.append( cl_idlist )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed31fa5-e491-4d0a-9c58-d67a9d212a90",
   "metadata": {},
   "source": [
    "#### 5.4 remove core neurons if firing unspecifically within and outside their cluster    \n",
    "We want to check whether a core is firing unspecifically inside and outside of events.    \n",
    "For each cluster color, take all its events and group or mask the intervals from the rest of the times.    \n",
    "Then for each core of this cluster, avg rate inside events - avg outside events.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958c5a6-8de5-49ed-aaa5-328006073052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"    removing cores firing unspecifically\")\n",
    "\n",
    "np_cells_firing_rate = np.array(cells_firing_rate)\n",
    "for caidx,ecolor in enumerate(cluster_color_array):\n",
    "    if ecolor=='gray':\n",
    "        continue\n",
    "    # prepare a row_mask to contain all False for events intervals.\n",
    "    # It will be used to compute the inside-cluster-events firing rate\n",
    "    # Its inverse will be used to compute the outsite-cluster-events firing rate\n",
    "    row_mask = np.array([False for el in range(np_cells_firing_rate.shape[1])])\n",
    "    # select all events of this cluster\n",
    "    event_idxs = np.argwhere(events_color_assignments==ecolor).flatten()\n",
    "    for event_idx in event_idxs:\n",
    "        # take start and end of the event\n",
    "        estart = events[event_idx]['start']\n",
    "        eend = events[event_idx]['end']\n",
    "        row_mask[ estart:eend ] = True\n",
    "    # remove cores whose firing rate outside its cluster events is higher than inside\n",
    "    # because it means they are firing unspecifically\n",
    "    for coids in clusters_cores_by_color[ecolor]:\n",
    "        if type(coids)!=type([]): # for single element list retrieved as element\n",
    "            coids = [coids]\n",
    "        for coid in coids:\n",
    "            coidx = ophys_cell_ids.index(coid)\n",
    "            event_meanrate = np.mean(np_cells_firing_rate[coidx][row_mask])\n",
    "            outside_meanrate = np.mean(np_cells_firing_rate[coidx])\n",
    "            meanratediff = event_meanrate-outside_meanrate\n",
    "            if meanratediff < 0.:\n",
    "                if caidx in clusters_cores:\n",
    "                    if coid in clusters_cores[caidx]:\n",
    "                        clusters_cores[caidx].remove(coid)\n",
    "\n",
    "np.save(exp_path+\"/results/clusters_cores%s.npy\"%scan_id, clusters_cores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25e34a-bb31-4d93-9e77-23d760c182c2",
   "metadata": {},
   "source": [
    "Gathering all cores ids and indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3ebf7-60a8-40cb-b19f-d47ad43a1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"    gathering cores from all clusters\")\n",
    "core_indexes = []\n",
    "other_indexes = []\n",
    "for dyn_core in clusters_cores:\n",
    "    core_indexes.extend( [ophys_cell_ids.index(strid) for strid in dyn_core] )\n",
    "core_indexes = np.unique(core_indexes)\n",
    "print(\"    # cores:\",len(core_indexes))\n",
    "other_indexes = [i for i in range(len(ophys_cell_ids)) if i not in core_indexes]\n",
    "print(\"    # non-cores:\",len(other_indexes))\n",
    "# details\n",
    "cxc = []\n",
    "oxc = []\n",
    "for ccl in clusters_cores_by_color.values():\n",
    "    cxc.append(len(ccl))\n",
    "    ool = [1 for i in ophys_cell_ids if i not in ccl]\n",
    "    oxc.append(len(ool))\n",
    "print(\"    cores per cluster: {:1.2f}±{:1.2f} (min {:d}, max {:d})\".format(np.mean(cxc),np.std(cxc),np.min(cxc),np.max(cxc)) )\n",
    "print(\"    others per cluster: {:1.2f}±{:1.2f} (min {:d}, max {:d})\".format(np.mean(oxc),np.std(oxc),np.min(oxc),np.max(oxc)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
