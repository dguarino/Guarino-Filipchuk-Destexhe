{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780c8e36-1686-4929-8d53-a52dbcadcc62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multiunit recordings from several cortical areas\n",
    "\n",
    "We will collect neurons from several cortical areas to perform population *dynamical* and *attractor* analysis, and get the *functional connectivity*.     \n",
    "This will address three relevant points:\n",
    "- Does the dynamic analysis hold at **higher temporal resolution**?\n",
    "    - How do events statistics (duration and size) compare to 2-photon?\n",
    "- Are population events only a **side-effect of behavior** (locomotion, whisker pad, pupil)?\n",
    "    - Do behavioral components explain pattern reproducibility?\n",
    "- Do **all areas of cortex** show attractor dynamics?\n",
    "    - How do pattern trajectories compare to MICrONS?\n",
    "\n",
    "To do all this, we analyse the [data](https://janelia.figshare.com/articles/dataset/Eight-probe_Neuropixels_recordings_during_spontaneous_behaviors/7739750/4) by [Stringer et al. 2019](science.org/doi/10.1126/science.aav7893).   \n",
    "Eight-probe Neuropixels recordings in three mice during spontaneous activity.   \n",
    "\n",
    "This notebook calls `dynamical_analysis.ipynb` and `attractor_analysis.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99df4ec6-2371-4d70-bfe2-c3e54d074390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "%run -i 'imports_functions.py' \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43d468-6d74-49c0-98ea-a4ee385dc16b",
   "metadata": {},
   "source": [
    "**WARNING**: the next cell takes time to download and unzip the neuropixel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a72f962-c629-4438-9264-9e1204cfe184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data available.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"stringer/7739750.zip\"):\n",
    "    print(\"Downloading neuropixel data ...\")\n",
    "    if not os.path.exists(\"stringer\"):\n",
    "        os.makedirs(\"stringer\")\n",
    "    resp = wget.download(\"https://janelia.figshare.com/ndownloader/articles/7739750/versions/4\", \"stringer/7739750.zip\")\n",
    "    print(\"... Done: \"+resp)\n",
    "\n",
    "if not os.path.exists(\"stringer/7739750\"):\n",
    "    # unzip downloaded folder\n",
    "    if os.path.exists(\"stringer/7739750.zip\"):\n",
    "        print(\"... unzipping\")\n",
    "        shutil.unpack_archive(\"stringer/7739750.zip\", \"stringer/7739750\")\n",
    "        shutil.unpack_archive(\"stringer/7739750/spks.zip\", \"stringer/7739750/spks\")\n",
    "        shutil.unpack_archive(\"stringer/7739750/faces.zip\", \"stringer/7739750/faces\")\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"All data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45d437-412e-4f0c-91cb-003c7ae09467",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "This analysis is based on the file `ephysLoad.m`.\n",
    "\n",
    "Each \"spks\" is a structure of length 8, where each entry is a different probe (these probes were recorded simultaneously). It contains the spike times (in seconds, e.g. 4048.44929626 sec (?kHz sampling)), the cluster identity of each spike (its cell), and the height of each cluster on the probe.\n",
    "\n",
    "The location of each site on the probe in microns in the Allen CCF framework is given in \"ccfCoords\". The brain area for each site is in \"borders\" as a function of the height of the site. \n",
    "\n",
    "We need the spikes from each area and probe to be separate lists. So, we build a dictionary to hold them, and save it locally as `area_spiketrains.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11030168-b84e-46a8-bda7-4a2fbab74a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded populations\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"stringer/7739750/area_spiketrains.npy\"):\n",
    "    area_spiketrains = np.load(\"stringer/7739750/area_spiketrains.npy\", allow_pickle=True).item()\n",
    "    print(\"... loaded populations\")\n",
    "else:\n",
    "    print(\"... collecting populations\")\n",
    "    \n",
    "    probeLoc = sio.loadmat('stringer/7739750/probeLocations.mat')\n",
    "    probeBorders = sio.loadmat('stringer/7739750/probeBorders.mat', squeeze_me=True)\n",
    "\n",
    "    mouse_names = ['Krebs','Waksman','Robbins']\n",
    "    cortical_areas = ['FrCtx','FrMoCtx','SomMoCtx','SSCtx','V1','V2','RSP']\n",
    "\n",
    "    # first count the cells you want to take with this structure\n",
    "    # then think on how you want to store the spikes... compatible with the dynamical_analysis\n",
    "    area_spiketrains = {\n",
    "        'Krebs' : {'FrCtx':[], 'FrMoCtx':[], 'SomMoCtx':[], 'SSCtx':[], 'V1':[], 'V2':[], 'RSP':[]},\n",
    "        'Waksman' : {'FrCtx':[], 'FrMoCtx':[], 'SomMoCtx':[], 'SSCtx':[], 'V1':[], 'V2':[], 'RSP':[]},\n",
    "        'Robbins' : {'FrCtx':[], 'FrMoCtx':[], 'SomMoCtx':[], 'SSCtx':[], 'V1':[], 'V2':[], 'RSP':[]}\n",
    "    }\n",
    "\n",
    "    for imouse in range(len(mouse_names)):\n",
    "        print(mouse_names[imouse])\n",
    "\n",
    "        spks = sio.loadmat('stringer/7739750/spks/spks%s_Feb18.mat'%mouse_names[imouse], squeeze_me=True)\n",
    "\n",
    "        # probe k\n",
    "        # k = 7\n",
    "        for k in range(8):\n",
    "            print(\"probe\",k)\n",
    "\n",
    "            # spike times (in seconds)\n",
    "            st = spks['spks'][k][0]\n",
    "            # clusters\n",
    "            clu = spks['spks'][k][1]\n",
    "            print(\"clusters (cells) of the spikes\",len(np.unique(clu)))\n",
    "            # cluster heights (in microns)\n",
    "            # (see siteCoords to convert to site location)\n",
    "            Wh = spks['spks'][k][2]\n",
    "\n",
    "            # where is the probe in the brain (consolidated labels)\n",
    "            # borders are in microns\n",
    "            # use Wh to determine which clusters are in which brain region\n",
    "            borders = probeBorders['probeBorders'][imouse]['borders'][k]\n",
    "            for j in range(len(borders)):\n",
    "                population = [] # one population per border, there can be several borders\n",
    "                b = borders[j]\n",
    "                if b[2] not in cortical_areas:\n",
    "                    continue\n",
    "                print('upper border %d um, lower border %d um, area %s'%(b[0],b[1],b[2]))\n",
    "                wneurons = np.logical_and(Wh>=b[1], Wh<b[0])\n",
    "                nn = wneurons.sum()\n",
    "                print('%d neurons in %s'%(nn,b[-1]))\n",
    "                # we should not include population smaller than those in MICrONS\n",
    "                if nn<10:\n",
    "                    print('population too small. Rejected.')\n",
    "                    continue\n",
    "\n",
    "                cortical_neurons = np.nonzero(wneurons)[0]\n",
    "                for cn in cortical_neurons:\n",
    "                    cn_idxs = [i for i in range(len(clu)) if clu[i]==cn]\n",
    "                    # print(cn_idxs)\n",
    "                    population.append( sorted(st[cn_idxs]) )\n",
    "                    \n",
    "                area_spiketrains[ mouse_names[imouse] ][ b[2] ].append( population )\n",
    "            print()\n",
    "\n",
    "    # save to file\n",
    "    np.save(\"stringer/7739750/area_spiketrains.npy\", area_spiketrains)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001efe03-3e0a-4255-aa62-23639d33a6eb",
   "metadata": {},
   "source": [
    "The following spiketrains will be loaded.\n",
    "\n",
    "| mouse | probe | borders (um) | area | #neurons |\n",
    "|:----|:----|:----|:---|:---|\n",
    "| **Krebs** | 0 | 4000, 1100 | FrMoCtx | 5 |\n",
    "|           | 1 | 4000, 1800 | FrMoCtx | 73 |\n",
    "|           | 2 | 4000, 2600 | V1 | 61 |\n",
    "|           | 3 | 4000, 2400 | V1 | 141 |\n",
    "|           | 4 | 4000, 1800 | SomMoCtx | 65 |\n",
    "|           | 5 | 4000, 2100 | SomMoCtx | 26 |\n",
    "|           | 6 | 4000, 2350 | V1 | 68 |\n",
    "|           | 7 | 4000, 2600 | V1 | 64 |\n",
    "| **Waksman** | 0 | 4000, 1700 | FrMoCtx | 446 |\n",
    "|             | 0 | 1200, 0 | FrMoCtx | 201 |\n",
    "|             | 1 | 4000, 2150 | FrCtx | 31 |\n",
    "|             | 2 | 4000, 2700 | V1 | 155 |\n",
    "|             | 3 | 4000, 2250 | RSP | 112 |\n",
    "|             | 4 | 4000, 2000 | SomMoCtx | 220 |\n",
    "|             | 5 | 4000, 2600 | SSCtx | 50 |\n",
    "|             | 6 | 4000, 2650 | V2 | 124 |\n",
    "|             | 7 | 4000, 2850 | V1 | 96 |\n",
    "| **Robbins** | 0 | 4000, 3400 | FrMoCtx | 16 |\n",
    "|             | 1 | 4000, 3100 | FrMoCtx | 70 |\n",
    "|             | 3 | 4000, 3550 | RSP | 10 |\n",
    "|             | 4 | 4000, 3500 | SomMoCtx | 10 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0754be-e291-4a07-9bda-2660756cdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_frame_duration = 0.00000001 # sec (e.g. 4048.44929626 s)\n",
    "# frame_duration = 0.001 # ms (e.g. 4048.449 s)\n",
    "frame_duration = 0.01 # 10ms (e.g. 4048.45 s)\n",
    "local_path = os.getcwd() + '/stringer/7739750/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62d98d-d74e-4a71-95fd-15d5313a04c8",
   "metadata": {},
   "source": [
    "#### Note\n",
    "The cell below is particularly long to execute if the dynamical and attractor analyses are used. We therefore used only the first mouse 'Krebs' to perform those analyses.     \n",
    "However, functional correlations and hirarchical modularity are light enough to be performed on all mice (with a good bit of patience). By commenting the sections on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c2d9c-d9c9-4139-a39a-cb6d4e19a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mouse: Krebs\n",
      "\n",
      "mouse: Waksman\n",
      "area:  FrCtx\n",
      "area:  FrMoCtx\n",
      "area:  SomMoCtx\n",
      "area:  SSCtx\n",
      "area:  V1\n",
      "population: 0\n",
      "    time: 246589\n",
      "    cells: 155\n",
      "... Dynamical Analysis\n",
      "... Manifold Analysis\n",
      "... Structural Analysis\n",
      "    binary spiketrains\n",
      "    shape: (155, 246591)\n",
      "    starting functional adjacency matrix\n",
      "    full adjacency matrix: (155, 155)\n",
      "    checking details of best cross-correlation pairs \n"
     ]
    }
   ],
   "source": [
    "# start of spontaneous activity in each mouse (in seconds)\n",
    "etstart = [3811, 3633, 3323]\n",
    "\n",
    "for imouse,(mousename,areas) in enumerate(area_spiketrains.items()):\n",
    "    print(\"\\nmouse:\",mousename)\n",
    "    \n",
    "    # # to reduce computations, use comments here\n",
    "    # if mousename not in [\"Robbins\"]:\n",
    "    # if mousename not in [\"Krebs\", \"Waksman\"]:\n",
    "    if mousename not in [\"Waksman\"]:\n",
    "        continue\n",
    "        \n",
    "    exp_path = local_path + '%s/'%mousename\n",
    "    exp_tstart = etstart[imouse]\n",
    "\n",
    "    # reading behavior data to make statistics about event dependence on it\n",
    "    # we will use the field 'stimulus' to store the avg motSVD of the frames \n",
    "    # The behavioral file is the processed version of a mouse face movie (time x pixels x pixels). \n",
    "    faces = sio.loadmat('stringer/7739750/faces/%s_face_proc.mat'%mousename, squeeze_me=True)\n",
    "    video_timestamps = faces['times'] # same temporal resolution of ephy\n",
    "    motSVD = faces['motionSVD']\n",
    "    exp_istart = (np.abs(video_timestamps - exp_tstart)).argmin()    \n",
    "    motSVD_1c = motSVD[:,0] # only first component\n",
    "    motSVD_1c[motSVD_1c < -4000] = np.mean(motSVD_1c) # corrections\n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "    ax.plot(video_timestamps[exp_istart:], motSVD_1c[exp_istart:], linewidth=0.5, color='k')\n",
    "    fig.savefig(exp_path+\"/motSVD_%s.png\"%mousename, transparent=True, dpi=900)\n",
    "    plt.close()\n",
    "    fig.clear()\n",
    "    fig.clf()\n",
    "\n",
    "    for area,probe_populations in areas.items():\n",
    "        if len(probe_populations)>0:\n",
    "            print(\"area: \",area)\n",
    "            \n",
    "            # # to reduce computations, use comments here\n",
    "            # if area not in ['FrCtx', 'FrMoCtx']:\n",
    "            if area not in ['V1']:\n",
    "                continue\n",
    "                                    \n",
    "            for ipop,spiketrains in enumerate(probe_populations): \n",
    "                print(\"population:\",ipop)\n",
    "\n",
    "                # rounding to ms\n",
    "                # spiketrains = [np.round(sp, 3) for sp in spiketrains] # ms\n",
    "                spiketrains = [np.round(sp, 2) for sp in spiketrains] # 10ms\n",
    "                start_time = min([min(st) if len(st) else 0 for st in spiketrains])\n",
    "                stop_time = max([max(st) if len(st) else 0 for st in spiketrains])\n",
    "                time = np.arange(start_time,stop_time,frame_duration)\n",
    "                print(\"    time:\", len(time))\n",
    "                print(\"    cells:\", len(spiketrains))\n",
    "\n",
    "                fig = plt.figure(figsize=[12.8,4.8])\n",
    "                for row,train in enumerate(spiketrains):\n",
    "                    plt.scatter( train, [row]*len(train), marker='o', edgecolors='none', s=1, c='k' )\n",
    "                plt.ylabel(\"cell IDs\")\n",
    "                plt.xlabel(\"time (s)\")\n",
    "                # plt.show()\n",
    "                fig.savefig(exp_path+'%s_%s_rasterplot.png'%(area,ipop), transparent=False, dpi=800)\n",
    "                plt.tight_layout()\n",
    "                plt.close()\n",
    "                \n",
    "                ophys_cell_ids = list(range(len(spiketrains)))\n",
    "                ophys_cell_indexes = ophys_cell_ids # here is an alias\n",
    "\n",
    "                scan_spiketrains = spiketrains\n",
    "                scan_id = '_%s_%s'%(area,ipop)\n",
    "                \n",
    "                # --------------------------------------------------------------------------\n",
    "                print(\"... Dynamical Analysis\")\n",
    "                core_reproducibility_perc = 99 # threshold for detecting cores\n",
    "                # %run \"dynamical_analysis.ipynb\"\n",
    "                \n",
    "                # # Match smooth motion energy curve with the cluster it belongs to\n",
    "                # # Count the number of events belonging to a pattern before and after the change.\n",
    "                # ccolors,ccounts = np.unique(cluster_color_array, return_counts=True)\n",
    "                # cluster_events_counts = dict(zip(ccolors,ccounts))\n",
    "                # Npre_beh_cluster = {el:0. for el in np.unique(cluster_color_array)}\n",
    "                # Npost_beh_cluster = {el:0. for el in np.unique(cluster_color_array)}\n",
    "                # for sni in smoothed_beh_indices:\n",
    "                #     snitime = exp_tstart + sni * frame_duration\n",
    "                #     snitime_pre = snitime - 0.15 # s\n",
    "                #     snitime_post = snitime + 0.15 # s\n",
    "                #     for ievent,(event,ecolor) in enumerate(zip(events,cluster_color_array)):\n",
    "                #         event_start_time = exp_tstart + event['start'] * frame_duration\n",
    "                #         if snitime_pre < event_start_time and event_start_time < snitime:\n",
    "                #             Npre_beh_cluster[ecolor] += 1\n",
    "                #         if snitime < event_start_time and event_start_time < snitime_post:\n",
    "                #             Npost_beh_cluster[ecolor] += 1\n",
    "                # # detail\n",
    "                # fig = plt.figure()\n",
    "                # plt.scatter(range(len(Npre_beh_cluster.keys())), Npre_beh_cluster.values(), marker='<', c=list(Npre_beh_cluster.keys()), edgecolors=list(Npre_beh_cluster.keys()), s=1)\n",
    "                # plt.scatter(range(len(Npost_beh_cluster.keys())), Npost_beh_cluster.values(), marker='>', c=list(Npost_beh_cluster.keys()), edgecolors='none', s=1)\n",
    "                # plt.vlines(range(len(Npost_beh_cluster.keys())), Npost_beh_cluster.values(), Npre_beh_cluster.values(), colors=list(Npost_beh_cluster.keys()), linewidths=0.6)\n",
    "                # plt.ylabel('occurrence')\n",
    "                # plt.xlabel('Patterns')\n",
    "                # fig.savefig(exp_path+\"/results/Pattern_behavior_%s_%s%s.png\"%(mousename,area,ipop), transparent=True, dpi=600)\n",
    "                # plt.close()\n",
    "                # fig.clear()\n",
    "                # fig.clf()\n",
    "                # # summary\n",
    "                # Nsame = 0\n",
    "                # Npost = 0\n",
    "                # Npre = 0\n",
    "                # for pre,post in zip(Npre_beh_cluster.values(),Npost_beh_cluster.values()):\n",
    "                #     if pre==post: Nsame +=1\n",
    "                #     if pre>post: Npre +=1\n",
    "                #     if pre<post: Npost +=1\n",
    "                # fig = plt.figure()\n",
    "                # plt.bar([0,1,2], [Npre,Nsame,Npost], width=0.8, color='C0')\n",
    "                # plt.ylabel('occurrences')\n",
    "                # plt.xlabel('pattern timing relative to movement')\n",
    "                # plt.xticks(range(3),['before','same','after'])\n",
    "                # fig.savefig(exp_path+\"/results/Pattern_behavior_summary_%s_%s%s.png\"%(mousename,area,ipop), transparent=True, dpi=600)\n",
    "                # plt.close()\n",
    "                # fig.clear()\n",
    "                # fig.clf()\n",
    "                \n",
    "                # --------------------------------------------------------------------------\n",
    "                # dimensional reduction, trajectories, and manifold analysis\n",
    "                print(\"... Manifold Analysis\")\n",
    "                # %run \"attractor_analysis.ipynb\"\n",
    "\n",
    "                # --------------------------------------------------------------------------\n",
    "                # structural analysis\n",
    "                print(\"... Structural Analysis\")\n",
    "                \n",
    "                # make binary spiketrains\n",
    "                print(\"    binary spiketrains\")\n",
    "                binary_spiketrains = np.zeros( (len(spiketrains),len(time)+2) )\n",
    "                print(\"    shape:\", binary_spiketrains.shape)\n",
    "                for row,train in enumerate(spiketrains):\n",
    "                    # iterate over spiketrains assigning 1 to the binary_spiketrains at the corresponding position\n",
    "                    tidxs = np.trunc(train/frame_duration).astype(int) - int(exp_tstart/frame_duration)\n",
    "                    tidxs[tidxs>len(time)] = len(time) \n",
    "                    binary_spiketrains[row][tidxs] = 1\n",
    "                \n",
    "                # --------------------------------------------------------------------------\n",
    "                # functional connectivity matrix\n",
    "                print(\"    starting functional adjacency matrix\")\n",
    "                functional_adjacency_matrix = []\n",
    "                for irow,bsti in enumerate(binary_spiketrains):\n",
    "                    row_xcorr = []\n",
    "                    for jrow,bstj in enumerate(binary_spiketrains):\n",
    "                        if irow==jrow:\n",
    "                            row_xcorr.append(0.0) # no self connections\n",
    "                            continue\n",
    "                        row_xcorr.append(crosscorrelation(bsti, bstj, maxlag=1, mode='corr')[2])\n",
    "                    functional_adjacency_matrix.append(row_xcorr)\n",
    "                functional_adjacency_matrix = np.array(functional_adjacency_matrix)\n",
    "                print(\"    full adjacency matrix:\",functional_adjacency_matrix.shape)\n",
    "                # To ensure sparseness of the matrix, discard weak correlations (<0.4, Sadovsky and MacLean 2013)\n",
    "                functional_adjacency_matrix[ functional_adjacency_matrix <= functional_adjacency_matrix.max()*0.4 ] = 0.0\n",
    "                np.save(exp_path+\"/results/functional_adjacency_matrix_%s%s.npy\"%(area,ipop), functional_adjacency_matrix)\n",
    "                # plot\n",
    "                fig = plt.figure()\n",
    "                plt.pcolormesh(functional_adjacency_matrix)\n",
    "                cbar = plt.colorbar()\n",
    "                fig.savefig(exp_path+'/results/adjacency_matrix_%s%s.png'%(area,ipop), transparent=True)\n",
    "                plt.close()\n",
    "                fig.clear()\n",
    "                fig.clf()\n",
    "                \n",
    "                # -----------------------------------------------------------------------------\n",
    "                # is the cross-correlation between cells significant to justify a functional connectivity analysis?\n",
    "                print(\"    checking details of best cross-correlation pairs \")\n",
    "                # pick highly correlated cells for further inspection\n",
    "                # thresholds for the top and bottom percentiles\n",
    "                top_threshold = np.percentile(functional_adjacency_matrix, 95)\n",
    "                highly_correlated_indices = np.where(functional_adjacency_matrix > top_threshold)\n",
    "                bin_size = 1 # can be made lower\n",
    "                lags = np.arange(-50, 51, bin_size)\n",
    "                num_pairs = 30\n",
    "                co_occurrence_counts = []\n",
    "                surrogates_co_occurrence_counts = []\n",
    "                for i in range(num_pairs):\n",
    "                    spike_times1 = binary_spiketrains[highly_correlated_indices[0][i]].astype(int)\n",
    "                    spike_times2 = binary_spiketrains[highly_correlated_indices[1][i]].astype(int)\n",
    "                    co_occurrence_counts_per_lag = np.convolve(spike_times1, spike_times2[::-1], mode='same')\n",
    "                    co_occurrence_counts.append(co_occurrence_counts_per_lag)\n",
    "                    # geenrate surrogates by reshuffling\n",
    "                    surrogates_co_occurrences = []\n",
    "                    for j in range(10):\n",
    "                        surrspike_times1 = np.random.permutation(spike_times1)\n",
    "                        surrspike_times2 = np.random.permutation(spike_times2)\n",
    "                        co_occurrence_counts_per_lag = np.convolve(surrspike_times1, surrspike_times2[::-1], mode='same')\n",
    "                        surrogates_co_occurrences.append(co_occurrence_counts_per_lag)\n",
    "                    surrogates_co_occurrence_counts.append(np.mean(surrogates_co_occurrences, axis=0))                    \n",
    "                # grid of individual cross-correlations\n",
    "                num_plots = len(co_occurrence_counts)\n",
    "                num_rows = int(np.sqrt(num_plots))\n",
    "                num_cols = int(np.ceil(num_plots / num_rows))\n",
    "                fig = plt.figure(figsize=(15, 10))\n",
    "                for i, co_occurrence_counts_pair in enumerate(co_occurrence_counts):\n",
    "                    plt.subplot(num_rows, num_cols, i+1)\n",
    "                    plt.plot(lags, co_occurrence_counts_pair[:len(lags)], color='blue')\n",
    "                    plt.plot(lags, surrogates_co_occurrence_counts[i][:len(lags)], color='red', linestyle='dashed')\n",
    "                fig.text(0.5, 0.04, 'Lag (ms)', ha='center')\n",
    "                fig.text(0.04, 0.5, 'Correlation', va='center', rotation='vertical')\n",
    "                plt.tight_layout()\n",
    "                fig.savefig(exp_path+'/results/correlogram_%s%s.svg'%(area,ipop), transparent=True)\n",
    "                plt.close()\n",
    "                fig.clear()\n",
    "                fig.clf()\n",
    "                \n",
    "                \n",
    "                \n",
    "                0/0\n",
    "                \n",
    "                \n",
    "                \n",
    "                # -----------------------------------------------------------------------------\n",
    "                # creating graph from functional_adjacency_matrix as in Sadovsky and MacLean 2013\n",
    "                # functional_adjacency_matrix[ functional_adjacency_matrix <= functional_adjacency_matrix.max()*0.4 ] = 0.0\n",
    "                functional_adjacency_matrix[ functional_adjacency_matrix >= functional_adjacency_matrix.max()*0.4 ] = 1.0\n",
    "                \n",
    "                dgraph = ig.Graph.Weighted_Adjacency(functional_adjacency_matrix, mode='directed')\n",
    "                ig.plot(dgraph, exp_path+'/results/ring_%s%s.png'%(area,ipop), layout=dgraph.layout(\"circle\"), edge_curved=0.2, edge_color='#000', edge_width=0.5, edge_arrow_size=0.1, vertex_size=5, vertex_color='#000', margin=50)\n",
    "                print('    preparing vertex labels for cores and others')\n",
    "                dgraph.vs[\"ophys_cell_id\"] = ophys_cell_ids\n",
    "                is_id_core = np.array( [0] * len(ophys_cell_ids) )\n",
    "                is_id_core[core_indexes] = 1\n",
    "                dgraph.vs[\"is_core\"] = is_id_core.tolist()\n",
    "\n",
    "                degrees = np.array(dgraph.degree())\n",
    "                print(\"    Degree distributions\")\n",
    "                # https://igraph.org/python/api/latest/igraph._igraph.GraphBase.html#degree\n",
    "                degdist = dgraph.degree_distribution(bin_width=5)\n",
    "                degree_counts = [bi[2] for bi in degdist.bins()]\n",
    "                fig = plt.figure()\n",
    "                plt.plot(range(len(degree_counts)), degree_counts, linewidth=3.0)\n",
    "                plt.ylabel('Number of vertices')\n",
    "                plt.xlabel('Degree')\n",
    "                plt.xscale('log')\n",
    "                plt.yscale('log')\n",
    "                plt.savefig(exp_path+'/results/degree_distribution_%s%s.png'%(area,ipop), transparent=True, dpi=300)\n",
    "                plt.close()\n",
    "                \n",
    "                # Clustering Coefficient of only excitatory cells\n",
    "                print('    Local Clustering Coefficient (cores too)')\n",
    "                local_clustering_coefficients = np.array(dgraph.transitivity_local_undirected(vertices=None, mode=\"zero\"))\n",
    "                \n",
    "                #--------------------------------------------------------------------\n",
    "                # cores have low LCC\n",
    "\n",
    "                # assign cores to their lcc and degree\n",
    "                # print(core_indexes)\n",
    "                core_local_clustering_coefficients = np.array(dgraph.transitivity_local_undirected(vertices=core_indexes, mode=\"zero\"))\n",
    "                core_degrees = np.array(dgraph.degree(vertices=core_indexes, mode=\"all\"))\n",
    "\n",
    "                # figure\n",
    "                fig, (hmmap, chist) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [6, 1]})\n",
    "                # hierarchy\n",
    "                hmmap.scatter( degrees, local_clustering_coefficients, marker='o', facecolor='#111111', s=50, edgecolors='none', alpha=0.5) \n",
    "                hmmap.scatter( core_degrees, core_local_clustering_coefficients, marker='o', facecolor='none', s=50, edgecolors='forestgreen') \n",
    "                hmmap.set_yscale('log')\n",
    "                hmmap.set_ylim([0.02,1.1])\n",
    "                hmmap.set_xscale('log')\n",
    "                hmmap.spines['top'].set_visible(False)\n",
    "                hmmap.spines['right'].set_visible(False)\n",
    "                hmmap.set_ylabel('LCC')\n",
    "                hmmap.set_xlabel('degree')\n",
    "                hmmap.tick_params(axis='both', bottom='on', top='on', left='off', right='off')\n",
    "                # core lcc histogram\n",
    "                bins = np.linspace(0.02,1,50)\n",
    "                barheight = (max(local_clustering_coefficients)-min(local_clustering_coefficients))/50\n",
    "                chist.barh(bins[:-1], lcc_hist, height=barheight, align='center', color='green', linewidth=0)\n",
    "                chist.spines['top'].set_visible(False)\n",
    "                chist.spines['right'].set_visible(False)\n",
    "                chist.tick_params(axis='x', which='both', bottom=True, top=False, labelsize='x-small')\n",
    "                chist.tick_params(axis='y', which='both', left=True, right=False, labelleft=True)\n",
    "                chist_ticks = chist.get_xticks()\n",
    "                chist.set_ylim([0.01,1.1])\n",
    "                chist.set_ylabel('LCC')\n",
    "                chist.set_xlabel('count')\n",
    "                chist.yaxis.set_label_position(\"right\")\n",
    "                chist.spines['top'].set_visible(False)\n",
    "                chist.spines['right'].set_visible(False)\n",
    "                chist.spines['bottom'].set_visible(False)\n",
    "                plt.tight_layout()\n",
    "                fig.savefig(exp_path+\"/results/cores_hierarchical_modularity_%s%s.svg\"%(area,ipop), transparent=True)\n",
    "                plt.close()\n",
    "                fig.clear()\n",
    "                fig.clf()\n",
    "                \n",
    "                #--------------------------------------------------------------------\n",
    "                # Flow analysis\n",
    "                print(\"... Flow Analysis\")\n",
    "                \n",
    "                if len(core_indexes)>1 and len(other_indexes)>1:\n",
    "                    # The amount of flow on an edge cannot exceed the capacity of the edge.\n",
    "                    # therefore, edges with high capacity will be more important for the flow.\n",
    "                    # here we test the hypothesis that edges towards cores have higher capacity\n",
    "                    # or that the sum of edges towards cores have a higher total capacity\n",
    "                    cell_total_capacity = {cid:list() for cid in ophys_cell_ids}\n",
    "                    edges_sourcing = {cid:0 for cid in ophys_cell_ids}\n",
    "                    edges_targeting = {cid:0 for cid in ophys_cell_ids}\n",
    "\n",
    "                    for cluster_k,events_cellids in scan_clustered_spectrums.items(): # we consider only the scan 0 because it's the largest (for now)\n",
    "                        cluster_k = cluster_k.split(',')[0]\n",
    "\n",
    "                        if cluster_k == 'gray':\n",
    "                            continue\n",
    "\n",
    "                        for vnt in events_cellids:\n",
    "                            for posi,vidj in enumerate(vnt[1:]):\n",
    "                                vidi = vnt[posi] # enumerate will go from 0\n",
    "                                # print(vidi, vidj)\n",
    "\n",
    "                                # check beginning and end are not the same\n",
    "                                if dgraph.vs.find(ophys_cell_id=vidi).index == dgraph.vs.find(ophys_cell_id=vidj).index:\n",
    "                                    continue\n",
    "                                # # check there is a path between the two\n",
    "                                # if len(spinesgraph.get_all_shortest_paths(spinesgraph.vs.find(name=vidi).index, to=spinesgraph.vs.find(name=vidj).index, weights=None, mode='out'))>0:\n",
    "                                #     continue\n",
    "\n",
    "                                # Take the maximum flow between the previous and next vertices\n",
    "                                mfres = dgraph.maxflow(dgraph.vs.find(ophys_cell_id=vidi).index, dgraph.vs.find(ophys_cell_id=vidj).index)\n",
    "                                # print(mfres)\n",
    "                                # returns a tuple containing the following:\n",
    "                                # graph - the graph on which this flow is defined\n",
    "                                # value - the value (capacity) of the maximum flow between the given vertices\n",
    "                                # flow - the flow values on each edge. For directed graphs, this is simply a list where element i corresponds to the flow on edge i.\n",
    "                                # cut - edge IDs in the minimal cut corresponding to the flow.\n",
    "                                # partition - vertex IDs in the parts created after removing edges in the cut\n",
    "                                # es - an edge selector restricted to the edges in the cut.\n",
    "\n",
    "                                # we get a flow value for each edge contributing to the flow.\n",
    "                                # source\n",
    "                                mfres_value = mfres.value\n",
    "                                if vidi in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                                    mfres_value /= len(core_indexes)\n",
    "                                else:\n",
    "                                    mfres_value /= len(other_indexes)\n",
    "                                cell_total_capacity[vidi].append(mfres_value)\n",
    "                                # target\n",
    "                                mfres_value = mfres.value\n",
    "                                if vidj in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                                    mfres_value /= len(core_indexes)\n",
    "                                else:\n",
    "                                    mfres_value /= len(other_indexes)\n",
    "                                cell_total_capacity[vidj].append(mfres_value)\n",
    "\n",
    "                                # Iterate over the edges identified by the flow.\n",
    "                                # count the edges sourcing from cores, and those targeting cores. Which is more?\n",
    "                                for edge in mfres.es:\n",
    "                                    sourceid = int(dgraph.vs[edge.source]['ophys_cell_id'])\n",
    "                                    targetid = int(dgraph.vs[edge.target]['ophys_cell_id'])\n",
    "                                    if sourceid in cell_total_capacity.keys():\n",
    "                                        edges_sourcing[sourceid] +=1 # just count\n",
    "                                    if targetid in cell_total_capacity.keys():\n",
    "                                        edges_targeting[targetid] +=1 # just count\n",
    "\n",
    "                    # Flow\n",
    "                    # print(cell_total_capacity)\n",
    "                    flowvalue_cores = []\n",
    "                    for cid in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                        flowvalue_cores.extend(cell_total_capacity[cid])\n",
    "                    flowvalue_others = []\n",
    "                    for cid in np.array(ophys_cell_ids)[other_indexes]:\n",
    "                        flowvalue_others.extend(cell_total_capacity[cid])\n",
    "\n",
    "                    # description\n",
    "                    print(\"    Flow cores: \"+str(stats.describe(flowvalue_cores)) )\n",
    "                    print(\"    Flow others: \"+str(stats.describe(flowvalue_others)) )\n",
    "                    # significativity\n",
    "                    print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowvalue_cores, flowvalue_others, equal_var=False))\n",
    "                    d,_ = stats.ks_2samp(flowvalue_cores, flowvalue_others) # non-parametric measure of effect size [0,1]\n",
    "                    print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "                    fig, ax = plt.subplots()\n",
    "                    xs = np.random.normal(1, 0.04, len(flowvalue_cores))\n",
    "                    plt.scatter(xs, flowvalue_cores, alpha=0.3, c='forestgreen')\n",
    "                    xs = np.random.normal(2, 0.04, len(flowvalue_others))\n",
    "                    plt.scatter(xs, flowvalue_others, alpha=0.3, c='silver')\n",
    "                    vp = ax.violinplot([flowvalue_cores,flowvalue_others], widths=0.15, showextrema=False, showmeans=True)\n",
    "                    for pc in vp['bodies']:\n",
    "                        pc.set_edgecolor('black')\n",
    "                    for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "                        pc.set_facecolor(cb)\n",
    "                    vp['cmeans'].set_color('orange')\n",
    "                    # vp['cmedians'].set_linewidth(2.)\n",
    "                    ax.spines['top'].set_visible(False)\n",
    "                    ax.spines['bottom'].set_visible(False)\n",
    "                    ax.spines['left'].set_visible(False)\n",
    "                    ax.spines['right'].set_visible(False)\n",
    "                    plt.ylabel('Normalized flow value')\n",
    "                    plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(flowvalue_cores)), \"other\\n(n={:d})\".format(len(flowvalue_others))])\n",
    "                    fig.savefig(exp_path+\"/results/global_cores_others_flowvalue_%s%s.svg\"%(area,ipop), transparent=True)\n",
    "                    plt.show()\n",
    "                    # fig.clf()\n",
    "                    # plt.close()\n",
    "\n",
    "                    print()\n",
    "                    # Cuts\n",
    "                    # print(edges_sourcing)\n",
    "                    # print(edges_targeting)\n",
    "                    flowcuts_core_sources = []\n",
    "                    flowcuts_core_targets = []\n",
    "                    for cid in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                        flowcuts_core_sources.append(edges_sourcing[cid]/len(core_indexes))\n",
    "                        flowcuts_core_targets.append(edges_targeting[cid]/len(core_indexes))\n",
    "                    flowcuts_other_sources = []\n",
    "                    flowcuts_other_targets = []\n",
    "                    for cid in np.array(ophys_cell_ids)[other_indexes]:\n",
    "                        flowcuts_other_sources.append(edges_sourcing[cid]/len(other_indexes))\n",
    "                        flowcuts_other_targets.append(edges_targeting[cid]/len(other_indexes))\n",
    "\n",
    "                    # description\n",
    "                    print(\"    Cut edges sourcing from cores: \"+str(stats.describe(flowcuts_core_sources)) )\n",
    "                    print(\"    Cut edges targeting cores: \"+str(stats.describe(flowcuts_core_targets)) )\n",
    "                    print(\"    Cut edges sourcing from others: \"+str(stats.describe(flowcuts_other_sources)) )\n",
    "                    print(\"    Cut edges targeting others: \"+str(stats.describe(flowcuts_other_targets)) )\n",
    "                    # significativity\n",
    "                    print(\"    Core targets vs sources Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowcuts_core_targets, flowcuts_core_sources, equal_var=False))\n",
    "                    d,_ = stats.ks_2samp(flowcuts_core_targets, flowcuts_core_sources) # non-parametric measure of effect size [0,1]\n",
    "                    print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "                    print(\"    Core targets vs Other targets Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowcuts_core_targets, flowcuts_other_targets, equal_var=False))\n",
    "                    d,_ = stats.ks_2samp(flowcuts_core_targets, flowcuts_other_targets) # non-parametric measure of effect size [0,1]\n",
    "                    print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "                    fig, ax = plt.subplots()\n",
    "                    xs = np.random.normal(1, 0.04, len(flowcuts_core_sources))\n",
    "                    plt.scatter(xs, flowcuts_core_sources, alpha=0.3, c='forestgreen')\n",
    "                    xs = np.random.normal(2, 0.04, len(flowcuts_core_targets))\n",
    "                    plt.scatter(xs, flowcuts_core_targets, alpha=0.3, c='forestgreen')\n",
    "                    xs = np.random.normal(3, 0.04, len(flowcuts_other_sources))\n",
    "                    plt.scatter(xs, flowcuts_other_sources, alpha=0.3, c='silver')\n",
    "                    xs = np.random.normal(4, 0.04, len(flowcuts_other_targets))\n",
    "                    plt.scatter(xs, flowcuts_other_targets, alpha=0.3, c='silver')\n",
    "                    vp = ax.violinplot([flowcuts_core_sources,flowcuts_core_targets,flowcuts_other_sources,flowcuts_other_targets], widths=0.15, showextrema=False, showmeans=True)\n",
    "                    for pc in vp['bodies']:\n",
    "                        pc.set_edgecolor('black')\n",
    "                    for pc in vp['bodies'][0:2]:\n",
    "                        pc.set_facecolor('#228B224d')\n",
    "                    for pc in vp['bodies'][2:]:\n",
    "                        pc.set_facecolor('#D3D3D34d')\n",
    "                    vp['cmeans'].set_color('orange')\n",
    "                    # vp['cmedians'].set_linewidth(2.)\n",
    "                    ax.spines['top'].set_visible(False)\n",
    "                    ax.spines['bottom'].set_visible(False)\n",
    "                    ax.spines['left'].set_visible(False)\n",
    "                    ax.spines['right'].set_visible(False)\n",
    "                    plt.ylabel('Normalized edges in the cut')\n",
    "                    plt.xticks([1, 2, 3, 4], [\"core as\\nsource\", \"core as\\ntarget\", \"other as\\nsource\", \"other as\\ntarget\"])\n",
    "                    fig.savefig(exp_path+\"/results/global_cores_others_cutvalue_%s%s.svg\"%(area,ipop), transparent=True)\n",
    "                    plt.show()\n",
    "                    # fig.clf()\n",
    "                    # plt.close()\n",
    "                #--------------------------------------------------------------------\n",
    "                \n",
    "    gc.collect()\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df7b3e-1e8f-433b-8d6a-74ab074db700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
