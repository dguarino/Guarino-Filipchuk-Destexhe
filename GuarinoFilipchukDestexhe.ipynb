{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reproducible activity without attractors in the mouse cortex \n",
    "\n",
    "Analysis code to reproduce all panels in figures 1 and 2 of the paper by Guarino, Filipchuk, Destexhe (2022)   \n",
    "preprint link: https://www.biorxiv.org/content/10.1101/2022.05.24.493230v2\n",
    "\n",
    "All this code is hosted on a github [repository](https://github.com/dguarino/Guarino-Filipchuk-Destexhe) (with a Zenodo DOI persistent identifier [here](https://zenodo.org)) and can be interactively executed here.  \n",
    "The repository also contains a copy of the required data files from the [MICrONS project phase1](https://www.microns-explorer.org/phase1) (freely available on the project website), to ease the setup on Binder. \n",
    "\n",
    "This notebook performs loading and selection of the MICrONS data, structural and dynamical analyses, and plots the results as in the paper panels.\n",
    "\n",
    "We divided the analysis code into:\n",
    "- `imports_functions.py` : performs the imports and definition of various helper functions.\n",
    "- `structural_analysis.py` : creates a graph from the connectivity matrix and computes several graph measures (using [igraph](https://igraph.org)).\n",
    "- `dynamical_analysis.py` : performs the same population event analysis as in [Filipchuk et al. 2022](https://www.biorxiv.org/content/10.1101/2021.08.31.458322v2) and then also extracts the core neurons of the events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "from builtins import exec\n",
    "exec(open(\"./imports_functions.py\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading curated data from MICrONS project phase 1\n",
    "\n",
    "The following code for data loading and selection is taken from   \n",
    "https://github.com/AllenInstitute/MicronsBinder/blob/master/notebooks/intro/MostSynapsesInAndOut.ipynb   \n",
    "https://github.com/AllenInstitute/MicronsBinder/blob/master/notebooks/vignette_analysis/function/structure_function_analysis.ipynb\n",
    "\n",
    "`Neurons.pkl` contains the `segment_id` for each pyramidal neuron in the EM volume.    \n",
    "`Soma.pkl` contains the soma position for all the cells in the EM volume.   \n",
    "`calcium_trace.pkl` contains the calcium imaging traces (including deconvolved spikes).    \n",
    "`soma_subgraph_synapses_spines_v185.csv` contains the list of synapses with root pre-/post-synaptic somas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"MICrONS_data/calcium_trace.pkl\"):\n",
    "    print(\"Downloading 2photon calcium traces ...\")\n",
    "    resp = wget.download(\"https://zenodo.org/record/5646567/files/calcium_trace.pkl?download=1\", \"MICrONS_data/calcium_trace.pkl\")\n",
    "    print(\"... Done: \"+resp)\n",
    "    \n",
    "if os.path.exists(\"MICrONS_data/calcium_trace.pkl\"):\n",
    "    calcium_trace = pd.read_pickle(\"MICrONS_data/calcium_trace.pkl\")\n",
    "    calcium_trace_df = pd.DataFrame.from_dict(calcium_trace, orient='index')\n",
    "# print(calcium_trace)\n",
    "# print(calcium_trace_df.columns) # ['scan', 'trace_raw', 'trace', 'spike', 'stimulus']\n",
    "# print(len(calcium_trace_df.index) ) # 112\n",
    "# print(len(calcium_trace_df[calcium_trace_df['scan']==1].index))\n",
    "# print(len(calcium_trace_df[calcium_trace_df['scan']==1].stimulus[648518346349539895]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAUTION: The cell below might take some time to load the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1961, 17)\n",
      "(3239275, 16)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"MICrONS_data/pni_synapses_v185.csv\"):\n",
    "    print(\"Downloading Synapse table ...\")\n",
    "    resp = wget.download(\"https://zenodo.org/record/3710459/files/pni_synapses_v185.csv?download=1\", \"MICrONS_data/pni_synapses_v185.csv\")\n",
    "    print(\"... Done: \"+resp)\n",
    "\n",
    "if not os.path.exists(\"MICrONS_data/soma_subgraph_synapses_spines_v185.csv\"):\n",
    "    print(\"Downloading soma_subgraph_synapses_spines_v185 ...\")\n",
    "    resp = wget.download(\"https://zenodo.org/record/3710459/files/soma_subgraph_synapses_spines_v185.csv?download=1\", \"MICrONS_data/soma_subgraph_synapses_spines_v185.csv\")\n",
    "    print(\"... Done: \"+resp)\n",
    "\n",
    "with open(\"MICrONS_data/Neuron.pkl\", 'rb') as handle:\n",
    "    Neuron = pickle.load(handle)\n",
    "with open(\"MICrONS_data/Soma.pkl\", 'rb') as handle:\n",
    "    Soma = pickle.load(handle)\n",
    "\n",
    "syn_spines_df = pd.read_csv('MICrONS_data/soma_subgraph_synapses_spines_v185.csv')\n",
    "# id, pre_root_id, post_root_id, cleft_vx, spine_vol_um3\n",
    "print(syn_spines_df.shape)\n",
    "\n",
    "syn_df = pd.read_csv('MICrONS_data/pni_synapses_v185.csv')\n",
    "print(syn_df.shape)\n",
    "# print(syn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the IDs and number of recorded pyramidal neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_list = Neuron[\"segment_id\"]\n",
    "n_pyc = pyc_list.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the folder to which all results will be saved, and the frame duration (from the MICrONS docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = os.getcwd()\n",
    "frame_duration = 0.0674 # sec, 14.8313 frames per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Accessing 2-photon Calcium imaging data subset\n",
    "\n",
    "We are interested in reading only the Ca-imaging data of the cells for which also the EM reconstruction is available.   \n",
    "\n",
    "##### CAUTION: next cell can take some time to load all calcium imaging data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyramidal neurons recorded with 2-photon Calcium imaging:  112\n",
      "27100\n",
      "... producing spike rasterplot\n"
     ]
    }
   ],
   "source": [
    "print(\"Pyramidal neurons recorded with 2-photon Calcium imaging: \",len(calcium_trace))\n",
    "ophys_cell_ids = list(calcium_trace.keys())\n",
    "n_frames = len(calcium_trace[ophys_cell_ids[0]]['spike'])\n",
    "print(n_frames)\n",
    "start_time = 0 # 200 frames of blank screen are already removed from the data\n",
    "stop_time = (n_frames)*frame_duration\n",
    "time = np.arange(start_time,stop_time,frame_duration)\n",
    "\n",
    "spiketrains = [[] for _ in range(5)] # five scans\n",
    "ophys_scan_ids = [[] for _ in range(5)] # five scans\n",
    "for ocell_id in ophys_cell_ids:\n",
    "    decst = calcium_trace[ocell_id][\"spike\"]\n",
    "    spiketrains[(calcium_trace[ocell_id][\"scan\"])-1].append( time[:][np.nonzero(decst)]) # deconvolved Ca spiketrains\n",
    "    ophys_scan_ids[(calcium_trace[ocell_id][\"scan\"])-1].append( ocell_id )\n",
    "\n",
    "print(\"... producing spike rasterplot\")\n",
    "fig = plt.figure(figsize=[12.8,4.8])\n",
    "rowg = 0\n",
    "rowc = ['b','g','r','c','m']\n",
    "for scanid,scan in enumerate(spiketrains):\n",
    "    for row,train in enumerate(scan):\n",
    "        plt.scatter( train, [row+rowg]*len(train), marker='o', edgecolors='none', s=1, c=rowc[scanid] )\n",
    "    rowg += row+1\n",
    "    # add here timing of oriented stimulus\n",
    "    \n",
    "plt.ylabel(\"cell IDs\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "fig.savefig(exp_path+'/results/rasterplot.png', transparent=False, dpi=800)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the cell indexes from the list of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ophys_cell_indexes = range(len(ophys_cell_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get soma center locations\n",
    "\n",
    "They are provided in voxels coordinates of 4,4,40 nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_soma_loc = np.zeros((n_pyc, 3))\n",
    "for i in range(n_pyc):\n",
    "    seg_id = pyc_list[i]\n",
    "    pyc_soma_loc[i,:] = get_soma_loc(Soma, seg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join cell indexes with their position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_ca_soma_loc = np.zeros((len(ophys_cell_indexes), 3))\n",
    "for i in ophys_cell_indexes:\n",
    "    seg_id = ophys_cell_ids[i]\n",
    "    idx = np.where(pyc_list==seg_id)[0][0]\n",
    "    pyc_ca_soma_loc[i,:] = pyc_soma_loc[idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency matrix\n",
    "\n",
    "First, we build an adjacency matrix for all the EM-imaged neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.zeros((len(ophys_cell_indexes), len(ophys_cell_indexes)))\n",
    "\n",
    "for i in ophys_cell_indexes:\n",
    "    root_id = ophys_cell_ids[i]\n",
    "    root_id_postsyn_list = syn_df[syn_df['pre_root_id'] == root_id]['post_root_id'].tolist()\n",
    "    for ps in root_id_postsyn_list:\n",
    "        if ps in ophys_cell_ids:\n",
    "            ips = ophys_cell_ids.index(ps)\n",
    "            adjacency_matrix[i][ips]=1\n",
    "np.save(exp_path+'/results/adjacency_matrix.npy', adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make also make 2p-scan-specific matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_adjacency_matrix = {}\n",
    "for scan_id in range(5):\n",
    "    \n",
    "    gshape = len(ophys_scan_ids[scan_id]) # num cells in the scan\n",
    "    adjacency_matrix = np.zeros((gshape, gshape))\n",
    "    \n",
    "    for i,root_id in enumerate(ophys_scan_ids[scan_id]):\n",
    "        root_id_postsyn_list = syn_df[syn_df['pre_root_id'] == root_id]['post_root_id'].tolist()\n",
    "        for ps in root_id_postsyn_list:\n",
    "            if ps in ophys_scan_ids[scan_id]:\n",
    "                ips = ophys_scan_ids[scan_id].index(ps)\n",
    "                adjacency_matrix[i][ips]=1\n",
    "        scan_adjacency_matrix[scan_id] = adjacency_matrix\n",
    "    \n",
    "np.save(exp_path+'/results/scan_adjacency_matrix.npy', scan_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Are cells with correlated firing connected?\n",
    "\n",
    "We first measure 1-lag correlation across all cells.   \n",
    "Then, we mask the functional adjacency matrix using the EM connectivity matrix.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09722222222222222 (14/144)\n"
     ]
    }
   ],
   "source": [
    "tot_correlated = []\n",
    "tot_connected = []\n",
    "for scan_id,scan in enumerate(spiketrains):\n",
    "    # make binary spiketrains\n",
    "    binary_spiketrains = np.zeros( (len(scan),len(time)+2) )\n",
    "    # print(binary_spiketrains.shape)\n",
    "    \n",
    "    for row,train in enumerate(scan):\n",
    "        # iterate over spiketrains assigning 1 to the binary_spiketrains at the corresponding position\n",
    "        tidxs = np.trunc(np.array(train)/frame_duration).astype(int)\n",
    "        tidxs[tidxs>len(time)] = len(time) \n",
    "        binary_spiketrains[row][tidxs] = 1\n",
    "\n",
    "    functional_adjacency_matrix = []\n",
    "    for irow,bsti in enumerate(binary_spiketrains):\n",
    "        row_xcorr = []\n",
    "        for jrow,bstj in enumerate(binary_spiketrains):\n",
    "            if irow==jrow:\n",
    "                row_xcorr.append(0.0) # no self connections\n",
    "                continue\n",
    "            row_xcorr.append(crosscorrelation(bsti, bstj, maxlag=1, mode='corr')[2])\n",
    "        functional_adjacency_matrix.append(row_xcorr)\n",
    "    functional_adjacency_matrix = np.array(functional_adjacency_matrix)\n",
    "    np.save(exp_path+\"/results/functional_adjacency_matrix_%d.npy\"%scan_id, functional_adjacency_matrix)\n",
    "    \n",
    "    # bootstrap confirms the percentile\n",
    "    # print(functional_adjacency_matrix.mean(), np.percentile(functional_adjacency_matrix, 95))\n",
    "    # func_percentile = []\n",
    "    # for bi in range(1000):\n",
    "    #     bootstrap_matrix = np.stack([np.random.choice(functional_adjacency_matrix.flatten(),size=functional_adjacency_matrix.shape[0],replace=False) for i in range(functional_adjacency_matrix.shape[0])])\n",
    "    #     func_percentile.append(np.percentile(bootstrap_matrix, 95))\n",
    "    # print(np.mean(func_percentile))\n",
    "    \n",
    "    # find mean and 95%, plot in on the colorbar, highlight which ones are above\n",
    "    above95points = np.argwhere(functional_adjacency_matrix >= np.percentile(functional_adjacency_matrix,95))\n",
    "    tot_correlated.extend(above95points)\n",
    "    \n",
    "    # plot\n",
    "    fig = plt.figure()\n",
    "    # norm = MidpointNormalize(vmin=np.amin(functional_adjacency_matrix), vmax=np.amax(functional_adjacency_matrix), midpoint=0)\n",
    "    norm = MidpointNormalize(vmin=-0.04, vmax=0.12, midpoint=0)\n",
    "    plt.pcolormesh(functional_adjacency_matrix, cmap='bwr', norm=norm)\n",
    "    ax = plt.gca()\n",
    "    for ap in above95points:\n",
    "        rect = patches.Rectangle((ap[1],ap[0]),1,1,linewidth=1,edgecolor='k',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.plot(0.5, functional_adjacency_matrix.mean(), 'k.') \n",
    "    cbar.ax.plot(0.5, np.percentile(functional_adjacency_matrix,95), 'w.')\n",
    "    fig.savefig(exp_path+\"/results/functional_adjacency_matrix_scan%d.png\"%scan_id, transparent=True)\n",
    "    plt.close()\n",
    "    fig.clear()\n",
    "    fig.clf()\n",
    "    \n",
    "    # masking\n",
    "    maskedmatrix = functional_adjacency_matrix*scan_adjacency_matrix[scan_id]\n",
    "    masked95points = np.argwhere(maskedmatrix >= np.percentile(functional_adjacency_matrix,95))\n",
    "    tot_connected.extend(masked95points)\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure()\n",
    "    # norm = MidpointNormalize(vmin=np.amin(maskedmatrix), vmax=np.amax(maskedmatrix), midpoint=0)\n",
    "    norm = MidpointNormalize(vmin=-0.04, vmax=0.12, midpoint=0)\n",
    "    plt.pcolormesh(maskedmatrix, cmap='bwr', norm=norm)\n",
    "    plt.title(\"EM-corrected connection rate: %.2f\"%(len(masked95points)/len(above95points)))\n",
    "    ax = plt.gca()\n",
    "    for ap in above95points:\n",
    "        rect = patches.Rectangle((ap[1],ap[0]),1,1,linewidth=1,edgecolor='k',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.plot(0.5, functional_adjacency_matrix.mean(), 'k.') \n",
    "    cbar.ax.plot(0.5, np.percentile(functional_adjacency_matrix,95), 'w.')\n",
    "    fig.savefig(exp_path+\"/results/EMmasked_functional_adjacency_matrix_scan%d.png\"%scan_id, transparent=True)\n",
    "    plt.close()\n",
    "    fig.clear()\n",
    "    fig.clf()\n",
    "\n",
    "# print(len(tot_correlated))\n",
    "# print(tot_connected)\n",
    "print(len(tot_connected)/len(tot_correlated), \"(%d/%d)\"%(len(tot_connected),len(tot_correlated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are co-tuned cells connected?\n",
    "All scans were made using the same 16 orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==1].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==2].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==3].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==4].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==5].stimulus.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copying for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27100\n"
     ]
    }
   ],
   "source": [
    "stimuli = [] \n",
    "for scan_id in range(5):\n",
    "    stimuli.append( calcium_trace_df[calcium_trace_df['scan']==scan_id+1].head(1).stimulus.tolist()[0] )\n",
    "print(len(stimuli[0])) # 27100, first 200 frames are used for the Df/f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocol\n",
    "- 1 orientation out of 16 is presented during 15 frames\n",
    "- followed by 40-41 frames of pink noise\n",
    "- all orientations are randomly repeated 30 times (trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5]\n"
     ]
    }
   ],
   "source": [
    "orientations = np.unique(stimuli[0])[:-1] # without the nan\n",
    "print(orientations)\n",
    "# print(list(stimuli[0]))\n",
    "# print(np.argwhere(stimuli[0]==45.).tolist())\n",
    "# print(np.argwhere(stimuli[4]==45.).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all cells, we are going to compute:\n",
    "- the *Orientation tuning* (ORT) as in [Ringach et al. 1997](https://pubmed.com):    \n",
    "    $ORT = \\frac{1}{T} \\sum_{t}^{T} R_{td}$    \n",
    "    where $T$ is the number of trials, $R_{td}$ is the number of spikes fired during one trial presentation of one stimulus with orientation $d$.\n",
    "    \n",
    "- the *Orientation Sensitivity Index* (OSI) as in [Ringach et al. 2002](https://pubmed.com):    \n",
    "    $OSI = |\\frac{\\sum_{d}^{D} R_d * exp(i\\frac{\\pi}{4}d)}{\\sum_{d}^{D} R_d}|$    \n",
    "    where $R_i$ is the response during stimulus presentation $d$.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute 100(or 1000) permuted spiketrains to test OSI significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... entering reshuffling for 0\n",
      "... entering reshuffling for 1\n",
      "... entering reshuffling for 2\n",
      "... entering reshuffling for 3\n",
      "... entering reshuffling for 4\n",
      "... done.\n"
     ]
    }
   ],
   "source": [
    "surrogate_spiketrains = [[[] for j in range(len(ophys_scan_ids[i])) ] for i in range(5)] # 3d: scan, ori, intervals\n",
    "# print(surrogate_spiketrains)\n",
    "\n",
    "for scan_id in range(5):\n",
    "    print(\"... entering reshuffling for\", scan_id)\n",
    "    for cidx,st in enumerate(spiketrains[scan_id]):\n",
    "        spiketrainsISI = []\n",
    "        spiketrainsISI.append( np.diff( st ) )\n",
    "        # build surrogate rasterplots\n",
    "        for isur in range(100):\n",
    "            isi = spiketrainsISI.copy()\n",
    "            random.shuffle(isi) # in-place function\n",
    "            if len(isi): # init of timing of first spike\n",
    "                spiketrainsISI[0] += start_time\n",
    "            surrogate_spiketrains[scan_id][cidx].extend( np.cumsum(isi) ) # extend() to speed later\n",
    "print(\"... done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSI for 648518346349539895: 0.6432632756577449\n",
      "sOSI for 648518346349539895: 0.20000000000000007\n",
      "Tunings for 648518346349539895: [0.03333333333333333, 0.03333333333333333, 0.0, 0.03333333333333333, 0.0, 0.06666666666666667, 0.0, 0.0, 0.03333333333333333, 0.06666666666666667, 0.0, 0.0, 0.06666666666666667, 0.03333333333333333, 0.1, 0.03333333333333333]\n",
      "Tunings for 648518346349539895: [0.13333333333333333, 0.0, 0.0, 0.06666666666666667, 0.06666666666666667, 0.3333333333333333, 0.6, 0.2, 0.2, 0.03333333333333333, 0.0, 0.0, 0.03333333333333333, 0.5, 0.3333333333333333, 0.16666666666666666]\n"
     ]
    }
   ],
   "source": [
    "gOSI_cell_ids = dict.fromkeys(list(calcium_trace.keys()))\n",
    "sOSI_cell_ids = dict.fromkeys(list(calcium_trace.keys()))\n",
    "gORT_cell_ids = dict.fromkeys(list(calcium_trace.keys()))\n",
    "sORT_cell_ids = dict.fromkeys(list(calcium_trace.keys()))\n",
    "\n",
    "orientation_intervals = [[[] for i in range(16) ] for i in range(5)] # 3d: scan, ori, intervals\n",
    "orientation_interval_frames = [[[] for i in range(16) ] for i in range(5)] # 3d: scan, ori, intervals\n",
    "non_significant_cid = []\n",
    "\n",
    "for scan_id in range(5):\n",
    "    tdarray = np.zeros( (len(orientations),len(spiketrains[scan_id])) )\n",
    "    surrogate_tdarray = np.zeros( (len(orientations),len(spiketrains[scan_id])) )\n",
    "    for oridx,ori in enumerate(orientations):\n",
    "        frames = np.argwhere(stimuli[scan_id]==ori)[:,0]\n",
    "        # print(frames)\n",
    "        # beginning frames for the presentation of this orientation \n",
    "        boundaries = [i for i in range(1, len(frames)) if frames[i] != frames[i-1]+1]\n",
    "        intervals = []\n",
    "        interframes = []\n",
    "        first = 0\n",
    "        for bstim in boundaries:\n",
    "            intervals.append( (frames[first]*frame_duration, frames[bstim-1]*frame_duration) )\n",
    "            interframes.append( (frames[first], frames[bstim-1]) )\n",
    "            first = bstim\n",
    "        intervals.append( (frames[first]*frame_duration, frames[-1]*frame_duration) ) # last\n",
    "        interframes.append( (frames[first], frames[-1]) ) # last\n",
    "        orientation_intervals[scan_id][oridx] = intervals\n",
    "        orientation_interval_frames[scan_id][oridx] = interframes\n",
    "        # print(intervals)\n",
    "        # for each cell in this scan ...\n",
    "        for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "            # gather all spikes fired during the presentation of the current orientation\n",
    "            for startstim,endstim in intervals:\n",
    "                tdarray[oridx][cidx] += len([sp for sp in spiketrains[scan_id][cidx] if (startstim<sp and sp<=endstim)])\n",
    "                # surrogates tuning\n",
    "                ssp = surrogate_spiketrains[scan_id][cidx]\n",
    "                surrogate_tdarray[oridx][cidx] += ((startstim<ssp)&(ssp<=endstim)).sum() \n",
    "    tdarray /= 30\n",
    "    surrogate_tdarray /= 30*100\n",
    "\n",
    "    for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "        gORT_cell_ids[cid] = list(tdarray.T[cidx])\n",
    "        sORT_cell_ids[cid] = list(surrogate_tdarray.T[cidx])\n",
    "    \n",
    "    # gOSI\n",
    "    for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "        OSI_cell = 0.0\n",
    "        Rcell = 0.0\n",
    "        for oridx,ori in enumerate(orientations):\n",
    "            OSI_cell += tdarray[oridx][cidx] * np.exp(1j * (np.pi/4) * oridx)\n",
    "            Rcell += tdarray[oridx][cidx]\n",
    "        gOSI_cell_ids[cid] = np.abs( OSI_cell / Rcell )\n",
    "    # sOSI\n",
    "    for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "        OSI_cell = 0.0\n",
    "        Rcell = 0.0\n",
    "        for oridx,ori in enumerate(orientations):\n",
    "            OSI_cell += surrogate_tdarray[oridx][cidx] * np.exp(1j * (np.pi/4) * oridx)\n",
    "            Rcell += surrogate_tdarray[oridx][cidx]\n",
    "        sOSI_cell_ids[cid] = np.abs( OSI_cell / Rcell )\n",
    "\n",
    "    # Which cells have an OSI below their surrogates?\n",
    "    for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "        # print(sOSI_cell_ids[cid], gOSI_cell_ids[cid])\n",
    "        if sOSI_cell_ids[cid] >= gOSI_cell_ids[cid]:\n",
    "            non_significant_cid.append(cid)\n",
    "\n",
    "print(\"OSI for 648518346349539895:\",gOSI_cell_ids[648518346349539895])\n",
    "fig = plt.figure()\n",
    "plt.hist(gOSI_cell_ids.values(), bins=10, range=(0.,1.), rwidth=0.8)\n",
    "fig.canvas.draw()\n",
    "plt.ylim([0,38])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('OSI')\n",
    "fig.savefig(exp_path+'/results/OSI.png', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "\n",
    "print(\"sOSI for 648518346349539895:\",sOSI_cell_ids[648518346349539895])\n",
    "fig = plt.figure()\n",
    "plt.hist(sOSI_cell_ids.values(), bins=10, range=(0.,1.), rwidth=0.8, color=\"gray\")\n",
    "fig.canvas.draw()\n",
    "plt.ylim([0,38])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('sOSI')\n",
    "fig.savefig(exp_path+'/results/sOSI.png', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "\n",
    "print(\"Tunings for 648518346349539895:\",sORT_cell_ids[648518346349539895])\n",
    "fig = plt.figure()\n",
    "barori = np.array([0. for _ in range(16)])\n",
    "for cid,oris in sORT_cell_ids.items():\n",
    "    barori += oris\n",
    "plt.bar(range(len(barori)), barori, 0.9, color=\"gray\")\n",
    "fig.canvas.draw()\n",
    "plt.ylim([0,20])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Preferred orientation (deg)')\n",
    "plt.xticks(range(17),['0','','45','','90','','135','','180','','225','','270','','315','','360'])\n",
    "fig.savefig(exp_path+'/results/sORI.png', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "                                                      \n",
    "print(\"Tunings for 648518346349539895:\",gORT_cell_ids[648518346349539895])\n",
    "fig = plt.figure()\n",
    "barori = np.array([0. for _ in range(16)])\n",
    "for cid,oris in gORT_cell_ids.items():\n",
    "    barori += oris\n",
    "plt.bar(range(len(barori)), barori, 0.9)\n",
    "fig.canvas.draw()\n",
    "plt.ylim([0,20])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Preferred orientation (deg)')\n",
    "plt.xticks(range(17),['0','','45','','90','','135','','180','','225','','270','','315','','360'])\n",
    "fig.savefig(exp_path+'/results/ORI.png', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surrogates need to be converted in cell significance of OSI and exclude the cell non-significantly tuned from the count below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[648518346349538527, 648518346349539423, 648518346349534072, 648518346349537814, 648518346349539892, 648518346349536366, 648518346349538730, 648518346349539096, 648518346349535074, 648518346349536851, 648518346349533252, 648518346349537741]\n"
     ]
    }
   ],
   "source": [
    "print(non_significant_cid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, are co-tuned cells connected?\n",
    "The classical [Ko et al. 2011](https://www.nature.com/articles/nature09880) (Fig. 2d) results show that co-tuned cells are preferably connected (left below).     \n",
    "![Ko et al. 2011, Fig. 2d](resources/Ko2011_Fig2d.png \"Ko et al. 2011, Fig. 2d\") \n",
    "![Ko et al. 2011 data](results/Ko_chart.png \"Ko et al. 2011 data\") \n",
    "\n",
    "However, the results should be evaluated in full (right above). Over 16 mice, 126 cells in total were recorded, 116 cells both *in-vivo* and *in-vitro*. Of the remaining cells, 94 were orientation selective, and 26 were connected. Of these, 10 were co-tuned (orientation preference delta within 22.5 deg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ko_data = np.array([[0., 0., 22., 10.], [69., 4., 11., 10.]])\n",
    "Ko_labels = [None, None, 'non-responsive', 'not in-vitro', 'just orientation selective', 'orientation selective and connected (4/24)', 'orientation selective and connected (11/44)', 'orientation selective, connected and co-tuned (10/26)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12.8,4.8])\n",
    "size = 0.3 # to have wedges\n",
    "outer_colors = ['gray','yellowgreen']\n",
    "inner_colors = ['gray','gray','gray','lightgray', 'yellowgreen','forestgreen','forestgreen','darkgreen']\n",
    "# ax.pie(vals.sum(axis=1), radius=1-size, autopct='%1.f%%', startangle=90, pctdistance=0.8, colors=outer_colors, wedgeprops=dict(width=size, edgecolor='w'))\n",
    "ax.pie(Ko_data.flatten(), radius=1, labels=Ko_labels, autopct=lambda pct:'{:1.0f}%'.format(pct) if pct > 0 else '', startangle=90, pctdistance=0.85, colors=inner_colors, wedgeprops=dict(width=size, edgecolor='w'))\n",
    "fig.savefig(exp_path+'/results/Ko_chart.png', transparent=True, dpi=60)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although in [Turner et al. 2022](https://doi.org/10.1016/j.cell.2022.01.023) it is reported a positive correlation between mean response to preferred directions and connections (Fig. 7H), this is only because of a **linear** fit. What would happen to the figure using [weighted least squares](http://www.itl.nist.gov/div898/handbook/pmd/section4/pmd432.htm) or [generalized least squares](https://en.wikipedia.org/wiki/Generalized_least_squares)? The outliers (here circled in red shades) would affect much less the fit (in dashed red).    \n",
    "![Turner et al. 2022, Fig. 7H](resources/Turner2022_Fig7h.png \"Turner et al. 2022, Fig. 7H\")   \n",
    "\n",
    "Here, we do the same analysis as Ko et al. on the MICrONS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all OSI cells: 112\n",
      "significant OSI cells: 100\n",
      "Connection probability between pairs of same orientation-tuned neurons: 0.06 (6/100)\n"
     ]
    }
   ],
   "source": [
    "ort_df = pd.DataFrame.from_dict(gORT_cell_ids, orient='index')\n",
    "# print(ort_df.shape)\n",
    "# print(ort_df)\n",
    "\n",
    "# find best orientation tuning for each cell\n",
    "cell_max_ort = ort_df.idxmax(axis=1).to_dict()\n",
    "# print(cell_max_ort)\n",
    "\n",
    "# how many co-tuned cells?\n",
    "rev_max_ort = {} # \n",
    "for key, value in cell_max_ort.items():\n",
    "     rev_max_ort.setdefault(value, set()).add(key)\n",
    "rev_max_ort = dict(sorted(rev_max_ort.items()))\n",
    "# cotuned_cells = [(key,values) for key,values in rev_max_ort.items() if len(values) > 1] \n",
    "cotuned_cells = [values for key,values in rev_max_ort.items() if len(values) > 1] \n",
    "# print(cotuned_cells)\n",
    "\n",
    "# how many co-tuned are connected?\n",
    "same_ort_connected_tot = 0\n",
    "ort45_ort_connected_tot = 0\n",
    "ort90_ort_connected_tot = 0\n",
    "same_ort_connected = 0\n",
    "ort45_ort_connected = 0\n",
    "ort90_ort_connected = 0\n",
    "for cellid, ori in cell_max_ort.items():\n",
    "    # print(cellid, ori)\n",
    "    \n",
    "    if cellid in non_significant_cid: # comment this line to have the total (use the commented bars with 'gray' color below to plot)\n",
    "        continue\n",
    "    # print(cellid)\n",
    "    \n",
    "    # cycle through cell ids\n",
    "    postsyn_list = syn_df[syn_df['pre_root_id'] == cellid]['post_root_id'].tolist()\n",
    "    for ps in postsyn_list:\n",
    "        if ps==cellid:\n",
    "            continue\n",
    "        if ps in cell_max_ort and cell_max_ort[ps]==cell_max_ort[cellid]:\n",
    "            # print(\"same ort:\",ps)\n",
    "            same_ort_connected += 1 \n",
    "            same_ort_connected_tot += len(cotuned_cells[ori])\n",
    "        if ps in cell_max_ort and (cell_max_ort[ps]==cell_max_ort[cellid]+2 or cell_max_ort[ps]==cell_max_ort[cellid]+2):\n",
    "            # print(\"45 deg apart ort:\",ps) \n",
    "            ort45_ort_connected += 1 \n",
    "            ort45_ort_connected_tot += len(cotuned_cells[ori+2])+len(cotuned_cells[ori-2])\n",
    "        if ps in cell_max_ort and (cell_max_ort[ps]==cell_max_ort[cellid]+4 or cell_max_ort[ps]==cell_max_ort[cellid]+4):\n",
    "            # print(\"90 deg apart ort:\",ps) \n",
    "            ort90_ort_connected += 1\n",
    "            ort90_ort_connected_tot += len(cotuned_cells[ori+4])+len(cotuned_cells[ori-4])\n",
    "\n",
    "print(\"all OSI cells:\", len(gOSI_cell_ids.items()))\n",
    "OSI_significant = [v for k,v in gOSI_cell_ids.items() if k not in non_significant_cid]\n",
    "print(\"significant OSI cells:\", len(OSI_significant))\n",
    "\n",
    "# print(same_ort_connected,ort45_ort_connected,ort90_ort_connected)\n",
    "# print(same_ort_connected_tot,ort45_ort_connected_tot,ort90_ort_connected_tot)\n",
    "ort_connected = (same_ort_connected+ort45_ort_connected+ort90_ort_connected)\n",
    "ort_tot_connected = (same_ort_connected_tot+ort45_ort_connected_tot+ort90_ort_connected_tot)\n",
    "print(\"Connection probability between pairs of same orientation-tuned neurons:\", same_ort_connected/len(OSI_significant), \"(%d/%d)\"%(same_ort_connected,len(OSI_significant)))\n",
    "                             \n",
    "fig = plt.figure()\n",
    "mbar = plt.bar(range(3), [same_ort_connected/same_ort_connected_tot, ort45_ort_connected/ort45_ort_connected_tot, ort90_ort_connected/ort90_ort_connected_tot], 0.8, color=\"C0\", zorder=3)\n",
    "# mbar = plt.bar(range(3), [same_ort_connected/same_ort_connected_tot, ort45_ort_connected/ort45_ort_connected_tot, ort90_ort_connected/ort90_ort_connected_tot], 0.8, color=\"gray\", zorder=3)\n",
    "# plt.hlines(ort_connected/ort_tot_connected, -0.5,2.5, linestyle='--', color=\"gray\", zorder=1)\n",
    "fig.canvas.draw()\n",
    "plt.ylim([0,0.1])\n",
    "plt.ylabel('Connection probability')\n",
    "plt.xlabel(r'$\\Delta$ ori (deg)')\n",
    "plt.xticks(range(3),['0','45','90'])\n",
    "for rect,con,otot in zip(mbar,[same_ort_connected, ort45_ort_connected, ort90_ort_connected],[same_ort_connected_tot, ort45_ort_connected_tot, ort90_ort_connected_tot]):\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{con}/{otot}', ha='center', va='bottom')\n",
    "fig.savefig(exp_path+'/results/conn_ori.png', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-squared statistical independence: 2.814644590754769\n",
      "p-value: 0.8317254321296013\n"
     ]
    }
   ],
   "source": [
    "ORT_connected_all_couples = [87, 85, 99] # from the cell above, commenting or not the filtering by OSI-significant cells\n",
    "ORT_connected_sign_couples = [76, 63, 66]\n",
    "ORT_connected_all_cellid = [7, 4, 6]\n",
    "ORT_connected_sign_cellid = [6, 3, 4]\n",
    "(statistic,pvalue,dof,expected_freqndarray) = stats.chi2_contingency([ORT_connected_sign_cellid, ORT_connected_all_cellid, ORT_connected_sign_couples, ORT_connected_all_couples])\n",
    "print(\"Chi-squared statistical independence:\", statistic)\n",
    "print(\"p-value:\", pvalue)\n",
    "\n",
    "MICrONS_data = np.array([len(non_significant_cid), len(OSI_significant)-len(ORT_connected_sign_cellid), 4., 3., 6.])\n",
    "MICrONS_labels = ['non significant OSI', 'just orientation selective', 'orientation selective and connected (4/13)', 'orientation selective and connected (3/13)', 'orientation selective, connected, and co-tuned (6/13)']\n",
    "fig, ax = plt.subplots(figsize=[9,4])\n",
    "size = 0.3 # to have wedges\n",
    "inner_colors = ['gray', 'yellowgreen', 'forestgreen', 'forestgreen', 'darkgreen']\n",
    "ax.pie(MICrONS_data, radius=1, labels=MICrONS_labels, autopct=lambda pct:'{:1.0f}%'.format(pct) if pct > 0 else '', startangle=90, pctdistance=0.85, colors=inner_colors, wedgeprops=dict(width=size, edgecolor='w'))\n",
    "fig.savefig(exp_path+'/results/MICrONS_chart.png', transparent=True, dpi=300)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Structural analysis\n",
    "\n",
    "This part deals with the purely structural description of the portion of VISp cortex L2/3 considered in the MICrONS dataset.    \n",
    "In particular, we want to know whether it has a structural property supposed to be common to all cortices: *hierarchical modularity* -- where cells have dense connectivity with other cells within the same module and sparse connectivity with cells in other modules ([Meunier et al. 2009](https://doi.org/10.3389/neuro.11.037.2009), [Newman 2010](https://academic.oup.com/book/27303)).    \n",
    "This property, together with the ability to measure it from 2-photon data as we will do later in this ipynb (see [Sadovsky and MacLean 2013](https://www.jneurosci.org/content/33/35/14048), but here their results are explicitly checked against EM for the first time, afaik), allows us to generalize our findings to the whole cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "    number of vertices: 334\n",
      "... Network nodes degrees\n",
      "... Degree distributions\n",
      "... Local Clustering Coefficient\n",
      "... Betweenness centrality\n",
      "... Motifs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58/2963444990.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  motifsratio = motifs/surrogate_motifs\n"
     ]
    }
   ],
   "source": [
    "global_degree_counts = []\n",
    "global_degree_distribution = []\n",
    "global_structural_betweeness = []\n",
    "global_structural_motifs = []\n",
    "global_structural_motifsratio = []\n",
    "global_structural_motifsurrogates = []\n",
    "\n",
    "%run \"structural_analysis.ipynb\"\n",
    "\n",
    "global_structural_betweeness.append(betweenness_centrality)\n",
    "global_degree_counts.append(degree_counts)\n",
    "global_degree_distribution.append(degrees)\n",
    "global_structural_motifs.append(motifs)\n",
    "global_structural_motifsurrogates.append(surrogate_motifs)\n",
    "global_structural_motifsratio.append(motifsratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dynamical Analysis\n",
    "\n",
    "If you consider (I share your concern) that the 2-photon temporal resolution is a problem for single cell responses, I believe (you may not share my belief) that the frame duration of the MICrONS L2/3 dataset is enough to have a (partial) view of the cortical population response, which takes hundreds of ms to build up.   \n",
    "\n",
    "Here below we will be looking into reproducible firing patterns and their stimulus-dependence.    \n",
    "All panels of Figure 1 are produced in the next cell by the `dynamical_analysis.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replotting spiketrains\n",
    "with stimulus times added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rowc = ['b','g','r','c','m']\n",
    "# for scanid,(scan,oris) in enumerate(zip(spiketrains,orientation_intervals)):\n",
    "#     height = len(scan)*0.042857142857143\n",
    "#     fig, ax = plt.subplots(figsize=(12.8,height))\n",
    "#     for row,train in enumerate(scan):\n",
    "#         ax.scatter( train, [row]*len(train), marker='o', edgecolors='none', s=1, c=rowc[scanid] )\n",
    "#     for stim in oris:\n",
    "#         for times in stim:\n",
    "#             ax.axvspan(times[0], times[1], alpha=0.1, linewidth=0, facecolor='grey', zorder=1)\n",
    "#     plt.ylabel(\"cell IDs\")\n",
    "#     plt.xlabel(\"time (s)\")\n",
    "#     fig.savefig(exp_path+\"/results/rasterplot_scan%d.png\"%scanid, transparent=False, dpi=1800)\n",
    "#     plt.close()\n",
    "#     fig.clear()\n",
    "#     fig.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan:  0\n",
      "    population firing: 0.40±0.65 sp/frame\n",
      "    cells firing rate: 0.01±0.11 sp/s\n",
      "... generating surrogates to establish population event threshold\n",
      "... loaded surrogates\n",
      "    event size threshold (mean): 2.018052946814365\n",
      "... find population events in the trial\n",
      "... signatures of population events\n",
      "    number of events: 83\n",
      "    number of events per sec: 0.0454411072300634\n",
      "    events duration: 0.674±0.240\n",
      "    events size: 5.000±1.496\n",
      "... Similarity of events matrix\n",
      "... clustering\n",
      "    linkage\n",
      "    surrogate events signatures for clustering threshold\n",
      "    cluster reproducibility threshold: 0.30468919887369783\n",
      "    cluster size threshold: 2\n",
      "    Total number of clusters: 17\n",
      "    # clusters (after removing those below reproducibility threshold): 7\n",
      "{0: {}, 1: {}, 2: {}, 3: {'#00b5eb': 1}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {'#80ffb4': 2}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {'#80ffb4': 1}}\n",
      "... finding cluster cores\n",
      "    removing cores firing unspecifically\n",
      "    gathering cores from all clusters\n",
      "    # cores: 19\n",
      "    # non-cores: 16\n",
      "    cores per cluster: 4.00±1.94 (min 0, max 7)\n",
      "    others per cluster: 31.00±1.94 (min 28, max 35)\n",
      "\n",
      "scan:  1\n",
      "    population firing: 0.28±0.54 sp/frame\n",
      "    cells firing rate: 0.01±0.12 sp/s\n",
      "... generating surrogates to establish population event threshold\n",
      "... loaded surrogates\n",
      "    event size threshold (mean): 2.00773238733685\n",
      "... find population events in the trial\n",
      "... signatures of population events\n",
      "    number of events: 54\n",
      "    number of events per sec: 0.029564093860523177\n",
      "    events duration: 0.708±0.242\n",
      "    events size: 4.000±1.181\n",
      "... Similarity of events matrix\n",
      "... clustering\n",
      "    linkage\n",
      "    surrogate events signatures for clustering threshold\n",
      "    cluster reproducibility threshold: 0.36028478070542547\n",
      "    cluster size threshold: 2\n",
      "    Total number of clusters: 10\n",
      "    # clusters (after removing those below reproducibility threshold): 3\n",
      "{0: {}, 1: {}, 2: {}, 3: {'#00b5eb': 1}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {'#80ffb4': 2}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {'#80ffb4': 1}}\n",
      "... finding cluster cores\n",
      "    removing cores firing unspecifically\n",
      "    gathering cores from all clusters\n",
      "    # cores: 6\n",
      "    # non-cores: 15\n",
      "    cores per cluster: 1.50±1.50 (min 0, max 3)\n",
      "    others per cluster: 19.50±1.50 (min 18, max 21)\n",
      "\n",
      "scan:  2\n",
      "    population firing: 0.27±0.53 sp/frame\n",
      "    cells firing rate: 0.01±0.11 sp/s\n",
      "... generating surrogates to establish population event threshold\n",
      "... loaded surrogates\n",
      "    event size threshold (mean): 2.007429433311131\n",
      "... find population events in the trial\n",
      "... signatures of population events\n",
      "    number of events: 35\n",
      "    number of events per sec: 0.01916191268737613\n",
      "    events duration: 0.741±0.245\n",
      "    events size: 5.000±1.610\n",
      "... Similarity of events matrix\n",
      "... clustering\n",
      "    linkage\n",
      "    surrogate events signatures for clustering threshold\n",
      "    cluster reproducibility threshold: 0.2967163431246496\n",
      "    cluster size threshold: 2\n",
      "    Total number of clusters: 7\n",
      "    # clusters (after removing those below reproducibility threshold): 2\n",
      "{0: {}, 1: {}, 2: {}, 3: {'#00b5eb': 1}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {'#80ffb4': 2}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {'#80ffb4': 1}}\n",
      "... finding cluster cores\n",
      "    removing cores firing unspecifically\n",
      "    gathering cores from all clusters\n",
      "    # cores: 3\n",
      "    # non-cores: 19\n",
      "    cores per cluster: 1.00±1.41 (min 0, max 3)\n",
      "    others per cluster: 21.00±1.41 (min 19, max 22)\n",
      "\n",
      "scan:  3\n",
      "    population firing: 0.20±0.46 sp/frame\n",
      "    cells firing rate: 0.01±0.10 sp/s\n",
      "... generating surrogates to establish population event threshold\n",
      "... loaded surrogates\n",
      "    event size threshold (mean): 2.0046498744910584\n",
      "... find population events in the trial\n",
      "... signatures of population events\n",
      "    number of events: 12\n",
      "    number of events per sec: 0.006569798635671816\n",
      "    events duration: 0.573±0.229\n",
      "    events size: 3.500±0.957\n",
      "... Similarity of events matrix\n",
      "... clustering\n",
      "    linkage\n",
      "    surrogate events signatures for clustering threshold\n",
      "    cluster reproducibility threshold: 0.31431945842973075\n",
      "    cluster size threshold: 2\n",
      "    Total number of clusters: 5\n",
      "    # clusters (after removing those below reproducibility threshold): 2\n",
      "{0: {}, 1: {}, 2: {}, 3: {'#00b5eb': 1}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {'#80ffb4': 2}, 9: {}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {'#80ffb4': 1}}\n",
      "... finding cluster cores\n",
      "    removing cores firing unspecifically\n",
      "    gathering cores from all clusters\n",
      "    # cores: 5\n",
      "    # non-cores: 17\n",
      "    cores per cluster: 2.00±0.82 (min 1, max 3)\n",
      "    others per cluster: 20.00±0.82 (min 19, max 21)\n",
      "\n",
      "scan:  4\n",
      "    population firing: 0.07±0.27 sp/frame\n",
      "    cells firing rate: 0.01±0.08 sp/s\n",
      "... generating surrogates to establish population event threshold\n",
      "... loaded surrogates\n",
      "    event size threshold (mean): 1.001158150005927\n",
      "... find population events in the trial\n",
      "... signatures of population events\n",
      "    number of events: 37\n",
      "    number of events per sec: 0.02025687912665477\n",
      "    events duration: 0.539±0.109\n",
      "    events size: 2.000±0.676\n",
      "... Similarity of events matrix\n",
      "... clustering\n",
      "    linkage\n",
      "    surrogate events signatures for clustering threshold\n",
      "    cluster reproducibility threshold: 0.4884591860218792\n",
      "    cluster size threshold: 2\n",
      "    Total number of clusters: 7\n",
      "    # clusters (after removing those below reproducibility threshold): 2\n",
      "{0: {'#80ffb4': 1}, 1: {}, 2: {}, 3: {'#00b5eb': 1}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {'#80ffb4': 2, '#ff7e41': 1}, 9: {'#80ffb4': 1}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {'#80ffb4': 1}}\n",
      "... finding cluster cores\n",
      "    removing cores firing unspecifically\n",
      "    gathering cores from all clusters\n",
      "    # cores: 3\n",
      "    # non-cores: 9\n",
      "    cores per cluster: 1.00±0.82 (min 0, max 2)\n",
      "    others per cluster: 11.00±0.82 (min 10, max 12)\n",
      "\n",
      "{0: {'#80ffb4': 1}, 1: {}, 2: {}, 3: {'#00b5eb': 1}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {'#80ffb4': 2, '#ff7e41': 1}, 9: {'#80ffb4': 1}, 10: {}, 11: {}, 12: {}, 13: {}, 14: {}, 15: {'#80ffb4': 1}}\n"
     ]
    }
   ],
   "source": [
    "global_structural_motif_cores = {k: 0 for k in range(16)}\n",
    "global_structural_motif_others = {k: 0 for k in range(16)}\n",
    "global_events_sec = []\n",
    "global_events_duration = []\n",
    "global_cluster_number = []\n",
    "global_cluster_selfsimilarity = []\n",
    "global_cluster_tuning = {k:{} for k in range(16)} # by orientation\n",
    "\n",
    "core_reproducibility_perc = 60 # threshold for detecting cores\n",
    "\n",
    "for scan_id,scan_spiketrains in enumerate(spiketrains):\n",
    "    print(\"scan: \",scan_id)\n",
    "    ophys_cell_ids = ophys_scan_ids[scan_id]\n",
    "    ophys_cell_indexes = range(len(ophys_cell_ids))\n",
    "    \n",
    "    %run \"dynamical_analysis.ipynb\"\n",
    "\n",
    "    global_events_sec.append(events_sec)\n",
    "    global_events_duration.extend(events_durations_f)\n",
    "    global_cluster_number.append(nclusters)\n",
    "    global_cluster_selfsimilarity.extend(reproducibility_list)\n",
    "    print()\n",
    "\n",
    "print(global_cluster_tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are cores more functionally connected?\n",
    "How likely is that a core is functionally efficient to elicit a response in a core or others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficacy probability as 1-lag correlations\n",
    "core2core_efficacy = [] # probability\n",
    "core2other_efficacy = []\n",
    "other2core_efficacy = []\n",
    "other2other_efficacy = []\n",
    "for dyn_core in clusters_cores:\n",
    "    dyn_core_indexes = [ophys_cell_ids.index(strid) for strid in dyn_core]\n",
    "    dyn_other_indexes = list(set(ophys_cell_indexes).symmetric_difference(set(dyn_core_indexes)))\n",
    "    # selection\n",
    "    core2core_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_core_indexes,:] for conns in cid[dyn_core_indexes]] )\n",
    "    core2other_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_core_indexes,:] for conns in cid[dyn_other_indexes]] )\n",
    "    other2core_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_other_indexes,:] for conns in cid[dyn_core_indexes]] )\n",
    "    other2other_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_other_indexes,:] for conns in cid[dyn_other_indexes]] )\n",
    "\n",
    "print(\"    {:d} core2core 1-lag R: {:1.3f}±{:1.2f}\".format(len(core2core_efficacy), np.mean(core2core_efficacy),np.std(core2core_efficacy)) )\n",
    "print(\"    {:d} core2other 1-lag R: {:1.3f}±{:1.2f}\".format(len(core2other_efficacy), np.mean(core2other_efficacy),np.std(core2other_efficacy)) )\n",
    "print(\"    {:d} other2core 1-lag R: {:1.3f}±{:1.2f}\".format(len(other2core_efficacy), np.mean(other2core_efficacy),np.std(other2core_efficacy)) )\n",
    "print(\"    {:d} other2other 1-lag R: {:1.3f}±{:1.2f}\".format(len(other2other_efficacy), np.mean(other2other_efficacy),np.std(other2other_efficacy)) )\n",
    "# significativity\n",
    "kwstat,pval = stats.kruskal(core2core_efficacy, other2other_efficacy)\n",
    "print(\"    core-core vs other-other 1-lag R Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2core_efficacy, other2other_efficacy) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2core_efficacy, core2other_efficacy)\n",
    "print(\"    core-core vs core-other 1-lag R Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2core_efficacy, core2other_efficacy) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2core_efficacy, other2core_efficacy)\n",
    "print(\"    core-core vs other-core 1-lag R Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2core_efficacy, other2core_efficacy) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all spine volumes by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(0, 0.04, len(core2core_efficacy))\n",
    "plt.scatter(xs, core2core_efficacy, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(1, 0.04, len(core2other_efficacy))\n",
    "plt.scatter(xs, core2other_efficacy, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2core_efficacy))\n",
    "plt.scatter(xs, other2core_efficacy, edgecolor='silver', facecolor=('#C0C0C04d'))\n",
    "xs = np.random.normal(3, 0.04, len(other2other_efficacy))\n",
    "plt.scatter(xs, other2other_efficacy, edgecolor='silver', facecolor=('#C0C0C04d'))\n",
    "vp = ax.violinplot([core2core_efficacy,core2other_efficacy,other2core_efficacy,other2other_efficacy], [0,1,2,3], widths=0.3, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc in vp['bodies'][0:1]:\n",
    "    pc.set_facecolor('#228B224d')\n",
    "for pc in vp['bodies'][1:]:\n",
    "    pc.set_facecolor('#D3D3D34d')\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Efficacy (1-lag R)')\n",
    "plt.xticks([0, 1, 2, 3], [\"core-core\\n(n={:d})\".format(len(core2core_efficacy)), \"core-other\\n(n={:d})\".format(len(core2other_efficacy)),\"other-core\\n(n={:d})\".format(len(other2core_efficacy)),\"other-other\\n(n={:d})\".format(len(other2other_efficacy))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_efficacy.png', transparent=True, dpi=1500)\n",
    "# fig.savefig(exp_path+'/results/global_cores_others_efficacy.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mixing structural and dynamical analyses results to characterize core connectivity\n",
    "\n",
    "Here, we collect the evidence contrasting the hypothesis that core neurons are strongly connected.   \n",
    "We tested two fundamental attractor-driven assumptions:\n",
    "- synapses between cores are more numerous and stronger compared to others   \n",
    "- circuits made by cores involve more recursive connections toward cores\n",
    "\n",
    "We can take the **number** and **volume** of post-synaptic spines as proxy for their functional efficacy. \n",
    "\n",
    "But two-photon imaged neurons are few (N=112), and core neurons even fewer (3 to 8 per cluster, 35 total). Therefore their count of (proofread) spines leads to underpowered statistics.    \n",
    "We took three inclusive solutions: \n",
    "- we lowered the threshold for core identification to a minimum (participation to 60% of the events)\n",
    "- we did not differentiate spines based on their point of contact (e.g. axo-somatic, axo-dendritic, axo-axonic, etc)\n",
    "- we considered the number of spines to and from all neurons that were not part of the two-photon imaged dataset (based on the total count of spines, which lacks the proofread volume of the spines). \n",
    "\n",
    "### Core vs other to/from all spine number and volume (panel 2A)\n",
    "We want to know whether cores and others differ globally in terms of their spines.    \n",
    "Knowing the global properties of cores is relevant to assess the significance of the (underpowered) subsequent statistics limited to core vs other spine volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... postsynaptic spines on cores or others from all sources in the EM volume\")\n",
    "all2core_spine_vol = [] # µm3\n",
    "core2all_spine_vol = []\n",
    "all2other_spine_vol = []\n",
    "other2all_spine_vol = []\n",
    "\n",
    "all2core_spine_num = [] # num\n",
    "core2all_spine_num = []\n",
    "all2other_spine_num = []\n",
    "other2all_spine_num = []\n",
    "norm_all2core_spine_num = 0.0 # normalized num\n",
    "norm_core2all_spine_num = 0.0\n",
    "norm_all2other_spine_num = 0.0\n",
    "norm_other2all_spine_num = 0.0\n",
    "\n",
    "set_ids = set(ophys_cell_ids)\n",
    "for dyn_core_ids in clusters_cores:\n",
    "    dyn_other_ids = set_ids.symmetric_difference(dyn_core_ids)\n",
    "    # searching\n",
    "    \n",
    "    all2core_synapse_df = syn_df.query(f'(post_root_id in {list(dyn_core_ids)})')\n",
    "    if not all2core_synapse_df.empty:\n",
    "        all2core_spine_num.extend( all2core_synapse_df.groupby('post_root_id').size() )\n",
    "        norm_all2core_spine_num += all2core_synapse_df.groupby('post_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_core_ids))) # normalized by source*target\n",
    "\n",
    "    all2other_synapse_df = syn_df.query(f'(post_root_id in {list(dyn_other_ids)})')\n",
    "    if not all2other_synapse_df.empty:\n",
    "        all2other_spine_num.extend( all2other_synapse_df.groupby('post_root_id').size() )\n",
    "        norm_all2other_spine_num += all2other_synapse_df.groupby('post_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_other_ids)))\n",
    "\n",
    "    core2all_synapse_df = syn_df.query(f'(pre_root_id in {list(dyn_core_ids)})')\n",
    "    if not core2all_synapse_df.empty:\n",
    "        core2all_spine_num.extend( core2all_synapse_df.groupby('pre_root_id').size() )\n",
    "        norm_core2all_spine_num += core2all_synapse_df.groupby('pre_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_core_ids)))\n",
    "\n",
    "    other2all_synapse_df = syn_df.query(f'(pre_root_id in {list(dyn_other_ids)})')\n",
    "    if not other2all_synapse_df.empty:\n",
    "        other2all_spine_num.extend( other2all_synapse_df.groupby('pre_root_id').size() )\n",
    "        norm_other2all_spine_num += other2all_synapse_df.groupby('pre_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_other_ids)))\n",
    "\n",
    "    # id, pre_root_id, post_root_id, cleft_vx, spine_vol_um3\n",
    "    all2core_synapse_df = syn_spines_df.query(f'(post_root_id in {list(dyn_core_ids)})')\n",
    "    if not all2core_synapse_df.empty:\n",
    "        all2core_spine_vol.extend( all2core_synapse_df['spine_vol_um3'].tolist() )\n",
    "    all2other_synapse_df = syn_spines_df.query(f'(post_root_id in {list(dyn_other_ids)})')\n",
    "    if not all2other_synapse_df.empty:\n",
    "        all2other_spine_vol.extend( all2other_synapse_df['spine_vol_um3'].tolist() )\n",
    "    core2all_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)})')\n",
    "    if not core2all_synapse_df.empty:\n",
    "        core2all_spine_vol.extend( core2all_synapse_df['spine_vol_um3'].tolist() )\n",
    "    other2all_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)})')\n",
    "    if not other2all_synapse_df.empty:\n",
    "        other2all_spine_vol.extend( other2all_synapse_df['spine_vol_um3'].tolist() )\n",
    "        \n",
    "# number description\n",
    "print(\"    all2core spines number: {:1.3f}±{:1.2f} \".format(np.mean(all2core_spine_num),np.std(all2core_spine_num)) )\n",
    "print(\"    core2all spines number: {:1.3f}±{:1.2f} \".format(np.mean(core2all_spine_num),np.std(core2all_spine_num)) )\n",
    "print(\"    all2other spines number: {:1.3f}±{:1.2f} \".format(np.mean(all2other_spine_num),np.std(all2other_spine_num)) )\n",
    "print(\"    other2all spines number: {:1.3f}±{:1.2f} \".format(np.mean(other2all_spine_num),np.std(other2all_spine_num)) )\n",
    "# number significativity\n",
    "kwstat,pval = stats.kruskal(all2core_spine_num, all2other_spine_num)\n",
    "print(\"    all-core vs all-other spine number Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(all2core_spine_num, all2other_spine_vol) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2all_spine_num, other2all_spine_num)\n",
    "print(\"    core-all vs other-all spine number Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2all_spine_num, other2all_spine_num) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core2all_spine_num))\n",
    "plt.scatter(xs, core2all_spine_num, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2all_spine_num))\n",
    "plt.scatter(xs, other2all_spine_num, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "xs = np.random.normal(3, 0.04, len(all2core_spine_num))\n",
    "plt.scatter(xs, all2core_spine_num, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(4, 0.04, len(all2other_spine_num))\n",
    "plt.scatter(xs, all2other_spine_num, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "vp = ax.violinplot([core2all_spine_num,other2all_spine_num,all2core_spine_num,all2other_spine_num], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d','#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Spine number per cell')\n",
    "plt.yscale('log')\n",
    "plt.xticks([1,2,3,4], [\"core-all\", \"other-all\", \"all-core\", \"all-other\"])\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_num.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n",
    "# normalized spine number by type\n",
    "x = np.array([\"all-core\", \"all-other\", \"core-all\", \"other-all\"])\n",
    "y = np.array([norm_all2core_spine_num, norm_all2other_spine_num, norm_core2all_spine_num, norm_other2all_spine_num])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, color=['forestgreen','silver','forestgreen','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized number of spines')\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_normnum.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()\n",
    "\n",
    "print()\n",
    "print(\"... postsynaptic spines on cores or others from sources in the EM volume (proofread with measured volume)\")\n",
    "# volume description\n",
    "print(\"    {:d} all2core spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(all2core_spine_vol), np.mean(all2core_spine_vol),np.std(all2core_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2core_spine_vol)) )\n",
    "print(\"    {:d} core2all spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(core2all_spine_vol), np.mean(core2all_spine_vol),np.std(core2all_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2core_spine_vol)) )\n",
    "print(\"    {:d} all2other spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(all2other_spine_vol), np.mean(all2other_spine_vol),np.std(all2other_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2other_spine_vol)) )\n",
    "print(\"    {:d} other2all spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(other2all_spine_vol), np.mean(other2all_spine_vol),np.std(other2all_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2other_spine_vol)) )\n",
    "# volume significativity\n",
    "kwstat,pval = stats.kruskal(all2core_spine_vol, all2other_spine_vol)\n",
    "print(\"    all-core vs all-other spine volume Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(all2core_spine_vol, all2other_spine_vol) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2all_spine_vol, other2all_spine_vol)\n",
    "print(\"    core-all vs other-all spine volume Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2all_spine_vol, other2all_spine_vol) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core2all_spine_vol))\n",
    "plt.scatter(xs, core2all_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2all_spine_vol))\n",
    "plt.scatter(xs, other2all_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "xs = np.random.normal(3, 0.04, len(all2core_spine_vol))\n",
    "plt.scatter(xs, all2core_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(4, 0.04, len(all2other_spine_vol))\n",
    "plt.scatter(xs, all2other_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "vp = ax.violinplot([core2all_spine_vol,other2all_spine_vol,all2core_spine_vol,all2other_spine_vol], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d','#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Spine Volume (µm^3)')\n",
    "plt.xticks([1,2,3,4], [\"core-all\\n(n={:d})\".format(len(core2all_spine_vol)), \"other-all\\n(n={:d})\".format(len(other2all_spine_vol)), \"all-core\\n(n={:d})\".format(len(all2core_spine_vol)), \"all-other\\n(n={:d})\".format(len(all2other_spine_vol))])\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_vol.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core vs others  \n",
    "The number of cores and non-cores for each cluster is different. Therefore we have to normalize this count to evaluate.\n",
    "\n",
    "For each set of reproducible cluster we count:    \n",
    "- the number of synapses made by a cell type (core or not) towards others, weighted by the squared number of target cells    \n",
    "    - the expectation is that core-to-core and core-to-other synapses should be numerous in order to pull the dynamics\n",
    "- the post-synaptic spine volume of synapses made by a cell type (core or not) towards others.   \n",
    "    - the expectation is that core-to-core and core-to-other spines should be larger in order to pull the dynamics\n",
    "\n",
    "**Synapses between core neurons of each cluster are less than every other combination.**    \n",
    "Note that the resulting normalized synapse counts (for the others) check with the network density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the density of the directed graph.\n",
    "network_density = dgraph.density(loops=True)\n",
    "print(\"... network density (ratio between the edges present and the maximum number of edges that the graph can contain):\", network_density )\n",
    "# spine number\n",
    "core2core_spine_num = 0.0 # to be normalized\n",
    "core2other_spine_num = 0.0\n",
    "other2core_spine_num = 0.0\n",
    "other2other_spine_num = 0.0\n",
    "# spine volume\n",
    "core2core_spine_vol = [] # µm3\n",
    "core2other_spine_vol = []\n",
    "other2core_spine_vol = []\n",
    "other2other_spine_vol = []\n",
    "\n",
    "set_ids = set(ophys_cell_ids)\n",
    "cluster_colors = [color for color in cluster_color_array if color!='gray']\n",
    "for cluster_k,dyn_core_ids in zip(cluster_colors,clusters_cores):\n",
    "    if cluster_k=='gray':\n",
    "        continue\n",
    "    dyn_other_ids = set_ids.symmetric_difference(dyn_core_ids)\n",
    "    \n",
    "    # spine number\n",
    "    core2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    core2core_spine_num += len(core2core_synapse_df['id'].tolist())/(len(dyn_core_ids)*len(dyn_core_ids)) # normalized by source*target\n",
    "\n",
    "    core2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    core2other_spine_num += len(core2other_synapse_df['id'].tolist())/(len(dyn_core_ids)*len(dyn_other_ids)) \n",
    "\n",
    "    other2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    other2core_spine_num += len(other2core_synapse_df['id'].tolist())/(len(dyn_core_ids)*len(dyn_other_ids)) \n",
    "    \n",
    "    other2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    other2other_spine_num += len(other2other_synapse_df['id'].tolist())/(len(dyn_other_ids)*len(dyn_other_ids)) \n",
    "\n",
    "    # spine volume\n",
    "    # id, pre_root_id, post_root_id, cleft_vx, spine_vol_um3\n",
    "    core2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    if not core2core_synapse_df.empty:\n",
    "        core2core_spine_vol.extend( core2core_synapse_df['spine_vol_um3'].tolist() )\n",
    "    \n",
    "    core2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    if not core2other_synapse_df.empty:\n",
    "        core2other_spine_vol.extend( core2other_synapse_df['spine_vol_um3'].tolist() )\n",
    "    \n",
    "    other2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    if not other2core_synapse_df.empty:\n",
    "        other2core_spine_vol.extend( other2core_synapse_df['spine_vol_um3'].tolist() )\n",
    " \n",
    "    other2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    if not other2other_synapse_df.empty:\n",
    "        other2other_spine_vol.extend( other2other_synapse_df['spine_vol_um3'].tolist() )\n",
    "\n",
    "# description\n",
    "# number\n",
    "print(\"... Normalized number of spines\")\n",
    "print(\"    {:f} core2core normalized spines number\".format((core2core_spine_num)) )\n",
    "print(\"    {:f} core2other normalized spines number\".format((core2other_spine_num)) )\n",
    "print(\"    {:f} other2core normalized spines number\".format((other2core_spine_num)) )\n",
    "print(\"    {:f} other2other normalized spines number\".format((other2other_spine_num)) )\n",
    "\n",
    "# spines\n",
    "print(\"... Spine volumes\")\n",
    "print(\"    {:d} core2core spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(core2core_spine_vol), np.mean(core2core_spine_vol),np.std(core2core_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(core2core_spine_vol)) )\n",
    "print(\"    {:d} core2other spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(core2other_spine_vol), np.mean(core2other_spine_vol),np.std(core2other_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(core2other_spine_vol)) )\n",
    "print(\"    {:d} other2core spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(other2core_spine_vol), np.mean(other2core_spine_vol),np.std(other2core_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(other2core_spine_vol)) )\n",
    "print(\"    {:d} other2other spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(other2other_spine_vol), np.mean(other2other_spine_vol),np.std(other2other_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(other2other_spine_vol)) )\n",
    "\n",
    "# significativity\n",
    "# this is just to test the significativity if the number of samples was correct\n",
    "kwstat,pval = stats.kruskal(other2core_spine_vol, other2other_spine_vol)\n",
    "print(\"   core vs other spine size Kruskal-Wallis test results:\",kwstat,pval)\n",
    "\n",
    "# plotting\n",
    "# all spine number by type\n",
    "x = np.array([\"core-core\", \"core-other\", \"other-core\", \"other-other\"])\n",
    "y = np.array([core2core_spine_num, core2other_spine_num, other2core_spine_num, other2other_spine_num])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, color=['forestgreen','forestgreen','silver','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized number of spines')\n",
    "fig.savefig(exp_path+'/results/global_cores_others_spine_num.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()\n",
    "\n",
    "# normalized spine number by type\n",
    "x = np.array([\"all\\ncore\", \"all\\nother\", \"core\\nall\", \"other\\nall\", \"core\\ncore\", \"core\\nother\", \"other\\ncore\", \"other\\nother\"])\n",
    "y = np.array([norm_all2core_spine_num, norm_all2other_spine_num, norm_core2all_spine_num, norm_other2all_spine_num, core2core_spine_num, core2other_spine_num, other2core_spine_num, other2other_spine_num])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, width=0.5, color=['forestgreen','silver','forestgreen','silver','forestgreen','forestgreen','silver','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Connection probability (relative spine freq.)')\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_normnum.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# all spine volumes by type\n",
    "fig, ax = plt.subplots()\n",
    "# xs = np.random.normal(0, 0.04, len(core2core_spine_vol))\n",
    "# plt.scatter(xs, core2core_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(1, 0.04, len(core2other_spine_vol))\n",
    "plt.scatter(xs, core2other_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2core_spine_vol))\n",
    "plt.scatter(xs, other2core_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "xs = np.random.normal(3, 0.04, len(other2other_spine_vol))\n",
    "plt.scatter(xs, other2other_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "# vp = ax.violinplot([core2core_spine_vol,core2other_spine_vol,other2core_spine_vol,other2other_spine_vol], [0,1,2,3], widths=0.3, showextrema=False, showmedians=True)\n",
    "vp = ax.violinplot([core2other_spine_vol,other2core_spine_vol,other2other_spine_vol], [1,2,3], widths=0.3, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc in vp['bodies'][0:1]:\n",
    "    pc.set_facecolor('#228B224d')\n",
    "for pc in vp['bodies'][1:]:\n",
    "    pc.set_facecolor('#D3D3D34d')\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Spine Volume (µm^3)')\n",
    "plt.xticks([0, 1, 2, 3], [ \"core-core\\n(n={:d})\".format(len(core2core_spine_vol)),\"core-other\\n(n={:d})\".format(len(core2other_spine_vol)),\"other-core\\n(n={:d})\".format(len(other2core_spine_vol)),\"other-other\\n(n={:d})\".format(len(other2other_spine_vol))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_spine_vol.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Ca-imaged and outside EM volume inputs (panel 2AB)\n",
    "\n",
    "Core responses could be due to non-imaged and outside volume sources. How can we rule this out (or reduce our lack of knowledge)?   \n",
    "We can ask *Are there more or stronger spines made by non-imaged neurons (either local or far) on cores or others?*   \n",
    "We have this information since we know the cell ID of all somas in the volume. We can take the spines having presynaptic ID different from the known Ca-imaged IDs or different from the somas within the EM volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are core more mutually connected than others?\n",
    "\n",
    "We started by asking whether a global measure such as assortativity - - gives a clear summary of mutuality between all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('... assortativity')\n",
    "print('    preparing vertex labels for cores and others')\n",
    "dgraph.vs[\"ophys_cell_id\"] = ophys_cell_ids\n",
    "is_id_core = np.array( [0] * len(ophys_cell_ids) )\n",
    "is_id_core[core_indexes] = 1\n",
    "dgraph.vs[\"is_core\"] = is_id_core.tolist()\n",
    "pyc_ca_syn_df = syn_df.query(f'(pre_root_id in {ophys_cell_ids}) and (post_root_id in {ophys_cell_ids})')\n",
    "is_syn_core = np.array( [0] * len(pyc_ca_syn_df) )\n",
    "for cid in [item for sublist in clusters_cores for item in sublist]:\n",
    "    is_syn_core[pyc_ca_syn_df['pre_root_id'] == cid] = 1\n",
    "dgraph.es[\"is_core\"] = is_syn_core.tolist()\n",
    "\n",
    "# is a preference for a network's nodes to attach to others that are similar in some way\n",
    "print(\"    overall:\", dgraph.assortativity_nominal(\"is_core\", directed=True) )\n",
    "# cores degree distro vs others degree distro\n",
    "# biological networks typically show negative assortativity, or disassortative mixing, or disassortativity, as high degree nodes tend to attach to low degree nodes.\n",
    "print(\"    assortativity degree:\", dgraph.assortativity_degree(directed=True) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Motif connectivity of cores and others (panel 2D)\n",
    "\n",
    "This measure of the network reports the participation of cores (or non-cores) in triplet motifs.    \n",
    "Note that the triplets are not exclusively made of cores (or non-cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each set of reproducible cluster cores we count their connectivity motifs.\n",
    "set_indexes = set(ophys_cell_indexes)\n",
    "for dyn_core_ids in clusters_cores:\n",
    "    dyn_core_indexes = set([ophys_cell_ids.index(strid) for strid in dyn_core_ids])\n",
    "    dyn_other_indexes = set_indexes.symmetric_difference(dyn_core_indexes)\n",
    "    for mclass, mlist in motif_vertices.items():\n",
    "        for mtriplet in mlist:\n",
    "            intersection_cores = len(list(dyn_core_indexes.intersection(mtriplet)))\n",
    "            intersection_others = len(list(dyn_other_indexes.intersection(mtriplet)))\n",
    "            global_structural_motif_cores[mclass] += intersection_cores\n",
    "            global_structural_motif_others[mclass] += intersection_others\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar(global_structural_motif_cores.keys(), global_structural_motif_cores.values(), color='forestgreen')\n",
    "plt.ylabel('cores occurrences')\n",
    "plt.yscale('log')\n",
    "plt.ylim([0.7,plt.ylim()[1]])\n",
    "plt.xlabel('motifs types')\n",
    "fig.savefig(exp_path+'/results/global_motifs_cores.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "fig = plt.figure()\n",
    "plt.bar(global_structural_motif_others.keys(), global_structural_motif_others.values(), color='silver')\n",
    "plt.ylabel('non-cores occurrences')\n",
    "plt.yscale('log')\n",
    "plt.ylim([0.7,plt.ylim()[1]])\n",
    "plt.xlabel('motifs types')\n",
    "fig.savefig(exp_path+'/results/global_motifs_others.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "print(\"... saved mutual connectivity of cores and others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgraph is already defined from the structural_analysis included file\n",
    "print(\"    graph diameter (#vertices):\", dgraph.diameter(directed=True, unconn=True, weights=None))\n",
    "print(\"    graph average path length (#vertices):\", dgraph.average_path_length(directed=True, unconn=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality of cores\n",
    "\n",
    "If cores are not more mutually connected compared to others, then what is their characterizing feature?    \n",
    "In the cells above, we saw indications of more interconnections between cores and others than within the same type.     \n",
    "This could hint at some form of centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree centrality of cores is not different from others (panel 2E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('... degree centrality')\n",
    "degree_centrality_cores = dgraph.degree(core_indexes, mode='out', loops=True)\n",
    "degree_centrality_others = dgraph.degree(other_indexes, mode='out', loops=True)\n",
    "# description\n",
    "print(\"    cores: \"+str(stats.describe(degree_centrality_cores)) )\n",
    "print(\"    others: \"+str(stats.describe(degree_centrality_others)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(degree_centrality_cores, degree_centrality_others, equal_var=False))\n",
    "d,_ = stats.ks_2samp(degree_centrality_cores, degree_centrality_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(degree_centrality_cores))\n",
    "plt.scatter(xs, degree_centrality_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(degree_centrality_others))\n",
    "plt.scatter(xs, degree_centrality_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([degree_centrality_cores,degree_centrality_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Degree')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(degree_centrality_cores)), \"other\\n(n={:d})\".format(len(degree_centrality_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_degree.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Betweenness cenrality of cores is not different from others (panel 2F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('... betweenness')\n",
    "cores_betweenness = np.array(dgraph.betweenness(vertices=core_indexes, directed=True))\n",
    "others_betweenness = np.array(dgraph.betweenness(vertices=other_indexes, directed=True))\n",
    "print(\"    cores: \"+str(stats.describe(cores_betweenness)) )\n",
    "print(\"    others: \"+str(stats.describe(others_betweenness)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(cores_betweenness, others_betweenness, equal_var=False))\n",
    "d,_ = stats.ks_2samp(cores_betweenness, others_betweenness) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(cores_betweenness))\n",
    "plt.scatter(xs, cores_betweenness, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(others_betweenness))\n",
    "plt.scatter(xs, others_betweenness, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([cores_betweenness,others_betweenness], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "# plt.yscale('log')\n",
    "# plt.ylim([0.00001,plt.ylim()[1]])\n",
    "plt.ylabel('Betweenness')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(cores_betweenness)), \"other\\n(n={:d})\".format(len(others_betweenness))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_betweenness.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hub scores of cores is not different from others (panel 2G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"... authority score\")\n",
    "# what is the overlap of cores and hubs?\n",
    "# Hub\n",
    "authority_scores = np.array(dgraph.authority_score(weights=None, scale=True, return_eigenvalue=False))\n",
    "authority_scores_cores = authority_scores[core_indexes]\n",
    "authority_scores_others = authority_scores[other_indexes]\n",
    "print(\"    authority cores: \"+str(stats.describe(authority_scores_cores)) )\n",
    "print(\"    authority others: \"+str(stats.describe(authority_scores_others)) )\n",
    "# significativity\n",
    "print(\"    Kruskal-Wallis test:  %.3f p= %.3f\" % stats.ttest_ind(authority_scores_cores, authority_scores_others))\n",
    "d,_ = stats.ks_2samp(authority_scores_cores, authority_scores_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all eccentricity by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(authority_scores_cores))\n",
    "plt.scatter(xs, authority_scores_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(authority_scores_others))\n",
    "plt.scatter(xs, authority_scores_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([authority_scores_cores,authority_scores_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Hub score')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(authority_scores_cores)), \"other\\n(n={:d})\".format(len(authority_scores_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_authority_score.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n",
    "print(\"... hub score\")\n",
    "# what is the overlap of cores and hubs?\n",
    "# Hub\n",
    "hub_scores = np.array(dgraph.hub_score(weights=None, scale=True, return_eigenvalue=False))\n",
    "hub_scores_cores = hub_scores[core_indexes]\n",
    "hub_scores_others = hub_scores[other_indexes]\n",
    "print(\"    hub cores: \"+str(stats.describe(hub_scores_cores)) )\n",
    "print(\"    hub others: \"+str(stats.describe(hub_scores_others)) )\n",
    "# significativity\n",
    "print(\"    Kruskal-Wallis test:  %.3f p= %.3f\" % stats.ttest_ind(hub_scores_cores, hub_scores_others))\n",
    "d,_ = stats.ks_2samp(hub_scores_cores, hub_scores_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all eccentricity by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(hub_scores_cores))\n",
    "plt.scatter(xs, hub_scores_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(hub_scores_others))\n",
    "plt.scatter(xs, hub_scores_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([hub_scores_cores,hub_scores_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Hub score')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(hub_scores_cores)), \"other\\n(n={:d})\".format(len(hub_scores_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_hub_score.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cores control the flow of cortical activity\n",
    "\n",
    "So far, we used structural (graph) measures of neurons selected by looking at their reproducibility (a form of regular activity). In a sense, we were already crossing structural and dynamical information about the network.    \n",
    "\n",
    "However, we could push this further.    \n",
    "\n",
    "### Structural underpinnings of clusters (panel 2H)\n",
    "\n",
    "What is the origin of pattern reproducibility?    \n",
    "We can look at **how** the underlying connectivity structure supports each events activities.    \n",
    "\n",
    "We can consider each event as a _network flow problem_, in which the activity can be transferred through cells along the available connections.    \n",
    "\n",
    "For each event, we compute the max flow between the cells IDs according to their firing sequence.      \n",
    "**Do core neurons sustain more flow compared to others?**\n",
    "\n",
    "And we also consider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# each edge has a capacity and each edge receives a flow. \n",
    "# The amount of flow on an edge cannot exceed the capacity of the edge.\n",
    "# therefore, edges with high capacity will be more important for the flow.\n",
    "# here we test the hypothesis that edges towards cores have higher capacity\n",
    "# or that the sum of edges towards cores have a higher total capacity\n",
    "cell_total_capacity = {cid:list() for cid in ophys_cell_ids}\n",
    "edges_sourcing = {cid:0 for cid in ophys_cell_ids}\n",
    "edges_targeting = {cid:0 for cid in ophys_cell_ids}\n",
    "\n",
    "for cluster_k,events_cellids in sorted_events_cidlist.items():\n",
    "    if cluster_k == 'gray':\n",
    "        continue\n",
    "\n",
    "    for vnt in events_cellids:\n",
    "        for posi,vidj in enumerate(vnt[1:]):\n",
    "            vidi = vnt[posi] # enumerate will go from 0\n",
    "            # print(vidi, vidj)\n",
    "\n",
    "            # check beginning and end are not the same\n",
    "            if dgraph.vs.find(ophys_cell_id=vidi).index == dgraph.vs.find(ophys_cell_id=vidj).index:\n",
    "                continue\n",
    "            # # check there is a path between the two\n",
    "            # if len(spinesgraph.get_all_shortest_paths(spinesgraph.vs.find(name=vidi).index, to=spinesgraph.vs.find(name=vidj).index, weights=None, mode='out'))>0:\n",
    "            #     continue\n",
    "\n",
    "            # Take the maximum flow between the previous and next vertices\n",
    "            mfres = dgraph.maxflow(dgraph.vs.find(ophys_cell_id=vidi).index, dgraph.vs.find(ophys_cell_id=vidj).index)\n",
    "            # print(mfres)\n",
    "            # returns a tuple containing the following:\n",
    "            # graph - the graph on which this flow is defined\n",
    "            # value - the value (capacity) of the maximum flow between the given vertices\n",
    "            # flow - the flow values on each edge. For directed graphs, this is simply a list where element i corresponds to the flow on edge i.\n",
    "            # cut - edge IDs in the minimal cut corresponding to the flow.\n",
    "            # partition - vertex IDs in the parts created after removing edges in the cut\n",
    "            # es - an edge selector restricted to the edges in the cut.\n",
    "\n",
    "            # we get a flow value for each edge contributing to the flow.\n",
    "            # source\n",
    "            mfres_value = mfres.value\n",
    "            if vidi in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                mfres_value /= len(core_indexes)\n",
    "            else:\n",
    "                mfres_value /= len(other_indexes)\n",
    "            cell_total_capacity[vidi].append(mfres_value)\n",
    "            # target\n",
    "            mfres_value = mfres.value\n",
    "            if vidj in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                mfres_value /= len(core_indexes)\n",
    "            else:\n",
    "                mfres_value /= len(other_indexes)\n",
    "            cell_total_capacity[vidj].append(mfres_value)\n",
    "            \n",
    "            # Iterate over the edges identified by the flow.\n",
    "            # count the edges sourcing from cores, and those targeting cores. Which is more?\n",
    "            for edge in mfres.es:\n",
    "                sourceid = int(dgraph.vs[edge.source]['ophys_cell_id'])\n",
    "                targetid = int(dgraph.vs[edge.target]['ophys_cell_id'])\n",
    "                if sourceid in cell_total_capacity.keys():\n",
    "                    edges_sourcing[sourceid] +=1 # just count\n",
    "                if targetid in cell_total_capacity.keys():\n",
    "                    edges_targeting[targetid] +=1 # just count\n",
    "\n",
    "# Flow\n",
    "# print(cell_total_capacity)\n",
    "flowvalue_cores = []\n",
    "for cid in np.array(ophys_cell_ids)[core_indexes]:\n",
    "    flowvalue_cores.extend(cell_total_capacity[cid])\n",
    "flowvalue_others = []\n",
    "for cid in np.array(ophys_cell_ids)[other_indexes]:\n",
    "    flowvalue_others.extend(cell_total_capacity[cid])\n",
    "\n",
    "# description\n",
    "print(\"    Flow cores: \"+str(stats.describe(flowvalue_cores)) )\n",
    "print(\"    Flow others: \"+str(stats.describe(flowvalue_others)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowvalue_cores, flowvalue_others, equal_var=False))\n",
    "d,_ = stats.ks_2samp(flowvalue_cores, flowvalue_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(flowvalue_cores))\n",
    "plt.scatter(xs, flowvalue_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(flowvalue_others))\n",
    "plt.scatter(xs, flowvalue_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([flowvalue_cores,flowvalue_others], widths=0.15, showextrema=False, showmeans=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmeans'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized flow value')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(flowvalue_cores)), \"other\\n(n={:d})\".format(len(flowvalue_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_flowvalue.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n",
    "print()\n",
    "# Cuts\n",
    "# print(edges_sourcing)\n",
    "# print(edges_targeting)\n",
    "flowcuts_core_sources = []\n",
    "flowcuts_core_targets = []\n",
    "for cid in np.array(ophys_cell_ids)[core_indexes]:\n",
    "    flowcuts_core_sources.append(edges_sourcing[cid]/len(core_indexes))\n",
    "    flowcuts_core_targets.append(edges_targeting[cid]/len(core_indexes))\n",
    "flowcuts_other_sources = []\n",
    "flowcuts_other_targets = []\n",
    "for cid in np.array(ophys_cell_ids)[other_indexes]:\n",
    "    flowcuts_other_sources.append(edges_sourcing[cid]/len(other_indexes))\n",
    "    flowcuts_other_targets.append(edges_targeting[cid]/len(other_indexes))\n",
    "\n",
    "# description\n",
    "print(\"    Cut edges sourcing from cores: \"+str(stats.describe(flowcuts_core_sources)) )\n",
    "print(\"    Cut edges targeting cores: \"+str(stats.describe(flowcuts_core_targets)) )\n",
    "print(\"    Cut edges sourcing from others: \"+str(stats.describe(flowcuts_other_sources)) )\n",
    "print(\"    Cut edges targeting others: \"+str(stats.describe(flowcuts_other_targets)) )\n",
    "# significativity\n",
    "print(\"    Core targets vs sources Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowcuts_core_targets, flowcuts_core_sources, equal_var=False))\n",
    "d,_ = stats.ks_2samp(flowcuts_core_targets, flowcuts_core_sources) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "print(\"    Core targets vs Other targets Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowcuts_core_targets, flowcuts_other_targets, equal_var=False))\n",
    "d,_ = stats.ks_2samp(flowcuts_core_targets, flowcuts_other_targets) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(flowcuts_core_sources))\n",
    "plt.scatter(xs, flowcuts_core_sources, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(flowcuts_core_targets))\n",
    "plt.scatter(xs, flowcuts_core_targets, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(3, 0.04, len(flowcuts_other_sources))\n",
    "plt.scatter(xs, flowcuts_other_sources, alpha=0.3, c='silver')\n",
    "xs = np.random.normal(4, 0.04, len(flowcuts_other_targets))\n",
    "plt.scatter(xs, flowcuts_other_targets, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([flowcuts_core_sources,flowcuts_core_targets,flowcuts_other_sources,flowcuts_other_targets], widths=0.15, showextrema=False, showmeans=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc in vp['bodies'][0:2]:\n",
    "    pc.set_facecolor('#228B224d')\n",
    "for pc in vp['bodies'][2:]:\n",
    "    pc.set_facecolor('#D3D3D34d')\n",
    "vp['cmeans'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized edges in the cut')\n",
    "plt.xticks([1, 2, 3, 4], [\"core as\\nsource\", \"core as\\ntarget\", \"other as\\nsource\", \"other as\\ntarget\"])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_cutvalue.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cores are targets of multiple paths\n",
    "\n",
    "If cores are more often than others part of the paths, it means that they might not be central by virtue of their degree, but by how many event trajectory path (not just any path as in the betweenness, or hubness) pass through them.     \n",
    "More in detail, cores are important because they are more often the target of cut flow edges. The **pagerank** - where a node rank is proportional to the total rank of the other nodes pointing to it - is a way to measure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('... PageRank centrality')\n",
    "pagerank_cores = np.array(dgraph.personalized_pagerank(vertices=core_indexes, directed=True, damping=0.85, reset=\"is_core\"))\n",
    "pagerank_others = np.array(dgraph.personalized_pagerank(vertices=other_indexes, directed=True, damping=0.85, reset=\"is_core\"))\n",
    "# description\n",
    "print(\"    cores: \"+str(stats.describe(pagerank_cores)) )\n",
    "print(\"    others: \"+str(stats.describe(pagerank_others)) )\n",
    "# significativity\n",
    "print(\"    Kruskal-Wallis test:  %.3f p= %.3f\" % stats.kruskal(pagerank_cores, pagerank_others))\n",
    "d,_ = stats.ks_2samp(pagerank_cores, pagerank_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(pagerank_cores))\n",
    "plt.scatter(xs, pagerank_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(pagerank_others))\n",
    "plt.scatter(xs, pagerank_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([pagerank_cores,pagerank_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('PageRank')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(pagerank_cores)), \"other\\n(n={:d})\".format(len(pagerank_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_pagerank.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between structural and dynamical cores\n",
    "\n",
    "### Hierarchical modularity\n",
    "The log-log linear relationship between Clustering coefficient and degree is plotted in the file `hierarchical modularity.png` (and .svg) produced by the `structural_analysis.py` and already stored in the `results` folder.\n",
    "\n",
    "### Bow-tie structure of modules\n",
    "Local bow-ties analysis as in Fujita et al. 2019.\n",
    "Identify communities based on (multiple trials) random walks as information flows.    \n",
    "With very sparsely connected networks, as MICrONS, the library igraph finds only one module (see [here](https://stackoverflow.com/questions/20364939/community-detection-with-infomap-algorithm-producing-one-massive-module)). Using the same algorithm (with teleportation) from the InfoMap authors (see [here](https://mapequation.github.io/infomap/python/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infomap import Infomap # with teleportation to ensure no local solution\n",
    "im = Infomap(no_self_links=True, flow_model=\"directed\", seed=2**32-2, prefer_modular_solution=True)\n",
    "im.add_networkx_graph( dgraph.to_networkx() ) # infomap accepts only networkx format\n",
    "print(\"    starting infomap analysis\")\n",
    "im.run()\n",
    "print(f\"    found {im.num_top_modules} modules with codelength: {im.codelength:.4f}  entropy: {im.entropy_rate:.4f}\")\n",
    "previous_id = 1\n",
    "communities_tot = []\n",
    "communities_lens = []\n",
    "community = []\n",
    "structural_cores = []\n",
    "for node_id, module_id in sorted(im.modules, key=lambda x: x[1]):\n",
    "    if module_id>previous_id: # simple module handling\n",
    "        community_graph = dgraph.subgraph(community) # community contains the indexes in dgraph\n",
    "        imcommunity = Infomap(no_self_links=True, flow_model=\"directed\", seed=2**32-2, prefer_modular_solution=True, silent=True)\n",
    "        imcommunity.add_networkx_graph( community_graph.to_networkx() )\n",
    "        imcommunity.run()\n",
    "        # unspecific submodule specification\n",
    "        if imcommunity.num_non_trivial_top_modules > 2:\n",
    "            communities_lens.append(len(community))\n",
    "        communities_tot.append(len(community))\n",
    "        # get central community cores\n",
    "        community_cores = []\n",
    "        for imnode, immodules in imcommunity.get_multilevel_modules().items():\n",
    "            if immodules[0]==1: # only the center\n",
    "                community_cores.append(imnode)\n",
    "        structural_cores.append(community_cores)\n",
    "        # simple module handling\n",
    "        previous_id=module_id\n",
    "        community = []\n",
    "    # print(node_id, module_id)\n",
    "    community.append(node_id)\n",
    "print(\"    bow-tie score:\", len(communities_lens)/len(communities_tot))\n",
    "print(\"    communities lens:\",stats.describe(communities_lens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL) Random rewiring to test bow-tie score\n",
    "\n",
    "The EM data gives a certain bow-tie score.    \n",
    "To see whether the score can be improved or worsen by different connectivities, we can rewire at random (easy), and study their statistics.   \n",
    "**Uncomment the lines below to run the (rather long) rewiring tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewired_bowtie_score = {}\n",
    "# for rewireprob in np.linspace(0.01, 0.05, num=10):\n",
    "#     print(\"    \\nrewiring probability:\",rewireprob)\n",
    "#     rewired_bowtie_score[rewireprob] = []\n",
    "#\n",
    "#     for trial in range(0,10):\n",
    "#         rewired_graph = dgraph.copy()\n",
    "#\n",
    "#         rewired_graph.rewire_edges(prob=rewireprob, loops=False, multiple=True) # in place!\n",
    "#\n",
    "#         # Clustering Coefficient of only excitatory cells\n",
    "#         local_clustering_coefficients = np.array(rewired_graph.transitivity_local_undirected(vertices=None, mode=\"zero\"))\n",
    "#         # plot\n",
    "#         paramsfit = [2, 1.05] # 334 EM-only all proofread\n",
    "#         pfit = powerlaw(degrees, *paramsfit)\n",
    "#         fig = plt.figure()\n",
    "#         summer = mpcm.summer\n",
    "#         plt.scatter( degrees,local_clustering_coefficients, marker='o', facecolor='#111111', s=50, edgecolors='none', alpha=0.5) #\n",
    "#         plt.plot(degrees,pfit,c='k')\n",
    "#         plt.yscale('log')\n",
    "#         plt.xscale('log')\n",
    "#         ax = plt.gca()\n",
    "#         ax.spines['top'].set_visible(False)\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         plt.ylabel('LCC')\n",
    "#         plt.xlabel('degree')\n",
    "#         plt.tick_params(axis='both', bottom='on', top='on', left='off', right='off')\n",
    "#         plt.tight_layout()\n",
    "#         #fig.savefig(exp_path+'/results/rewiring/hierarchical_modularity'+str(rewireprob)+str(trial)+'.png', transparent=True, dpi=900)\n",
    "#         fig.savefig(exp_path+'/results/rewiring/hierarchical_modularity'+str(rewireprob)+\"_\"+str(trial)+'.svg', transparent=True)\n",
    "#         plt.close()\n",
    "#         fig.clf()\n",
    "#\n",
    "#         # Local bow-ties analysis as in FujitaKichikawaFujiwaraSoumaIyetomi2019\n",
    "#         from infomap import Infomap\n",
    "#         im = Infomap(silent=True, no_self_links=True, flow_model=\"directed\", seed=2**32-1, core_loop_limit=10, prefer_modular_solution=True, inner_parallelization=True, num_trials=10)\n",
    "#         im.add_networkx_graph( rewired_graph.to_networkx() ) # infomap accepts only networkx format\n",
    "#         im.run()\n",
    "#         previous_id = 1\n",
    "#         communities_tot = []\n",
    "#         communities_lens = []\n",
    "#         community = []\n",
    "#         for node_id, module_id in sorted(im.modules, key=lambda x: x[1]):\n",
    "#             if module_id>previous_id: # simple module handling\n",
    "#                 community_graph = rewired_graph.subgraph(community) # community contains the indexes in rewired_graph\n",
    "#                 imcommunity = Infomap(no_self_links=True, flow_model=\"directed\", seed=2**32-1, core_loop_limit=10, prefer_modular_solution=True, silent=True, num_trials=10)\n",
    "#                 imcommunity.add_networkx_graph( community_graph.to_networkx() )\n",
    "#                 imcommunity.run()\n",
    "#                 if imcommunity.num_non_trivial_top_modules > 2:\n",
    "#                     communities_lens.append(len(community))\n",
    "#                 communities_tot.append(len(community))\n",
    "#                 previous_id=module_id\n",
    "#                 community = []\n",
    "#             community.append(node_id)\n",
    "#         if len(communities_tot)<5:\n",
    "#             continue\n",
    "#         bowtie_score = len(communities_lens)/len(communities_tot)\n",
    "#         print(\"    trial:\", trial, \"score:\",bowtie_score)\n",
    "#         rewired_bowtie_score[rewireprob].append( bowtie_score )\n",
    "#\n",
    "# for rwiredk, rewiredv in rewired_bowtie_score.items():\n",
    "#     print(\"dgraph rewired with prob:\",rwiredk)\n",
    "#     print(\"    bow-tie score avg:\", stats.describe(rewiredv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap between structural and dynamical clusters\n",
    "\n",
    "Check the consistency of our hypothesis chain by looking at the overlap between dynamically-identified core neurons – those reliably participating in multiple clustered population events – and structurally-identified core neurons – those repetitively found in bow-tie modules.    \n",
    "(it requires the file with dynamical cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./results/clusters_cores.npy'):\n",
    "    clusters_cores = np.load('./results/clusters_cores.npy', allow_pickle=True)\n",
    "    print(\"... loaded:\", clusters_cores.shape)\n",
    "    # print(clusters_cores)\n",
    "    # print(clusters_cores.shape)\n",
    "\n",
    "    core_indexes = []\n",
    "    other_indexes = []\n",
    "    clusters_cores_indexes = []\n",
    "    for dyn_core in clusters_cores:\n",
    "        core_indexes.extend( [ophys_cell_ids.index(strid) for strid in dyn_core] )\n",
    "        clusters_cores_indexes.append( [ophys_cell_ids.index(strid) for strid in dyn_core] )\n",
    "    core_indexes = np.unique(core_indexes)\n",
    "    print(\"    # cores:\",len(core_indexes))\n",
    "    other_indexes = [i for i in range(len(ophys_cell_ids)) if i not in core_indexes]\n",
    "    print(\"    # non-cores:\",len(other_indexes))\n",
    "\n",
    "    print(\"... overlap between structural cores and dynamical cores\")\n",
    "    # for each set of dynamical cores from a cluster\n",
    "    # we compare it with all the sets of structral core nodes identified by the InfoMap (and take the max to avoid duplicates)\n",
    "    overlapSD = {}\n",
    "    overlap_ratio = []\n",
    "    ccs_len = []\n",
    "    scs_len = []\n",
    "    for iccs,ccs in enumerate(clusters_cores_indexes):\n",
    "        ccs_len.append(len(ccs))\n",
    "        kccs = \"{}_{}_\".format(iccs,len(ccs))\n",
    "        overlapSD[kccs+\"lens\"] = []\n",
    "        overlapSD[kccs+\"ratio\"] = []\n",
    "        for scs in structural_cores:\n",
    "            scs_len.append(len(scs))\n",
    "            overlapSD[kccs+\"lens\"].append( \"{}/{}\".format( len(set(ccs)&set(scs)), len(ccs) ) )\n",
    "            overlapSD[kccs+\"ratio\"].append( len(set(ccs)&set(scs))/len(ccs) )\n",
    "        # print(len(scs),len(ccs))\n",
    "        # choose which structural_cores better matches\n",
    "        index_max = max(range(len(overlapSD[kccs+\"ratio\"])), key=overlapSD[kccs+\"ratio\"].__getitem__)\n",
    "        print(overlapSD[kccs+\"ratio\"][index_max], overlapSD[kccs+\"lens\"][index_max])\n",
    "        overlap_ratio.append(overlapSD[kccs+\"ratio\"][index_max])\n",
    "    print(stats.describe(overlap_ratio))\n",
    "\n",
    "    # plot all ratio\n",
    "    fig, ax = plt.subplots()\n",
    "    xs = np.random.normal(1, 0.04, len(overlap_ratio))\n",
    "    plt.scatter(xs, overlap_ratio, alpha=0.3, c='gray', edgecolors='none')\n",
    "    vp = ax.violinplot([overlap_ratio], widths=0.15, showextrema=False, showmeans=True)\n",
    "    for pc in vp['bodies']:\n",
    "        pc.set_edgecolor('black')\n",
    "    for pc,cb in zip(vp['bodies'],['black']):\n",
    "        pc.set_facecolor(cb)\n",
    "    vp['cmeans'].set_color('orange')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.ylim([0,0.7])\n",
    "    plt.ylabel('Overlap ratio')\n",
    "    fig.set_figwidth(1.5)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('./results/dynamical_structural_cores.svg', transparent=True)\n",
    "    plt.close()\n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Supplementary figure 3\n",
    "   \n",
    "To have keep cores within the attractor framework, cores activity could be sustained by indirect synaptic feedback, through highly connected secondary paths.   \n",
    "To back up the attractor idea, one would expect that core neurons would have shorter paths or cycles, compared to others. \n",
    "\n",
    "### Shortest paths of cores and others (panel S3A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... number of paths in a complete graph of the same size:\", (np.math.factorial(112-2)*np.e))\n",
    "print('... number of shortest paths between cores')\n",
    "core_shortestpaths = []\n",
    "for coreidx in core_indexes:\n",
    "    othercores = list(core_indexes)\n",
    "    othercores.remove(coreidx)\n",
    "    shrtpth = dgraph.get_shortest_paths(coreidx, to=othercores, weights=None, mode='out', output='vpath')\n",
    "    for strp in shrtpth:\n",
    "        core_shortestpaths.append(len(strp))\n",
    "other_shortestpaths = []\n",
    "for otheridx in other_indexes:\n",
    "    otherothers = list(other_indexes)\n",
    "    otherothers.remove(otheridx)\n",
    "    shrtpth = dgraph.get_shortest_paths(otheridx, to=otherothers, weights=None, mode='out', output='vpath')\n",
    "    for strp in shrtpth:\n",
    "        other_shortestpaths.append(len(strp))\n",
    "print(\"    cores shortest paths: \"+str(stats.describe(core_shortestpaths)) )\n",
    "print(\"    others shortest paths: \"+str(stats.describe(other_shortestpaths)) )\n",
    "print(\"    equal variances? \"+str(stats.levene(core_shortestpaths, other_shortestpaths)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(core_shortestpaths, other_shortestpaths, equal_var=False))\n",
    "d,_ = stats.ks_2samp(core_shortestpaths, other_shortestpaths) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core_shortestpaths))\n",
    "plt.scatter(xs, core_shortestpaths, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(other_shortestpaths))\n",
    "plt.scatter(xs, other_shortestpaths, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([core_shortestpaths,other_shortestpaths], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Shortest path length')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(core_shortestpaths)), \"other\\n(n={:d})\".format(len(other_shortestpaths))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_shortestpath.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycles between cores or others (panel S3B)\n",
    "\n",
    "Cycles are built starting from a core (or other) and iterating neighbors of different lenghts, where the last vertex is the starting one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('... cycles')\n",
    "# breadth first search of paths and unique cycles\n",
    "def get_cycles(adj, paths, maxlen):\n",
    "    # tracking the actual path length:\n",
    "    maxlen -= 1\n",
    "    nxt_paths = []\n",
    "    # iterating over all paths:\n",
    "    for path in paths['paths']:\n",
    "        # iterating neighbors of the last vertex in the path:\n",
    "        for nxt in adj[path[-1]]:\n",
    "            # attaching the next vertex to the path:\n",
    "            nxt_path = path + [nxt]\n",
    "            if path[0] == nxt and min(path) == nxt:\n",
    "                # the next vertex is the starting vertex, we found a cycle\n",
    "                # we keep the cycle only if the starting vertex has the\n",
    "                # lowest vertex id, to avoid having the same cycles\n",
    "                # more than once\n",
    "                paths['cycles'].append(nxt_path)\n",
    "                # if you don't need the starting vertex\n",
    "                # included at the end:\n",
    "                # paths$cycles <- c(paths$cycles, list(path))\n",
    "            elif nxt not in path:\n",
    "                # keep the path only if we don't create\n",
    "                # an internal cycle in the path\n",
    "                nxt_paths.append(nxt_path)\n",
    "    # paths grown by one step:\n",
    "    paths['paths'] = nxt_paths\n",
    "    if maxlen == 0:\n",
    "        # the final return when maximum search length reached\n",
    "        return paths\n",
    "    else:\n",
    "        # recursive return, to grow paths further\n",
    "        return get_cycles(adj, paths, maxlen)\n",
    "# Comparison of core based cycles vs other based cycles\n",
    "maxlen = 10 # the maximum length to limit computation time\n",
    "# creating an adjacency list\n",
    "adj = [[n.index for n in v.neighbors()] for v in dgraph.vs]\n",
    "# recursive search of cycles\n",
    "# for each core vertex as candidate starting point\n",
    "core_cycles = []\n",
    "for start in core_indexes:\n",
    "    core_cycles += get_cycles(adj,{'paths': [[start]], 'cycles': []}, maxlen)['cycles']\n",
    "print(\"    # core-based cycles:\", len(core_cycles) )\n",
    "# count the length of loops involving 1 core\n",
    "core_cycles_lens = [len(cycle) for cycle in core_cycles]\n",
    "print(\"    core-based cycles length: \"+str(stats.describe(core_cycles_lens)) )\n",
    "\n",
    "other_cycles = []\n",
    "for start in other_indexes:\n",
    "    other_cycles += get_cycles(adj,{'paths': [[start]], 'cycles': []}, maxlen)['cycles']\n",
    "print(\"    # other-based cycles:\", len(other_cycles) )\n",
    "# count the length of loops involving 1 core\n",
    "other_cycles_lens = [len(cycle) for cycle in other_cycles]\n",
    "print(\"    other-based cycles length: \"+str(stats.describe(other_cycles_lens)) )\n",
    "\n",
    "d,_ = stats.ks_2samp(core_cycles_lens, other_cycles_lens) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all cycles by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core_cycles_lens))\n",
    "plt.scatter(xs, core_cycles_lens, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(other_cycles_lens))\n",
    "plt.scatter(xs, other_cycles_lens, alpha=0.3, c='silver')\n",
    "bp = ax.boxplot([core_cycles_lens,other_cycles_lens], notch=0, sym='', showcaps=False, zorder=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Cycles length')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(core_cycles_lens)), \"other\\n(n={:d})\".format(len(other_cycles_lens))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_cyclelens.png', transparent=True, dpi=1500)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Do the cores of each cluster form more cliques than others? (Panel S3C)\n",
    "\n",
    "If the cores of each cluster are pattern completion units, they should participate in more cliques (set of vertices where an edge is present between any two of them) than other non-core neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cliques = dgraph.cliques(min=2)\n",
    "\n",
    "cliques_cores = []\n",
    "cliques_others = []\n",
    "\n",
    "for cluster_cids in clustered_spectrums:\n",
    "    cluster_core_indices = []\n",
    "    # we take the index of the cell participating in this cluster\n",
    "    cluster_indices = [ophys_cell_ids.index(strid) for strid in cluster_cids]\n",
    "    # we take the cores of this cluster\n",
    "    cluster_core_indices = list(set(core_indexes).intersection(cluster_indices))\n",
    "    cluster_other_indices = list(set(other_indexes).intersection(cluster_indices))\n",
    "    # we take the edges between the cores\n",
    "    for clique in cliques:\n",
    "        if set(clique).issubset(cluster_core_indices):\n",
    "            cliques_cores.append(clique)\n",
    "        if set(clique).issubset(cluster_other_indices):\n",
    "            cliques_others.append(clique)\n",
    "print(cliques_cores)\n",
    "cores_cliques_count = len(cliques_cores)/len(core_indexes)\n",
    "others_cliques_count = len(cliques_others)/len(other_indexes)\n",
    "\n",
    "print(\"    cliques made by cores:\",cores_cliques_count)\n",
    "print(\"    cliques made by others:\",others_cliques_count)\n",
    "\n",
    "# print(core_edges)\n",
    "x = np.array([\"cores\", \"others\"])\n",
    "y = np.array([cores_cliques_count, others_cliques_count])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, color=['forestgreen','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized count of cliques')\n",
    "plt.xticks([0, 1], [\"core\\n(n={:.3f})\".format(cores_cliques_count), \"other\\n(n={:.3f})\".format(others_cliques_count)])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_cliques.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters of events are not reproducible trajectories of the population dynamics\n",
    "\n",
    "Clusters of population events are found by correlating population vectors, which only retain the cell IDs while ignoring the time of firing.    \n",
    "We can consider also time.\n",
    "\n",
    "Each recorded frame (~67ms) is an instantaneous population state defined by all its cells (112 of them are known for their firing, the others are unkown).    \n",
    "A sequence of population states is a trajectory in the population dynamical state space.    \n",
    "In this space, clusters of reproducible population events are represented by reproducible trajectories. \n",
    "\n",
    "We can compare the event trajectories visited within a cluster by comparing their patterns.    \n",
    "Events are made by cells firing (often multiple times) during the event interval, so each sequence is a 2D submatrix of the population rasterplot. This gives a measure of trajectory reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(cluster_events_spiketrains) # already expressed in integer (ms)\n",
    "\n",
    "print(\"... sequence internal consistency\")\n",
    "\n",
    "# cycle over clusters\n",
    "for cluster_k, events_cellindexes in sorted_events_indexes.items():\n",
    "    if cluster_k == 'gray':\n",
    "        continue\n",
    "    print()\n",
    "\n",
    "    # We want to compare the trajectories of this cluster.\n",
    "    # Trajectories should have same shape. We will subract them to get the difference (/num of events).\n",
    "    \n",
    "    # Finding the common-shape trajectory\n",
    "    # n is the maximal number of cells participating to events in this cluster\n",
    "    maxcells = max(events_cellindexes, key = lambda i: len(i))\n",
    "    # m is the largest interval between the first min spiketrain and the last max spiketrain of all events in the cluster\n",
    "    events_spiketrains = cluster_events_spiketrains[cluster_k]\n",
    "    # print(events_spiketrains)\n",
    "    maxinterval = 0 # \n",
    "    for evt_spktrains in events_spiketrains:\n",
    "        mint = np.amin([x for xs in evt_spktrains for x in xs]) # for cases of just one spiketinme in list\n",
    "        if isinstance(mint, list): mint = mint[0] # for cases of list\n",
    "        maxt = np.amax([x for xs in evt_spktrains for x in xs])\n",
    "        if isinstance(maxt, list): maxt = maxt[-1]\n",
    "        if maxt-mint > maxinterval:\n",
    "            maxinterval = maxt-mint\n",
    "    print(\"    common trajectory pattern with n cells:\", len(maxcells), \" and m intervals:\", maxinterval)\n",
    "    \n",
    "    # cluster trajectories, one per event, all same shape\n",
    "    cluster_trajectories = []\n",
    "    for evt_indexes,evt_spktrains in zip(events_cellindexes,events_spiketrains):\n",
    "        # create empty trajectory of shape n cell, m interval\n",
    "        trajectory = np.zeros((len(maxcells),maxinterval+1))\n",
    "        mint = np.amin([x for xs in evt_spktrains for x in xs]) # take local mintime to find the trajectory m index\n",
    "        if isinstance(mint, list): mint = mint[0] # for cases of just one spiketinme in list\n",
    "        for ncell,spktrain in enumerate(evt_spktrains):\n",
    "            trajectory[ncell][spktrain-mint] = 1\n",
    "        cluster_trajectories.append(trajectory)\n",
    "    \n",
    "    # correlation between trajectories\n",
    "    # very simple (probably too much) measure of trajectory correspondence\n",
    "    trajR = []\n",
    "    for itr,itrajectory in enumerate(cluster_trajectories):\n",
    "        for jtr,jtrajectory in enumerate(cluster_trajectories):\n",
    "            if itr!=jtr:\n",
    "                trajR.append( np.nanmean(np.corrcoef(itrajectory,jtrajectory)) )\n",
    "    print(\"    correlation across all trajectories: {:1.3f}±{:1.2f}\".format(np.mean(trajR),np.std(trajR)))\n",
    "\n",
    "    print(\"... searching for repeating sequences in the ordered firing of cell IDs\")\n",
    "    size = 2\n",
    "    # size = 3\n",
    "    cluster_sequences = [x for xs in events_cellindexes for x in xs]\n",
    "    # print(cluster_sequences)\n",
    "    windows = [\n",
    "        tuple(window)\n",
    "        for window in more_itertools.windowed(cluster_sequences, size)\n",
    "    ]\n",
    "    counter = collections.Counter(windows)\n",
    "    for window, count in counter.items():\n",
    "        if count > 1:\n",
    "            print(\"   \",window, count)\n",
    "            print(core_indexes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
