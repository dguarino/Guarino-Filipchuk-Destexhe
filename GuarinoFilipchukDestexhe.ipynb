{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reproducible activity without attractors in the mouse cortex \n",
    "\n",
    "Analysis code to reproduce all panels in figures 1 and 2 of the paper by Guarino, Filipchuk, Destexhe (2022)   \n",
    "preprint link: https://www.biorxiv.org/content/10.1101/2022.05.24.493230v2\n",
    "\n",
    "All this code is hosted on a github [repository](https://github.com/dguarino/Guarino-Filipchuk-Destexhe) (with a Zenodo DOI persistent identifier [here](https://zenodo.org)) and can be interactively executed here.  \n",
    "The repository also contains a copy of the required data files from the [MICrONS project phase1](https://www.microns-explorer.org/phase1) (freely available on the project website), to ease the setup on Binder. \n",
    "\n",
    "This notebook performs loading and selection of the MICrONS data, structural and dynamical analyses, and plots the results as in the paper panels.\n",
    "\n",
    "We divided the analysis code into:\n",
    "- `imports_functions.py` : performs the imports and definition of various helper functions.\n",
    "- `structural_analysis.py` : creates a graph from the connectivity matrix and computes several graph measures (using [igraph](https://igraph.org)).\n",
    "- `dynamical_analysis.py` : performs the same population event analysis as in [Filipchuk et al. 2022](https://www.biorxiv.org/content/10.1101/2021.08.31.458322v2) and then also extracts the core neurons of the events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "from builtins import exec\n",
    "exec(open(\"./imports_functions.py\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading curated data from MICrONS project phase 1\n",
    "\n",
    "The following code for data loading and selection is taken from   \n",
    "https://github.com/AllenInstitute/MicronsBinder/blob/master/notebooks/intro/MostSynapsesInAndOut.ipynb   \n",
    "https://github.com/AllenInstitute/MicronsBinder/blob/master/notebooks/vignette_analysis/function/structure_function_analysis.ipynb\n",
    "\n",
    "`Neurons.pkl` contains the `segment_id` for each pyramidal neuron in the EM volume.    \n",
    "`Soma.pkl` contains the soma position for all the cells in the EM volume.   \n",
    "`calcium_trace.pkl` contains the calcium imaging traces (including deconvolved spikes).    \n",
    "`soma_subgraph_synapses_spines_v185.csv` contains the list of synapses with root pre-/post-synaptic somas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"MICrONS_data/calcium_trace.pkl\"):\n",
    "    print(\"Downloading 2photon calcium traces ...\")\n",
    "    resp = wget.download(\"https://zenodo.org/record/5646567/files/calcium_trace.pkl?download=1\", \"MICrONS_data/calcium_trace.pkl\")\n",
    "    print(\"... Done: \"+resp)\n",
    "    \n",
    "if os.path.exists(\"MICrONS_data/calcium_trace.pkl\"):\n",
    "    calcium_trace = pd.read_pickle(\"MICrONS_data/calcium_trace.pkl\")\n",
    "    calcium_trace_df = pd.DataFrame.from_dict(calcium_trace, orient='index')\n",
    "# print(calcium_trace)\n",
    "# print(calcium_trace_df.columns) # ['scan', 'trace_raw', 'trace', 'spike', 'stimulus']\n",
    "# print(len(calcium_trace_df.index) ) # 112\n",
    "# print(len(calcium_trace_df[calcium_trace_df['scan']==1].index))\n",
    "# print(len(calcium_trace_df[calcium_trace_df['scan']==1].stimulus[648518346349539895]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAUTION: The cell below might take some time to load the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1961, 17)\n",
      "(3239275, 16)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"MICrONS_data/pni_synapses_v185.csv\"):\n",
    "    print(\"Downloading Synapse table ...\")\n",
    "    resp = wget.download(\"https://zenodo.org/record/3710459/files/pni_synapses_v185.csv?download=1\", \"MICrONS_data/pni_synapses_v185.csv\")\n",
    "    print(\"... Done: \"+resp)\n",
    "\n",
    "if not os.path.exists(\"MICrONS_data/soma_subgraph_synapses_spines_v185.csv\"):\n",
    "    print(\"Downloading soma_subgraph_synapses_spines_v185 ...\")\n",
    "    resp = wget.download(\"https://zenodo.org/record/3710459/files/soma_subgraph_synapses_spines_v185.csv?download=1\", \"MICrONS_data/soma_subgraph_synapses_spines_v185.csv\")\n",
    "    print(\"... Done: \"+resp)\n",
    "\n",
    "with open(\"MICrONS_data/Neuron.pkl\", 'rb') as handle:\n",
    "    Neuron = pickle.load(handle)\n",
    "with open(\"MICrONS_data/Soma.pkl\", 'rb') as handle:\n",
    "    Soma = pickle.load(handle)\n",
    "\n",
    "syn_spines_df = pd.read_csv('MICrONS_data/soma_subgraph_synapses_spines_v185.csv')\n",
    "# id, pre_root_id, post_root_id, cleft_vx, spine_vol_um3\n",
    "print(syn_spines_df.shape)\n",
    "\n",
    "syn_df = pd.read_csv('MICrONS_data/pni_synapses_v185.csv')\n",
    "print(syn_df.shape)\n",
    "# print(syn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the IDs and number of recorded pyramidal neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_list = Neuron[\"segment_id\"]\n",
    "n_pyc = pyc_list.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the folder to which all results will be saved, and the frame duration (from the MICrONS docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = os.getcwd()\n",
    "frame_duration = 0.0674 # sec, 14.8313 frames per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Accessing 2-photon Calcium imaging data subset\n",
    "\n",
    "We are interested in reading only the Ca-imaging data of the cells for which also the EM reconstruction is available.   \n",
    "\n",
    "##### CAUTION: next cell can take some time to load all calcium imaging data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyramidal neurons recorded with 2-photon Calcium imaging:  112\n",
      "27100\n",
      "... producing spike rasterplot\n"
     ]
    }
   ],
   "source": [
    "print(\"Pyramidal neurons recorded with 2-photon Calcium imaging: \",len(calcium_trace))\n",
    "ophys_cell_ids = list(calcium_trace.keys())\n",
    "n_frames = len(calcium_trace[ophys_cell_ids[0]]['spike'])\n",
    "print(n_frames)\n",
    "start_time = 0 # 200 frames of blank screen are already removed from the data\n",
    "stop_time = (n_frames)*frame_duration\n",
    "time = np.arange(start_time,stop_time,frame_duration)\n",
    "\n",
    "spiketrains = [[] for _ in range(5)] # five scans\n",
    "ophys_scan_ids = [[] for _ in range(5)] # five scans\n",
    "for ocell_id in ophys_cell_ids:\n",
    "    decst = calcium_trace[ocell_id][\"spike\"]\n",
    "    spiketrains[(calcium_trace[ocell_id][\"scan\"])-1].append( time[:][np.nonzero(decst)]) # deconvolved Ca spiketrains\n",
    "    ophys_scan_ids[(calcium_trace[ocell_id][\"scan\"])-1].append( ocell_id )\n",
    "\n",
    "print(\"... producing spike rasterplot\")\n",
    "fig = plt.figure(figsize=[12.8,4.8])\n",
    "rowg = 0\n",
    "rowc = ['b','g','r','c','m']\n",
    "for scanid,scan in enumerate(spiketrains):\n",
    "    for row,train in enumerate(scan):\n",
    "        plt.scatter( train, [row+rowg]*len(train), marker='o', edgecolors='none', s=1, c=rowc[scanid] )\n",
    "    rowg += row+1\n",
    "    # add here timing of oriented stimulus\n",
    "    \n",
    "plt.ylabel(\"cell IDs\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "fig.savefig(exp_path+'/results/rasterplot.png', transparent=False, dpi=800)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the cell indexes from the list of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ophys_cell_indexes = range(len(ophys_cell_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get soma center locations\n",
    "\n",
    "They are provided in voxels coordinates of 4,4,40 nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_soma_loc = np.zeros((n_pyc, 3))\n",
    "for i in range(n_pyc):\n",
    "    seg_id = pyc_list[i]\n",
    "    pyc_soma_loc[i,:] = get_soma_loc(Soma, seg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join cell indexes with their position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyc_ca_soma_loc = np.zeros((len(ophys_cell_indexes), 3))\n",
    "for i in ophys_cell_indexes:\n",
    "    seg_id = ophys_cell_ids[i]\n",
    "    idx = np.where(pyc_list==seg_id)[0][0]\n",
    "    pyc_ca_soma_loc[i,:] = pyc_soma_loc[idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Structural Analysis\n",
    "\n",
    "First, we build an adjacency matrix for all the EM-imaged neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.zeros((len(ophys_cell_indexes), len(ophys_cell_indexes)))\n",
    "\n",
    "for i in ophys_cell_indexes:\n",
    "    root_id = ophys_cell_ids[i]\n",
    "    root_id_postsyn_list = syn_df[syn_df['pre_root_id'] == root_id]['post_root_id'].tolist()\n",
    "    for ps in root_id_postsyn_list:\n",
    "        if ps in ophys_cell_ids:\n",
    "            ips = ophys_cell_ids.index(ps)\n",
    "            adjacency_matrix[i][ips]=1\n",
    "np.save(exp_path+'/results/adjacency_matrix.npy', adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make another one 2p-scan-specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_adjacency_matrix = {}\n",
    "for scan_id in range(5):\n",
    "    \n",
    "    gshape = len(ophys_scan_ids[scan_id]) # num cells in the scan\n",
    "    adjacency_matrix = np.zeros((gshape, gshape))\n",
    "    \n",
    "    for i,root_id in enumerate(ophys_scan_ids[scan_id]):\n",
    "        root_id_postsyn_list = syn_df[syn_df['pre_root_id'] == root_id]['post_root_id'].tolist()\n",
    "        for ps in root_id_postsyn_list:\n",
    "            if ps in ophys_scan_ids[scan_id]:\n",
    "                ips = ophys_scan_ids[scan_id].index(ps)\n",
    "                adjacency_matrix[i][ips]=1\n",
    "        scan_adjacency_matrix[scan_id] = adjacency_matrix\n",
    "    \n",
    "np.save(exp_path+'/results/scan_adjacency_matrix.npy', scan_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Are co-active cells also connected?\n",
    "\n",
    "We first measure 1-lag correlation across all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scan_id,scan in enumerate(spiketrains):\n",
    "    # make binary spiketrains\n",
    "    binary_spiketrains = np.zeros( (len(scan),len(time)+2) )\n",
    "    # print(binary_spiketrains.shape)\n",
    "    \n",
    "    for row,train in enumerate(scan):\n",
    "        # iterate over spiketrains assigning 1 to the binary_spiketrains at the corresponding position\n",
    "        tidxs = np.trunc(np.array(train)/frame_duration).astype(int)\n",
    "        tidxs[tidxs>len(time)] = len(time) \n",
    "        binary_spiketrains[row][tidxs] = 1\n",
    "\n",
    "    functional_adjacency_matrix = []\n",
    "    for irow,bsti in enumerate(binary_spiketrains):\n",
    "        row_xcorr = []\n",
    "        for jrow,bstj in enumerate(binary_spiketrains):\n",
    "            if irow==jrow:\n",
    "                row_xcorr.append(0.0) # no self connections\n",
    "                continue\n",
    "            row_xcorr.append(crosscorrelation(bsti, bstj, maxlag=1, mode='corr')[2])\n",
    "        functional_adjacency_matrix.append(row_xcorr)\n",
    "    functional_adjacency_matrix = np.array(functional_adjacency_matrix)\n",
    "    np.save(exp_path+\"/results/functional_adjacency_matrix_%d.npy\"%scan_id, functional_adjacency_matrix)\n",
    "    \n",
    "    # bootstrap confirms the percentile\n",
    "    # print(functional_adjacency_matrix.mean(), np.percentile(functional_adjacency_matrix, 95))\n",
    "    # func_percentile = []\n",
    "    # for bi in range(1000):\n",
    "    #     bootstrap_matrix = np.stack([np.random.choice(functional_adjacency_matrix.flatten(),size=functional_adjacency_matrix.shape[0],replace=False) for i in range(functional_adjacency_matrix.shape[0])])\n",
    "    #     func_percentile.append(np.percentile(bootstrap_matrix, 95))\n",
    "    # print(np.mean(func_percentile))\n",
    "    \n",
    "    # find mean and 95%, plot in on the colorbar, highlight which ones are above\n",
    "    above95points = np.argwhere(functional_adjacency_matrix >= np.percentile(functional_adjacency_matrix,95))\n",
    "    \n",
    "    # plot\n",
    "    fig = plt.figure()\n",
    "    # norm = MidpointNormalize(vmin=np.amin(functional_adjacency_matrix), vmax=np.amax(functional_adjacency_matrix), midpoint=0)\n",
    "    norm = MidpointNormalize(vmin=-0.04, vmax=0.12, midpoint=0)\n",
    "    plt.pcolormesh(functional_adjacency_matrix, cmap='bwr', norm=norm)\n",
    "    ax = plt.gca()\n",
    "    for ap in above95points:\n",
    "        rect = patches.Rectangle((ap[1],ap[0]),1,1,linewidth=1,edgecolor='k',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.plot(0.5, functional_adjacency_matrix.mean(), 'k.') \n",
    "    cbar.ax.plot(0.5, np.percentile(functional_adjacency_matrix,95), 'w.')\n",
    "    fig.savefig(exp_path+\"/results/functional_adjacency_matrix_scan%d.png\"%scan_id, transparent=True)\n",
    "    plt.close()\n",
    "    fig.clear()\n",
    "    fig.clf()\n",
    "    \n",
    "    # masking\n",
    "    maskedmatrix = functional_adjacency_matrix*scan_adjacency_matrix[scan_id]\n",
    "    masked95points = np.argwhere(maskedmatrix >= np.percentile(functional_adjacency_matrix,95))\n",
    "    fig = plt.figure()\n",
    "    # norm = MidpointNormalize(vmin=np.amin(maskedmatrix), vmax=np.amax(maskedmatrix), midpoint=0)\n",
    "    norm = MidpointNormalize(vmin=-0.04, vmax=0.12, midpoint=0)\n",
    "    plt.pcolormesh(maskedmatrix, cmap='bwr', norm=norm)\n",
    "    plt.title(\"EM-corrected connection rate: %.2f\"%(len(masked95points)/len(above95points)))\n",
    "    ax = plt.gca()\n",
    "    for ap in above95points:\n",
    "        rect = patches.Rectangle((ap[1],ap[0]),1,1,linewidth=1,edgecolor='k',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.plot(0.5, functional_adjacency_matrix.mean(), 'k.') \n",
    "    cbar.ax.plot(0.5, np.percentile(functional_adjacency_matrix,95), 'w.')\n",
    "    fig.savefig(exp_path+\"/results/EMmasked_functional_adjacency_matrix_scan%d.png\"%scan_id, transparent=True)\n",
    "    plt.close()\n",
    "    fig.clear()\n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional and EM connectivity together\n",
    "We mask the functional adjacency matrix using the EM connectivity matrix.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are co-tuned cells also co-active and connected?\n",
    "Reading the stimulation protocol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scans were made using the same 16 orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n",
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5   nan]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==1].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==2].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==3].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==4].stimulus.tolist()))\n",
    "print(np.unique(calcium_trace_df[calcium_trace_df['scan']==5].stimulus.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copying for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27100\n"
     ]
    }
   ],
   "source": [
    "stimuli = [] \n",
    "for scan_id in range(5):\n",
    "    stimuli.append( calcium_trace_df[calcium_trace_df['scan']==scan_id+1].head(1).stimulus.tolist()[0] )\n",
    "print(len(stimuli[0])) # 27100, first 200 frames are used for the Df/f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protocol is made by\n",
    "- 1 orientation out of 16 (0., 22.5, 45., 67.5, 90., 112.5, 135., 157.5, 180., 202.5, 225., 247.5, 270., 292.5, 315., 337.5), presented for 15 frames\n",
    "- followed by 40-41 frames of pink noise\n",
    "- all orientations are repeated 30 times (trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5\n",
      " 270.  292.5 315.  337.5]\n"
     ]
    }
   ],
   "source": [
    "orientations = np.unique(stimuli[0])[:-1] # without the nan\n",
    "print(orientations)\n",
    "# print(list(stimuli[0]))\n",
    "# print(np.argwhere(stimuli[0]==45.).tolist())\n",
    "# print(np.argwhere(stimuli[4]==45.).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all cells, we are going to compute:\n",
    "- the *Orientation tuning* (ORT) as in [Ringach et al. 1997](https://pubmed.com):    \n",
    "    $ORT = \\frac{1}{T} \\sum_{t}^{T} R_{td}$    \n",
    "    where $T$ is the number of trials, $R_{td}$ is the number of spikes fired during one trial presentation of one stimulus with orientation $d$.\n",
    "    \n",
    "- the *Orientation Sensitivity Index* (OSI) as in [Ringach et al. 2002](https://pubmed.com):    \n",
    "    $OSI = |\\frac{\\sum_{d}^{D} R_d * exp(i\\frac{\\pi}{4}d)}{\\sum_{d}^{D} R_d}|$    \n",
    "    where $R_i$ is the response during stimulus presentation $d$.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSI for 648518346349539895: 0.6432632756577449\n",
      "Tunings for 648518346349539895: [0.13333333333333333, 0.0, 0.0, 0.06666666666666667, 0.06666666666666667, 0.3333333333333333, 0.6, 0.2, 0.2, 0.03333333333333333, 0.0, 0.0, 0.03333333333333333, 0.5, 0.3333333333333333, 0.16666666666666666]\n"
     ]
    }
   ],
   "source": [
    "gOSI_cell_ids = dict.fromkeys(list(calcium_trace.keys()))\n",
    "gORT_cell_ids = dict.fromkeys(list(calcium_trace.keys()))\n",
    "\n",
    "for scan_id in range(5):\n",
    "    tdarray = np.zeros( (len(orientations),len(spiketrains[scan_id])) )\n",
    "    for oridx,ori in enumerate(orientations):\n",
    "        frames = np.argwhere(stimuli[scan_id]==ori)[:,0]\n",
    "        # print(frames)\n",
    "        # beginning frames for the presentation of this orientation \n",
    "        boundaries = [i for i in range(1, len(frames)) if frames[i] != frames[i-1]+1]\n",
    "        intervals = []\n",
    "        first = 0\n",
    "        for bstim in boundaries:\n",
    "            intervals.append( (frames[first]*frame_duration, frames[bstim-1]*frame_duration) )\n",
    "            first = bstim\n",
    "        intervals.append( (frames[first]*frame_duration, frames[-1]*frame_duration) ) # last\n",
    "        # print(intervals)\n",
    "        # for each cell in this scan ...\n",
    "        spike_count = [0 for x in range(len(spiketrains[scan_id]))]\n",
    "        for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "            # gather all spikes fired during the presentation of the current orientation\n",
    "            for startstim,endstim in intervals:\n",
    "                tdarray[oridx][cidx] += len([spt for spt in spiketrains[scan_id][cidx] if (startstim<spt and spt<=endstim)])\n",
    "    tdarray /= 30\n",
    "\n",
    "    for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "        gORT_cell_ids[cid] = list(tdarray.T[cidx])\n",
    "\n",
    "    # gOSI\n",
    "    for cidx,cid in enumerate(ophys_scan_ids[scan_id]):\n",
    "        gOSI_cell = 0.0\n",
    "        Rcell = 0.0\n",
    "        for oridx,ori in enumerate(orientations):\n",
    "            gOSI_cell += tdarray[oridx][cidx] * np.exp(1j * (np.pi/4) * oridx)\n",
    "            Rcell += tdarray[oridx][cidx]\n",
    "        gOSI_cell_ids[cid] = np.abs( gOSI_cell / Rcell )\n",
    "        \n",
    "print(\"OSI for 648518346349539895:\",gOSI_cell_ids[648518346349539895])\n",
    "fig = plt.figure()\n",
    "plt.hist(gOSI_cell_ids.values(),bins=10)\n",
    "fig.canvas.draw()\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('OSI')\n",
    "fig.savefig(exp_path+'/results/OSI.png', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "\n",
    "print(\"Tunings for 648518346349539895:\",gORT_cell_ids[648518346349539895])\n",
    "fig = plt.figure()\n",
    "barori = np.array([0. for _ in range(16)])\n",
    "for cid,oris in gORT_cell_ids.items():\n",
    "    barori += oris\n",
    "plt.bar(range(len(barori)), barori, 1.)\n",
    "fig.canvas.draw()\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Preferred orientation (deg)')\n",
    "plt.xticks(range(17),['0','','45','','90','','135','','180','','225','','270','','315','','360'])\n",
    "fig.savefig(exp_path+'/results/ORI.png', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are co-tuned cells connected?\n",
    "The classical [Ko et al. 2011](https://www.nature.com/articles/nature09880) results show that co-tuned cells are preferably connected.    \n",
    "Do we find the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 16)\n",
      "#co-tuned cells: 112\n",
      "648518346349539895 6\n",
      "45 deg apart ort: 648518346349539840\n",
      "648518346349537860 10\n",
      "22.5 deg apart ort: 648518346349537513\n",
      "same ort: 648518346349537790\n",
      "648518346349538440 14\n",
      "648518346349538527 13\n",
      "648518346349538209 8\n",
      "45 deg apart ort: 648518346349539844\n",
      "45 deg apart ort: 648518346349537860\n",
      "45 deg apart ort: 648518346349537860\n",
      "same ort: 648518346349538209\n",
      "22.5 deg apart ort: 648518346349539900\n",
      "648518346349492682 6\n",
      "same ort: 648518346349539579\n",
      "648518346349537818 3\n",
      "648518346349539464 1\n",
      "22.5 deg apart ort: 648518346349534072\n",
      "648518346349539852 6\n",
      "648518346349539599 15\n",
      "648518346349539423 6\n",
      "same ort: 648518346349539852\n",
      "648518346349540053 5\n",
      "648518346349539100 12\n",
      "648518346349538391 15\n",
      "648518346349538239 8\n",
      "same ort: 648518346349539794\n",
      "648518346349537153 8\n",
      "648518346349539579 6\n",
      "648518346349539840 8\n",
      "648518346349539900 9\n",
      "648518346349536680 10\n",
      "648518346349538112 8\n",
      "same ort: 648518346349538112\n",
      "648518346349537487 2\n",
      "648518346349539865 4\n",
      "648518346349539780 14\n",
      "648518346349538336 11\n",
      "648518346349539510 9\n",
      "648518346349537989 8\n",
      "648518346349534048 5\n",
      "22.5 deg apart ort: 648518346349492682\n",
      "648518346349537853 4\n",
      "648518346349539572 13\n",
      "648518346349524141 1\n",
      "648518346349539598 4\n",
      "648518346349537790 10\n",
      "648518346349537516 0\n",
      "22.5 deg apart ort: 648518346349537300\n",
      "648518346349539934 4\n",
      "648518346349539862 11\n",
      "648518346349537509 0\n",
      "same ort: 648518346349538416\n",
      "648518346349539524 7\n",
      "22.5 deg apart ort: 648518346349537989\n",
      "22.5 deg apart ort: 648518346349538209\n",
      "648518346349539376 15\n",
      "648518346349539366 5\n",
      "648518346349539794 8\n",
      "648518346349539554 10\n",
      "648518346349537961 14\n",
      "same ort: 648518346349537961\n",
      "same ort: 648518346349537961\n",
      "648518346349537242 8\n",
      "648518346349537848 14\n",
      "648518346349537300 1\n",
      "648518346349536851 1\n",
      "648518346349537513 11\n",
      "648518346349538715 13\n",
      "648518346349538730 10\n",
      "648518346349536924 0\n",
      "22.5 deg apart ort: 648518346349539464\n",
      "648518346349532180 14\n",
      "648518346349539834 12\n",
      "648518346349537897 0\n",
      "648518346349539961 6\n",
      "648518346349540051 8\n",
      "648518346349539435 13\n",
      "648518346349539885 5\n",
      "648518346349539401 1\n",
      "same ort: 648518346349539401\n",
      "648518346349536929 9\n",
      "648518346349538426 0\n",
      "648518346349538721 4\n",
      "648518346349524063 8\n",
      "648518346349523030 12\n",
      "648518346349537331 8\n",
      "648518346349537255 0\n",
      "648518346349537042 7\n",
      "22.5 deg apart ort: 648518346349539821\n",
      "648518346349534072 2\n",
      "648518346349537814 1\n",
      "648518346349538005 15\n",
      "648518346349539067 0\n",
      "648518346349539591 4\n",
      "648518346349539892 0\n",
      "648518346349538193 2\n",
      "648518346349538332 0\n",
      "648518346349539836 10\n",
      "648518346349539096 0\n",
      "648518346349537515 13\n",
      "648518346349531414 14\n",
      "648518346349532006 8\n",
      "648518346349539347 12\n",
      "648518346349539821 8\n",
      "648518346349537657 13\n",
      "648518346349538231 12\n",
      "648518346349536769 6\n",
      "same ort: 648518346349536366\n",
      "648518346349536811 12\n",
      "648518346349536534 7\n",
      "648518346349536366 6\n",
      "648518346349522230 14\n",
      "648518346349537609 10\n",
      "648518346349538416 0\n",
      "648518346349531851 13\n",
      "648518346349532050 1\n",
      "648518346349534945 3\n",
      "648518346349536788 9\n",
      "648518346349534580 2\n",
      "648518346349538746 15\n",
      "648518346349535074 15\n",
      "648518346349539844 10\n",
      "648518346349533252 6\n",
      "648518346349538431 10\n",
      "648518346349534079 6\n",
      "648518346349523266 8\n",
      "648518346349535435 8\n",
      "same ort: 648518346349538112\n",
      "648518346349521083 10\n",
      "648518346349532086 1\n",
      "648518346349536354 7\n",
      "648518346349537741 7\n",
      "648518346349537901 0\n",
      "648518346349538001 14\n",
      "648518346349538251 8\n",
      "648518346349538259 6\n"
     ]
    }
   ],
   "source": [
    "ort_df = pd.DataFrame.from_dict(gORT_cell_ids, orient='index')\n",
    "print(ort_df.shape)\n",
    "# print(ort_df)\n",
    "\n",
    "# find best orientation tuning for each cell\n",
    "cell_max_ort = ort_df.idxmax(axis=1).to_dict()\n",
    "# print(cell_max_ort)\n",
    "\n",
    "# how many co-tuned cells?\n",
    "rev_max_ort = {} # \n",
    "for key, value in cell_max_ort.items():\n",
    "     rev_max_ort.setdefault(value, set()).add(key)\n",
    "cotuned_cells = [values for key, values in rev_max_ort.items() if len(values) > 1] \n",
    "# print(cotuned_cells)\n",
    "num_cotuned_cells = 0\n",
    "for cotuneids in cotuned_cells:\n",
    "    num_cotuned_cells += len(cotuneids)\n",
    "print(\"#co-tuned cells:\",num_cotuned_cells) # happens to be the same\n",
    "\n",
    "# how many co-tuned are connected?\n",
    "for cellid, ori in cell_max_ort.items():\n",
    "    print(cellid, ori)\n",
    "\n",
    "    # cycle through cell ids\n",
    "    postsyn_list = syn_df[syn_df['pre_root_id'] == cellid]['post_root_id'].tolist()\n",
    "    for ps in postsyn_list:\n",
    "        if ps in cell_max_ort and cell_max_ort[ps]==cell_max_ort[cellid]:\n",
    "            print(\"same ort:\",ps) # 0.107 (12/112)\n",
    "        if ps in cell_max_ort and (cell_max_ort[ps]==cell_max_ort[cellid]+1 or cell_max_ort[ps]==cell_max_ort[cellid]+1):\n",
    "            print(\"22.5 deg apart ort:\",ps) # 0.08 (9/112)\n",
    "        if ps in cell_max_ort and (cell_max_ort[ps]==cell_max_ort[cellid]+2 or cell_max_ort[ps]==cell_max_ort[cellid]+2):\n",
    "            print(\"45 deg apart ort:\",ps) # 0.02 (3/112)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Structural analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... adjacency matrix\n",
      "... loaded\n",
      "334\n",
      "    number of vertices: 334\n",
      "... Network nodes degrees\n",
      "... Degree distributions\n",
      "... Local Clustering Coefficient\n",
      "    min 0.0\n",
      "    mean 0.15509339039514802\n",
      "    max 1.0\n",
      "... Betweenness centrality\n",
      "... Motifs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:249: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "global_degree_counts = []\n",
    "global_degree_distribution = []\n",
    "global_structural_betweeness = []\n",
    "global_structural_motifs = []\n",
    "global_structural_motifsratio = []\n",
    "global_structural_motifsurrogates = []\n",
    "\n",
    "exec(open(\"./structural_analysis.py\").read())\n",
    "\n",
    "global_structural_betweeness.append(betweenness_centrality)\n",
    "global_degree_counts.append(degree_counts)\n",
    "global_degree_distribution.append(degrees)\n",
    "global_structural_motifs.append(motifs)\n",
    "global_structural_motifsurrogates.append(surrogate_motifs)\n",
    "global_structural_motifsratio.append(motifsratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dynamical Analysis\n",
    "\n",
    "Here we first population events, we quantify them, and we extract their core neurons.   \n",
    "This analysis extends (from step 5 on) that performed by Filipchuk et al. 2022:\n",
    "1. Compute population instantaneous firing rate (bin)\n",
    "\n",
    "2. Establish significance threshold for population events   \n",
    "    2.1 compute Inter-Spike Intervals (ISI) of the original spiketrains   \n",
    "    2.2 reshuffle ISI to create (1000) surrogates   \n",
    "    2.3 compute the population instantaneous firing rate for each surrogate time-binned rasterplot   \n",
    "\n",
    "3. Find population events   \n",
    "    3.1 smoothed firing rate   \n",
    "    3.2 instantaneous threshold is the 99% of the surrogate population instantaneous firing rate   \n",
    "    3.3 the peaks above intersections of smoothed fr and threshold mark population events   \n",
    "    3.4 the minima before and after a peak are taken as start and end times of the population event   \n",
    "    \n",
    "4. Find clusters of events   \n",
    "    4.1 produce a cell id signature vector of each population event   \n",
    "    4.2 perform clustering linkage by complete cross-correlation of event vectors   \n",
    "    4.3 produce surrogates clusters to establish a cluster significance threshold (95%)     \n",
    "    4.4 find the event reproducibility within each cluster (cluster events cross-correlation)   \n",
    "\n",
    "5. Find core neurons   \n",
    "    5.1 take all neurons participating to a cluster of events   \n",
    "    5.2 use the a percentage (from 60 to 99%) of the cluster event reproducibility as core significance threshold   \n",
    "    5.3 if the occurrence frequency of a neuron is beyond threshold, then the neuron is taken as core   \n",
    "    5.4 remove core neurons if firing unspecifically within and outside their cluster   \n",
    "    \n",
    "### All panels of Figure 1\n",
    "\n",
    "are produced in the next cell by the file `dynamical_analysis.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... firing statistics\n",
      "    population firing: 1.23±1.14 sp/frame\n",
      "    smoothing\n",
      "... generating surrogates to establish population event threshold\n",
      "    cells firing rate: 0.01±0.10 sp/s\n",
      "    event size threshold (mean): 3.2139256165099335\n",
      "... find population events in the trial\n",
      "... signatures of population events\n",
      "    number of events: 226\n",
      "    number of events per sec: 0.1228247519048706\n",
      "    events duration: 0.674±0.229\n",
      "    events size: 8.000±3.775\n",
      "... Similarity of events matrix\n",
      "... clustering\n",
      "    linkage\n",
      "    surrogate events signatures for clustering threshold\n",
      "    cluster reproducibility threshold: 0.20448144105807747\n",
      "    cluster size threshold: 2\n",
      "    Total number of clusters: 97\n",
      "    # clusters (after removing those below reproducibility threshold): 10\n",
      "... finding cluster cores\n",
      "    removing cores firing unspecifically\n",
      "    gathering cores from all clusters\n",
      "    # cores: 35\n",
      "    # non-cores: 77\n",
      "    cores per cluster: 4.64±1.92 (min 2, max 8)\n",
      "    others per cluster: 107.36±1.92 (min 104, max 110)\n",
      "    plotting single events rasterplots ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "global_structural_motif_cores = {k: 0 for k in range(16)}\n",
    "global_structural_motif_others = {k: 0 for k in range(16)}\n",
    "global_events_sec = []\n",
    "global_events_duration = []\n",
    "global_cluster_number = []\n",
    "global_cluster_selfsimilarity = []\n",
    "\n",
    "core_reproducibility_perc = 60 # threshold for detecting cores\n",
    "exec(open(\"./dynamical_analysis.py\").read())\n",
    "\n",
    "global_events_sec.append(events_sec)\n",
    "global_events_duration.extend(events_durations_f)\n",
    "global_cluster_number.append(nclusters)\n",
    "global_cluster_selfsimilarity.extend(reproducibility_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are cores more functionally connected?\n",
    "How likely is that a core is functionally efficient to elicit a response in a core or others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficacy probability as 1-lag correlations\n",
    "core2core_efficacy = [] # probability\n",
    "core2other_efficacy = []\n",
    "other2core_efficacy = []\n",
    "other2other_efficacy = []\n",
    "for dyn_core in clusters_cores:\n",
    "    dyn_core_indexes = [ophys_cell_ids.index(strid) for strid in dyn_core]\n",
    "    dyn_other_indexes = list(set(ophys_cell_indexes).symmetric_difference(set(dyn_core_indexes)))\n",
    "    # selection\n",
    "    core2core_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_core_indexes,:] for conns in cid[dyn_core_indexes]] )\n",
    "    core2other_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_core_indexes,:] for conns in cid[dyn_other_indexes]] )\n",
    "    other2core_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_other_indexes,:] for conns in cid[dyn_core_indexes]] )\n",
    "    other2other_efficacy.extend( [conns for cid in functional_adjacency_matrix[dyn_other_indexes,:] for conns in cid[dyn_other_indexes]] )\n",
    "\n",
    "print(\"    {:d} core2core 1-lag R: {:1.3f}±{:1.2f}\".format(len(core2core_efficacy), np.mean(core2core_efficacy),np.std(core2core_efficacy)) )\n",
    "print(\"    {:d} core2other 1-lag R: {:1.3f}±{:1.2f}\".format(len(core2other_efficacy), np.mean(core2other_efficacy),np.std(core2other_efficacy)) )\n",
    "print(\"    {:d} other2core 1-lag R: {:1.3f}±{:1.2f}\".format(len(other2core_efficacy), np.mean(other2core_efficacy),np.std(other2core_efficacy)) )\n",
    "print(\"    {:d} other2other 1-lag R: {:1.3f}±{:1.2f}\".format(len(other2other_efficacy), np.mean(other2other_efficacy),np.std(other2other_efficacy)) )\n",
    "# significativity\n",
    "kwstat,pval = stats.kruskal(core2core_efficacy, other2other_efficacy)\n",
    "print(\"    core-core vs other-other 1-lag R Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2core_efficacy, other2other_efficacy) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2core_efficacy, core2other_efficacy)\n",
    "print(\"    core-core vs core-other 1-lag R Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2core_efficacy, core2other_efficacy) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2core_efficacy, other2core_efficacy)\n",
    "print(\"    core-core vs other-core 1-lag R Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2core_efficacy, other2core_efficacy) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all spine volumes by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(0, 0.04, len(core2core_efficacy))\n",
    "plt.scatter(xs, core2core_efficacy, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(1, 0.04, len(core2other_efficacy))\n",
    "plt.scatter(xs, core2other_efficacy, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2core_efficacy))\n",
    "plt.scatter(xs, other2core_efficacy, edgecolor='silver', facecolor=('#C0C0C04d'))\n",
    "xs = np.random.normal(3, 0.04, len(other2other_efficacy))\n",
    "plt.scatter(xs, other2other_efficacy, edgecolor='silver', facecolor=('#C0C0C04d'))\n",
    "vp = ax.violinplot([core2core_efficacy,core2other_efficacy,other2core_efficacy,other2other_efficacy], [0,1,2,3], widths=0.3, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc in vp['bodies'][0:1]:\n",
    "    pc.set_facecolor('#228B224d')\n",
    "for pc in vp['bodies'][1:]:\n",
    "    pc.set_facecolor('#D3D3D34d')\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Efficacy (1-lag R)')\n",
    "plt.xticks([0, 1, 2, 3], [\"core-core\\n(n={:d})\".format(len(core2core_efficacy)), \"core-other\\n(n={:d})\".format(len(core2other_efficacy)),\"other-core\\n(n={:d})\".format(len(other2core_efficacy)),\"other-other\\n(n={:d})\".format(len(other2other_efficacy))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_efficacy.png', transparent=True, dpi=1500)\n",
    "# fig.savefig(exp_path+'/results/global_cores_others_efficacy.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mixing structural and dynamical analyses results to characterize core connectivity\n",
    "\n",
    "Here, we collect the evidence contrasting the hypothesis that core neurons are strongly connected.   \n",
    "We tested two fundamental attractor-driven assumptions:\n",
    "- synapses between cores are more numerous and stronger compared to others   \n",
    "- circuits made by cores involve more recursive connections toward cores\n",
    "\n",
    "We can take the **number** and **volume** of post-synaptic spines as proxy for their functional efficacy. \n",
    "\n",
    "But two-photon imaged neurons are few (N=112), and core neurons even fewer (3 to 8 per cluster, 35 total). Therefore their count of (proofread) spines leads to underpowered statistics.    \n",
    "We took three inclusive solutions: \n",
    "- we lowered the threshold for core identification to a minimum (participation to 60% of the events)\n",
    "- we did not differentiate spines based on their point of contact (e.g. axo-somatic, axo-dendritic, axo-axonic, etc)\n",
    "- we considered the number of spines to and from all neurons that were not part of the two-photon imaged dataset (based on the total count of spines, which lacks the proofread volume of the spines). \n",
    "\n",
    "### Core vs other to/from all spine number and volume (panel 2A)\n",
    "We want to know whether cores and others differ globally in terms of their spines.    \n",
    "Knowing the global properties of cores is relevant to assess the significance of the (underpowered) subsequent statistics limited to core vs other spine volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... postsynaptic spines on cores or others from all sources in the EM volume\n",
      "    all2core spines number: 2262.490±985.38 \n",
      "    core2all spines number: 16.560±22.24 \n",
      "    all2other spines number: 2690.411±1031.29 \n",
      "    other2all spines number: 13.524±23.78 \n",
      "    all-core vs all-other spine number Kruskal-Wallis test results: 7.504163352539384 0.006155652888035151\n",
      "    Kolmogorov-Smirnov Effect Size: 1.000\n",
      "    core-all vs other-all spine number Kruskal-Wallis test results: 4.734519917290584 0.029563097881958964\n",
      "    Kolmogorov-Smirnov Effect Size: 0.187\n",
      "\n",
      "... postsynaptic spines on cores or others from sources in the EM volume (proofread with measured volume)\n",
      "    242 all2core spines, volume: 0.069±0.07 µm3\n",
      "    122 core2all spines, volume: 0.078±0.07 µm3\n",
      "    5863 all2other spines, volume: 0.081±0.08 µm3\n",
      "    1770 other2all spines, volume: 0.076±0.07 µm3\n",
      "    all-core vs all-other spine volume Kruskal-Wallis test results: 3.3893914505796774 0.06561716292555728\n",
      "    Kolmogorov-Smirnov Effect Size: 0.088\n",
      "    core-all vs other-all spine volume Kruskal-Wallis test results: 1.4323360244116572 0.2313835761336236\n",
      "    Kolmogorov-Smirnov Effect Size: 0.131\n"
     ]
    }
   ],
   "source": [
    "print(\"... postsynaptic spines on cores or others from all sources in the EM volume\")\n",
    "all2core_spine_vol = [] # µm3\n",
    "core2all_spine_vol = []\n",
    "all2other_spine_vol = []\n",
    "other2all_spine_vol = []\n",
    "\n",
    "all2core_spine_num = [] # num\n",
    "core2all_spine_num = []\n",
    "all2other_spine_num = []\n",
    "other2all_spine_num = []\n",
    "norm_all2core_spine_num = 0.0 # normalized num\n",
    "norm_core2all_spine_num = 0.0\n",
    "norm_all2other_spine_num = 0.0\n",
    "norm_other2all_spine_num = 0.0\n",
    "\n",
    "set_ids = set(ophys_cell_ids)\n",
    "for dyn_core_ids in clusters_cores:\n",
    "    dyn_other_ids = set_ids.symmetric_difference(dyn_core_ids)\n",
    "    # searching\n",
    "    \n",
    "    all2core_synapse_df = syn_df.query(f'(post_root_id in {list(dyn_core_ids)})')\n",
    "    if not all2core_synapse_df.empty:\n",
    "        all2core_spine_num.extend( all2core_synapse_df.groupby('post_root_id').size() )\n",
    "        norm_all2core_spine_num += all2core_synapse_df.groupby('post_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_core_ids))) # normalized by source*target\n",
    "\n",
    "    all2other_synapse_df = syn_df.query(f'(post_root_id in {list(dyn_other_ids)})')\n",
    "    if not all2other_synapse_df.empty:\n",
    "        all2other_spine_num.extend( all2other_synapse_df.groupby('post_root_id').size() )\n",
    "        norm_all2other_spine_num += all2other_synapse_df.groupby('post_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_other_ids)))\n",
    "\n",
    "    core2all_synapse_df = syn_df.query(f'(pre_root_id in {list(dyn_core_ids)})')\n",
    "    if not core2all_synapse_df.empty:\n",
    "        core2all_spine_num.extend( core2all_synapse_df.groupby('pre_root_id').size() )\n",
    "        norm_core2all_spine_num += core2all_synapse_df.groupby('pre_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_core_ids)))\n",
    "\n",
    "    other2all_synapse_df = syn_df.query(f'(pre_root_id in {list(dyn_other_ids)})')\n",
    "    if not other2all_synapse_df.empty:\n",
    "        other2all_spine_num.extend( other2all_synapse_df.groupby('pre_root_id').size() )\n",
    "        norm_other2all_spine_num += other2all_synapse_df.groupby('pre_root_id').size().sum()/(syn_df.shape[0]*len(list(dyn_other_ids)))\n",
    "\n",
    "    # id, pre_root_id, post_root_id, cleft_vx, spine_vol_um3\n",
    "    all2core_synapse_df = syn_spines_df.query(f'(post_root_id in {list(dyn_core_ids)})')\n",
    "    if not all2core_synapse_df.empty:\n",
    "        all2core_spine_vol.extend( all2core_synapse_df['spine_vol_um3'].tolist() )\n",
    "    all2other_synapse_df = syn_spines_df.query(f'(post_root_id in {list(dyn_other_ids)})')\n",
    "    if not all2other_synapse_df.empty:\n",
    "        all2other_spine_vol.extend( all2other_synapse_df['spine_vol_um3'].tolist() )\n",
    "    core2all_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)})')\n",
    "    if not core2all_synapse_df.empty:\n",
    "        core2all_spine_vol.extend( core2all_synapse_df['spine_vol_um3'].tolist() )\n",
    "    other2all_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)})')\n",
    "    if not other2all_synapse_df.empty:\n",
    "        other2all_spine_vol.extend( other2all_synapse_df['spine_vol_um3'].tolist() )\n",
    "        \n",
    "# number description\n",
    "print(\"    all2core spines number: {:1.3f}±{:1.2f} \".format(np.mean(all2core_spine_num),np.std(all2core_spine_num)) )\n",
    "print(\"    core2all spines number: {:1.3f}±{:1.2f} \".format(np.mean(core2all_spine_num),np.std(core2all_spine_num)) )\n",
    "print(\"    all2other spines number: {:1.3f}±{:1.2f} \".format(np.mean(all2other_spine_num),np.std(all2other_spine_num)) )\n",
    "print(\"    other2all spines number: {:1.3f}±{:1.2f} \".format(np.mean(other2all_spine_num),np.std(other2all_spine_num)) )\n",
    "# number significativity\n",
    "kwstat,pval = stats.kruskal(all2core_spine_num, all2other_spine_num)\n",
    "print(\"    all-core vs all-other spine number Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(all2core_spine_num, all2other_spine_vol) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2all_spine_num, other2all_spine_num)\n",
    "print(\"    core-all vs other-all spine number Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2all_spine_num, other2all_spine_num) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core2all_spine_num))\n",
    "plt.scatter(xs, core2all_spine_num, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2all_spine_num))\n",
    "plt.scatter(xs, other2all_spine_num, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "xs = np.random.normal(3, 0.04, len(all2core_spine_num))\n",
    "plt.scatter(xs, all2core_spine_num, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(4, 0.04, len(all2other_spine_num))\n",
    "plt.scatter(xs, all2other_spine_num, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "vp = ax.violinplot([core2all_spine_num,other2all_spine_num,all2core_spine_num,all2other_spine_num], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d','#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Spine number per cell')\n",
    "plt.yscale('log')\n",
    "plt.xticks([1,2,3,4], [\"core-all\", \"other-all\", \"all-core\", \"all-other\"])\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_num.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n",
    "# normalized spine number by type\n",
    "x = np.array([\"all-core\", \"all-other\", \"core-all\", \"other-all\"])\n",
    "y = np.array([norm_all2core_spine_num, norm_all2other_spine_num, norm_core2all_spine_num, norm_other2all_spine_num])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, color=['forestgreen','silver','forestgreen','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized number of spines')\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_normnum.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()\n",
    "\n",
    "print()\n",
    "print(\"... postsynaptic spines on cores or others from sources in the EM volume (proofread with measured volume)\")\n",
    "# volume description\n",
    "print(\"    {:d} all2core spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(all2core_spine_vol), np.mean(all2core_spine_vol),np.std(all2core_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2core_spine_vol)) )\n",
    "print(\"    {:d} core2all spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(core2all_spine_vol), np.mean(core2all_spine_vol),np.std(core2all_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2core_spine_vol)) )\n",
    "print(\"    {:d} all2other spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(all2other_spine_vol), np.mean(all2other_spine_vol),np.std(all2other_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2other_spine_vol)) )\n",
    "print(\"    {:d} other2all spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(other2all_spine_vol), np.mean(other2all_spine_vol),np.std(other2all_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(all2other_spine_vol)) )\n",
    "# volume significativity\n",
    "kwstat,pval = stats.kruskal(all2core_spine_vol, all2other_spine_vol)\n",
    "print(\"    all-core vs all-other spine volume Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(all2core_spine_vol, all2other_spine_vol) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "kwstat,pval = stats.kruskal(core2all_spine_vol, other2all_spine_vol)\n",
    "print(\"    core-all vs other-all spine volume Kruskal-Wallis test results:\",kwstat,pval)\n",
    "d,_ = stats.ks_2samp(core2all_spine_vol, other2all_spine_vol) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core2all_spine_vol))\n",
    "plt.scatter(xs, core2all_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2all_spine_vol))\n",
    "plt.scatter(xs, other2all_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "xs = np.random.normal(3, 0.04, len(all2core_spine_vol))\n",
    "plt.scatter(xs, all2core_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(4, 0.04, len(all2other_spine_vol))\n",
    "plt.scatter(xs, all2other_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "vp = ax.violinplot([core2all_spine_vol,other2all_spine_vol,all2core_spine_vol,all2other_spine_vol], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d','#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Spine Volume (µm^3)')\n",
    "plt.xticks([1,2,3,4], [\"core-all\\n(n={:d})\".format(len(core2all_spine_vol)), \"other-all\\n(n={:d})\".format(len(other2all_spine_vol)), \"all-core\\n(n={:d})\".format(len(all2core_spine_vol)), \"all-other\\n(n={:d})\".format(len(all2other_spine_vol))])\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_vol.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core vs others  \n",
    "The number of cores and non-cores for each cluster is different. Therefore we have to normalize this count to evaluate.\n",
    "\n",
    "For each set of reproducible cluster we count:    \n",
    "- the number of synapses made by a cell type (core or not) towards others, weighted by the squared number of target cells    \n",
    "    - the expectation is that core-to-core and core-to-other synapses should be numerous in order to pull the dynamics\n",
    "- the post-synaptic spine volume of synapses made by a cell type (core or not) towards others.   \n",
    "    - the expectation is that core-to-core and core-to-other spines should be larger in order to pull the dynamics\n",
    "\n",
    "**Synapses between core neurons of each cluster are less than every other combination.**    \n",
    "Note that the resulting normalized synapse counts (for the others) check with the network density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... network density (ratio between the edges present and the maximum number of edges that the graph can contain): 0.017578615224640538\n",
      "... Normalized number of spines\n",
      "    0.000000 core2core normalized spines number\n",
      "    0.095146 core2other normalized spines number\n",
      "    0.052929 other2core normalized spines number\n",
      "    0.042870 other2other normalized spines number\n",
      "... Spine volumes\n",
      "    0 core2core spines, volume: nan±nan µm3\n",
      "    50 core2other spines, volume: 0.089±0.07 µm3\n",
      "    26 other2core spines, volume: 0.092±0.06 µm3\n",
      "    494 other2other spines, volume: 0.074±0.06 µm3\n",
      "   core vs other spine size Kruskal-Wallis test results: 5.629426395826594 0.017661409795233805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# the density of the directed graph.\n",
    "network_density = dgraph.density(loops=True)\n",
    "print(\"... network density (ratio between the edges present and the maximum number of edges that the graph can contain):\", network_density )\n",
    "# spine number\n",
    "core2core_spine_num = 0.0 # to be normalized\n",
    "core2other_spine_num = 0.0\n",
    "other2core_spine_num = 0.0\n",
    "other2other_spine_num = 0.0\n",
    "# spine volume\n",
    "core2core_spine_vol = [] # µm3\n",
    "core2other_spine_vol = []\n",
    "other2core_spine_vol = []\n",
    "other2other_spine_vol = []\n",
    "\n",
    "set_ids = set(ophys_cell_ids)\n",
    "cluster_colors = [color for color in cluster_color_array if color!='gray']\n",
    "for cluster_k,dyn_core_ids in zip(cluster_colors,clusters_cores):\n",
    "    if cluster_k=='gray':\n",
    "        continue\n",
    "    dyn_other_ids = set_ids.symmetric_difference(dyn_core_ids)\n",
    "    \n",
    "    # spine number\n",
    "    core2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    core2core_spine_num += len(core2core_synapse_df['id'].tolist())/(len(dyn_core_ids)*len(dyn_core_ids)) # normalized by source*target\n",
    "\n",
    "    core2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    core2other_spine_num += len(core2other_synapse_df['id'].tolist())/(len(dyn_core_ids)*len(dyn_other_ids)) \n",
    "\n",
    "    other2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    other2core_spine_num += len(other2core_synapse_df['id'].tolist())/(len(dyn_core_ids)*len(dyn_other_ids)) \n",
    "    \n",
    "    other2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    other2other_spine_num += len(other2other_synapse_df['id'].tolist())/(len(dyn_other_ids)*len(dyn_other_ids)) \n",
    "\n",
    "    # spine volume\n",
    "    # id, pre_root_id, post_root_id, cleft_vx, spine_vol_um3\n",
    "    core2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    if not core2core_synapse_df.empty:\n",
    "        core2core_spine_vol.extend( core2core_synapse_df['spine_vol_um3'].tolist() )\n",
    "    \n",
    "    core2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_core_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    if not core2other_synapse_df.empty:\n",
    "        core2other_spine_vol.extend( core2other_synapse_df['spine_vol_um3'].tolist() )\n",
    "    \n",
    "    other2core_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_core_ids)})')\n",
    "    if not other2core_synapse_df.empty:\n",
    "        other2core_spine_vol.extend( other2core_synapse_df['spine_vol_um3'].tolist() )\n",
    " \n",
    "    other2other_synapse_df = syn_spines_df.query(f'(pre_root_id in {list(dyn_other_ids)}) and (post_root_id in {list(dyn_other_ids)})')\n",
    "    if not other2other_synapse_df.empty:\n",
    "        other2other_spine_vol.extend( other2other_synapse_df['spine_vol_um3'].tolist() )\n",
    "\n",
    "# description\n",
    "# number\n",
    "print(\"... Normalized number of spines\")\n",
    "print(\"    {:f} core2core normalized spines number\".format((core2core_spine_num)) )\n",
    "print(\"    {:f} core2other normalized spines number\".format((core2other_spine_num)) )\n",
    "print(\"    {:f} other2core normalized spines number\".format((other2core_spine_num)) )\n",
    "print(\"    {:f} other2other normalized spines number\".format((other2other_spine_num)) )\n",
    "\n",
    "# spines\n",
    "print(\"... Spine volumes\")\n",
    "print(\"    {:d} core2core spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(core2core_spine_vol), np.mean(core2core_spine_vol),np.std(core2core_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(core2core_spine_vol)) )\n",
    "print(\"    {:d} core2other spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(core2other_spine_vol), np.mean(core2other_spine_vol),np.std(core2other_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(core2other_spine_vol)) )\n",
    "print(\"    {:d} other2core spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(other2core_spine_vol), np.mean(other2core_spine_vol),np.std(other2core_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(other2core_spine_vol)) )\n",
    "print(\"    {:d} other2other spines, volume: {:1.3f}±{:1.2f} µm3\".format(len(other2other_spine_vol), np.mean(other2other_spine_vol),np.std(other2other_spine_vol)) )\n",
    "# print(\"    \"+str(stats.describe(other2other_spine_vol)) )\n",
    "\n",
    "# significativity\n",
    "# this is just to test the significativity if the number of samples was correct\n",
    "kwstat,pval = stats.kruskal(other2core_spine_vol, other2other_spine_vol)\n",
    "print(\"   core vs other spine size Kruskal-Wallis test results:\",kwstat,pval)\n",
    "\n",
    "# plotting\n",
    "# all spine number by type\n",
    "x = np.array([\"core-core\", \"core-other\", \"other-core\", \"other-other\"])\n",
    "y = np.array([core2core_spine_num, core2other_spine_num, other2core_spine_num, other2other_spine_num])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, color=['forestgreen','forestgreen','silver','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized number of spines')\n",
    "fig.savefig(exp_path+'/results/global_cores_others_spine_num.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()\n",
    "\n",
    "# normalized spine number by type\n",
    "x = np.array([\"all\\ncore\", \"all\\nother\", \"core\\nall\", \"other\\nall\", \"core\\ncore\", \"core\\nother\", \"other\\ncore\", \"other\\nother\"])\n",
    "y = np.array([norm_all2core_spine_num, norm_all2other_spine_num, norm_core2all_spine_num, norm_other2all_spine_num, core2core_spine_num, core2other_spine_num, other2core_spine_num, other2other_spine_num])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, width=0.5, color=['forestgreen','silver','forestgreen','silver','forestgreen','forestgreen','silver','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Connection probability (relative spine freq.)')\n",
    "fig.savefig(exp_path+'/results/global_all_cores_others_spine_normnum.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# all spine volumes by type\n",
    "fig, ax = plt.subplots()\n",
    "# xs = np.random.normal(0, 0.04, len(core2core_spine_vol))\n",
    "# plt.scatter(xs, core2core_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(1, 0.04, len(core2other_spine_vol))\n",
    "plt.scatter(xs, core2other_spine_vol, edgecolor='forestgreen', facecolor=('#228B224d'))\n",
    "xs = np.random.normal(2, 0.04, len(other2core_spine_vol))\n",
    "plt.scatter(xs, other2core_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "xs = np.random.normal(3, 0.04, len(other2other_spine_vol))\n",
    "plt.scatter(xs, other2other_spine_vol, edgecolor='silver', facecolor=('#D3D3D34d'))\n",
    "# vp = ax.violinplot([core2core_spine_vol,core2other_spine_vol,other2core_spine_vol,other2other_spine_vol], [0,1,2,3], widths=0.3, showextrema=False, showmedians=True)\n",
    "vp = ax.violinplot([core2other_spine_vol,other2core_spine_vol,other2other_spine_vol], [1,2,3], widths=0.3, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc in vp['bodies'][0:1]:\n",
    "    pc.set_facecolor('#228B224d')\n",
    "for pc in vp['bodies'][1:]:\n",
    "    pc.set_facecolor('#D3D3D34d')\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Spine Volume (µm^3)')\n",
    "plt.xticks([0, 1, 2, 3], [ \"core-core\\n(n={:d})\".format(len(core2core_spine_vol)),\"core-other\\n(n={:d})\".format(len(core2other_spine_vol)),\"other-core\\n(n={:d})\".format(len(other2core_spine_vol)),\"other-other\\n(n={:d})\".format(len(other2other_spine_vol))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_spine_vol.svg', transparent=True)\n",
    "fig.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Ca-imaged and outside EM volume inputs (panel 2AB)\n",
    "\n",
    "Core responses could be due to non-imaged and outside volume sources. How can we rule this out (or reduce our lack of knowledge)?   \n",
    "We can ask *Are there more or stronger spines made by non-imaged neurons (either local or far) on cores or others?*   \n",
    "We have this information since we know the cell ID of all somas in the volume. We can take the spines having presynaptic ID different from the known Ca-imaged IDs or different from the somas within the EM volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are core more mutually connected than others?\n",
    "\n",
    "We started by asking whether a global measure such as assortativity - - gives a clear summary of mutuality between all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... assortativity\n",
      "    preparing vertex labels for cores and others\n",
      "    overall: 0.024409568514006098\n",
      "    assortativity degree: 0.007494609849787891\n"
     ]
    }
   ],
   "source": [
    "print('... assortativity')\n",
    "print('    preparing vertex labels for cores and others')\n",
    "dgraph.vs[\"ophys_cell_id\"] = ophys_cell_ids\n",
    "is_id_core = np.array( [0] * len(ophys_cell_ids) )\n",
    "is_id_core[core_indexes] = 1\n",
    "dgraph.vs[\"is_core\"] = is_id_core.tolist()\n",
    "pyc_ca_syn_df = syn_df.query(f'(pre_root_id in {ophys_cell_ids}) and (post_root_id in {ophys_cell_ids})')\n",
    "is_syn_core = np.array( [0] * len(pyc_ca_syn_df) )\n",
    "for cid in [item for sublist in clusters_cores for item in sublist]:\n",
    "    is_syn_core[pyc_ca_syn_df['pre_root_id'] == cid] = 1\n",
    "dgraph.es[\"is_core\"] = is_syn_core.tolist()\n",
    "\n",
    "# is a preference for a network's nodes to attach to others that are similar in some way\n",
    "print(\"    overall:\", dgraph.assortativity_nominal(\"is_core\", directed=True) )\n",
    "# cores degree distro vs others degree distro\n",
    "# biological networks typically show negative assortativity, or disassortative mixing, or disassortativity, as high degree nodes tend to attach to low degree nodes.\n",
    "print(\"    assortativity degree:\", dgraph.assortativity_degree(directed=True) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Motif connectivity of cores and others (panel 2D)\n",
    "\n",
    "This measure of the network reports the participation of cores (or non-cores) in triplet motifs.    \n",
    "Note that the triplets are not exclusively made of cores (or non-cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saved mutual connectivity of cores and others\n"
     ]
    }
   ],
   "source": [
    "# For each set of reproducible cluster cores we count their connectivity motifs.\n",
    "set_indexes = set(ophys_cell_indexes)\n",
    "for dyn_core_ids in clusters_cores:\n",
    "    dyn_core_indexes = set([ophys_cell_ids.index(strid) for strid in dyn_core_ids])\n",
    "    dyn_other_indexes = set_indexes.symmetric_difference(dyn_core_indexes)\n",
    "    for mclass, mlist in motif_vertices.items():\n",
    "        for mtriplet in mlist:\n",
    "            intersection_cores = len(list(dyn_core_indexes.intersection(mtriplet)))\n",
    "            intersection_others = len(list(dyn_other_indexes.intersection(mtriplet)))\n",
    "            global_structural_motif_cores[mclass] += intersection_cores\n",
    "            global_structural_motif_others[mclass] += intersection_others\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar(global_structural_motif_cores.keys(), global_structural_motif_cores.values(), color='forestgreen')\n",
    "plt.ylabel('cores occurrences')\n",
    "plt.yscale('log')\n",
    "plt.ylim([0.7,plt.ylim()[1]])\n",
    "plt.xlabel('motifs types')\n",
    "fig.savefig(exp_path+'/results/global_motifs_cores.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "fig = plt.figure()\n",
    "plt.bar(global_structural_motif_others.keys(), global_structural_motif_others.values(), color='silver')\n",
    "plt.ylabel('non-cores occurrences')\n",
    "plt.yscale('log')\n",
    "plt.ylim([0.7,plt.ylim()[1]])\n",
    "plt.xlabel('motifs types')\n",
    "fig.savefig(exp_path+'/results/global_motifs_others.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clear()\n",
    "fig.clf()\n",
    "print(\"... saved mutual connectivity of cores and others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    graph diameter (#vertices): 7\n",
      "    graph average path length (#vertices): 2.9393095768374167\n"
     ]
    }
   ],
   "source": [
    "# dgraph is already defined from the structural_analysis included file\n",
    "print(\"    graph diameter (#vertices):\", dgraph.diameter(directed=True, unconn=True, weights=None))\n",
    "print(\"    graph average path length (#vertices):\", dgraph.average_path_length(directed=True, unconn=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality of cores\n",
    "\n",
    "If cores are not more mutually connected compared to others, then what is their characterizing feature?    \n",
    "In the cells above, we saw indications of more interconnections between cores and others than within the same type.     \n",
    "This could hint at some form of centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree centrality of cores is not different from others (panel 2E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... degree centrality\n",
      "    cores: DescribeResult(nobs=35, minmax=(0, 79), mean=11.542857142857143, variance=319.66722689075624, skewness=1.9980140270397448, kurtosis=4.196511544747042)\n",
      "    others: DescribeResult(nobs=77, minmax=(0, 39), mean=5.753246753246753, variance=92.13568010936429, skewness=1.6772884894874924, kurtosis=2.118802706844791)\n",
      "    Welch t test:  1.801 p= 0.079\n",
      "    Kolmogorov-Smirnov Effect Size: 0.169\n"
     ]
    }
   ],
   "source": [
    "print('... degree centrality')\n",
    "degree_centrality_cores = dgraph.degree(core_indexes, mode='out', loops=True)\n",
    "degree_centrality_others = dgraph.degree(other_indexes, mode='out', loops=True)\n",
    "# description\n",
    "print(\"    cores: \"+str(stats.describe(degree_centrality_cores)) )\n",
    "print(\"    others: \"+str(stats.describe(degree_centrality_others)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(degree_centrality_cores, degree_centrality_others, equal_var=False))\n",
    "d,_ = stats.ks_2samp(degree_centrality_cores, degree_centrality_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(degree_centrality_cores))\n",
    "plt.scatter(xs, degree_centrality_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(degree_centrality_others))\n",
    "plt.scatter(xs, degree_centrality_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([degree_centrality_cores,degree_centrality_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Degree')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(degree_centrality_cores)), \"other\\n(n={:d})\".format(len(degree_centrality_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_degree.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Betweenness cenrality of cores is not different from others (panel 2F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... betweenness\n",
      "    cores: DescribeResult(nobs=35, minmax=(0.0, 2611.278988942133), mean=372.4290036027808, variance=404149.9675061276, skewness=1.9705740574279296, kurtosis=3.2944463171590233)\n",
      "    others: DescribeResult(nobs=77, minmax=(0.0, 1683.5995508593448), mean=225.43407589305858, variance=172752.41816079392, skewness=1.9865603004296337, kurtosis=3.191352629595454)\n",
      "    Welch t test:  1.252 p= 0.217\n",
      "    Kolmogorov-Smirnov Effect Size: 0.151\n"
     ]
    }
   ],
   "source": [
    "print('... betweenness')\n",
    "cores_betweenness = np.array(dgraph.betweenness(vertices=core_indexes, directed=True))\n",
    "others_betweenness = np.array(dgraph.betweenness(vertices=other_indexes, directed=True))\n",
    "print(\"    cores: \"+str(stats.describe(cores_betweenness)) )\n",
    "print(\"    others: \"+str(stats.describe(others_betweenness)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(cores_betweenness, others_betweenness, equal_var=False))\n",
    "d,_ = stats.ks_2samp(cores_betweenness, others_betweenness) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(cores_betweenness))\n",
    "plt.scatter(xs, cores_betweenness, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(others_betweenness))\n",
    "plt.scatter(xs, others_betweenness, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([cores_betweenness,others_betweenness], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "# plt.yscale('log')\n",
    "# plt.ylim([0.00001,plt.ylim()[1]])\n",
    "plt.ylabel('Betweenness')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(cores_betweenness)), \"other\\n(n={:d})\".format(len(others_betweenness))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_betweenness.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hub scores of cores is not different from others (panel 2G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... authority score\n",
      "    authority cores: DescribeResult(nobs=35, minmax=(0.050932284009955996, 0.6393118539241462), mean=0.28633501916044823, variance=0.025168617579472842, skewness=0.4045893820631807, kurtosis=-0.7041437958817056)\n",
      "    authority others: DescribeResult(nobs=77, minmax=(0.009830045486743951, 1.0), mean=0.26912859944504264, variance=0.03275365462579275, skewness=1.7294508148746914, kurtosis=4.733534247428858)\n",
      "    Kruskal-Wallis test:  0.484 p= 0.629\n",
      "    Kolmogorov-Smirnov Effect Size: 0.145\n",
      "... hub score\n",
      "    hub cores: DescribeResult(nobs=35, minmax=(9.140308603652596e-18, 1.0), mean=0.12379854977870158, variance=0.04505489094516889, skewness=2.5058978577266298, kurtosis=6.831877154233105)\n",
      "    hub others: DescribeResult(nobs=77, minmax=(9.140308603652596e-18, 0.4848643968219558), mean=0.05749152510277729, variance=0.01151653843129077, skewness=2.2962041715178856, kurtosis=5.109152512168375)\n",
      "    Kruskal-Wallis test:  2.199 p= 0.030\n",
      "    Kolmogorov-Smirnov Effect Size: 0.226\n"
     ]
    }
   ],
   "source": [
    "print(\"... authority score\")\n",
    "# what is the overlap of cores and hubs?\n",
    "# Hub\n",
    "authority_scores = np.array(dgraph.authority_score(weights=None, scale=True, return_eigenvalue=False))\n",
    "authority_scores_cores = authority_scores[core_indexes]\n",
    "authority_scores_others = authority_scores[other_indexes]\n",
    "print(\"    authority cores: \"+str(stats.describe(authority_scores_cores)) )\n",
    "print(\"    authority others: \"+str(stats.describe(authority_scores_others)) )\n",
    "# significativity\n",
    "print(\"    Kruskal-Wallis test:  %.3f p= %.3f\" % stats.ttest_ind(authority_scores_cores, authority_scores_others))\n",
    "d,_ = stats.ks_2samp(authority_scores_cores, authority_scores_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all eccentricity by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(authority_scores_cores))\n",
    "plt.scatter(xs, authority_scores_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(authority_scores_others))\n",
    "plt.scatter(xs, authority_scores_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([authority_scores_cores,authority_scores_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Hub score')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(authority_scores_cores)), \"other\\n(n={:d})\".format(len(authority_scores_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_authority_score.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n",
    "print(\"... hub score\")\n",
    "# what is the overlap of cores and hubs?\n",
    "# Hub\n",
    "hub_scores = np.array(dgraph.hub_score(weights=None, scale=True, return_eigenvalue=False))\n",
    "hub_scores_cores = hub_scores[core_indexes]\n",
    "hub_scores_others = hub_scores[other_indexes]\n",
    "print(\"    hub cores: \"+str(stats.describe(hub_scores_cores)) )\n",
    "print(\"    hub others: \"+str(stats.describe(hub_scores_others)) )\n",
    "# significativity\n",
    "print(\"    Kruskal-Wallis test:  %.3f p= %.3f\" % stats.ttest_ind(hub_scores_cores, hub_scores_others))\n",
    "d,_ = stats.ks_2samp(hub_scores_cores, hub_scores_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all eccentricity by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(hub_scores_cores))\n",
    "plt.scatter(xs, hub_scores_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(hub_scores_others))\n",
    "plt.scatter(xs, hub_scores_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([hub_scores_cores,hub_scores_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Hub score')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(hub_scores_cores)), \"other\\n(n={:d})\".format(len(hub_scores_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_hub_score.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cores control the flow of cortical activity\n",
    "\n",
    "So far, we used structural (graph) measures of neurons selected by looking at their reproducibility (a form of regular activity). In a sense, we were already crossing structural and dynamical information about the network.    \n",
    "\n",
    "However, we could push this further.    \n",
    "\n",
    "### Structural underpinnings of clusters (panel 2H)\n",
    "\n",
    "What is the origin of pattern reproducibility?    \n",
    "We can look at **how** the underlying connectivity structure supports each events activities.    \n",
    "\n",
    "We can consider each event as a _network flow problem_, in which the activity can be transferred through cells along the available connections.    \n",
    "\n",
    "For each event, we compute the max flow between the cells IDs according to their firing sequence.      \n",
    "**Do core neurons sustain more flow compared to others?**\n",
    "\n",
    "And we also consider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Flow cores: DescribeResult(nobs=491, minmax=(0.0, 0.6285714285714286), mean=0.054524294442828046, variance=0.006535403188250653, skewness=1.9746861750038214, kurtosis=6.168518072312809)\n",
      "    Flow others: DescribeResult(nobs=179, minmax=(0.0, 0.2857142857142857), mean=0.019371689762751217, variance=0.0014466480981574071, skewness=3.0221010239560293, kurtosis=13.700985247727274)\n",
      "    Welch t test:  7.600 p= 0.000\n",
      "    Kolmogorov-Smirnov Effect Size: 0.283\n",
      "\n",
      "    Cut edges sourcing from cores: DescribeResult(nobs=35, minmax=(0.0, 1.6), mean=0.29469387755102044, variance=0.1933623735208369, skewness=1.6545061248825716, kurtosis=1.6924146771017323)\n",
      "    Cut edges targeting cores: DescribeResult(nobs=35, minmax=(0.0, 1.1714285714285715), mean=0.236734693877551, variance=0.09174412622191734, skewness=1.7458774827764796, kurtosis=2.3082162325499427)\n",
      "    Cut edges sourcing from others: DescribeResult(nobs=77, minmax=(0.0, 0.5064935064935064), mean=0.040647664024287405, variance=0.0065350092291708425, skewness=3.5855880469974224, kurtosis=15.528031042757313)\n",
      "    Cut edges targeting others: DescribeResult(nobs=77, minmax=(0.0, 0.4155844155844156), mean=0.05262270197335133, variance=0.00797550001285432, skewness=2.153317766837499, kurtosis=4.330648427971897)\n",
      "    Core targets vs sources Welch t test:  -0.642 p= 0.523\n",
      "    Kolmogorov-Smirnov Effect Size: 0.171\n",
      "    Core targets vs Other targets Welch t test:  3.527 p= 0.001\n",
      "    Kolmogorov-Smirnov Effect Size: 0.460\n"
     ]
    }
   ],
   "source": [
    "# each edge has a capacity and each edge receives a flow. \n",
    "# The amount of flow on an edge cannot exceed the capacity of the edge.\n",
    "# therefore, edges with high capacity will be more important for the flow.\n",
    "# here we test the hypothesis that edges towards cores have higher capacity\n",
    "# or that the sum of edges towards cores have a higher total capacity\n",
    "cell_total_capacity = {cid:list() for cid in ophys_cell_ids}\n",
    "edges_sourcing = {cid:0 for cid in ophys_cell_ids}\n",
    "edges_targeting = {cid:0 for cid in ophys_cell_ids}\n",
    "\n",
    "for cluster_k,events_cellids in sorted_events_cidlist.items():\n",
    "    if cluster_k == 'gray':\n",
    "        continue\n",
    "\n",
    "    for vnt in events_cellids:\n",
    "        for posi,vidj in enumerate(vnt[1:]):\n",
    "            vidi = vnt[posi] # enumerate will go from 0\n",
    "            # print(vidi, vidj)\n",
    "\n",
    "            # check beginning and end are not the same\n",
    "            if dgraph.vs.find(ophys_cell_id=vidi).index == dgraph.vs.find(ophys_cell_id=vidj).index:\n",
    "                continue\n",
    "            # # check there is a path between the two\n",
    "            # if len(spinesgraph.get_all_shortest_paths(spinesgraph.vs.find(name=vidi).index, to=spinesgraph.vs.find(name=vidj).index, weights=None, mode='out'))>0:\n",
    "            #     continue\n",
    "\n",
    "            # Take the maximum flow between the previous and next vertices\n",
    "            mfres = dgraph.maxflow(dgraph.vs.find(ophys_cell_id=vidi).index, dgraph.vs.find(ophys_cell_id=vidj).index)\n",
    "            # print(mfres)\n",
    "            # returns a tuple containing the following:\n",
    "            # graph - the graph on which this flow is defined\n",
    "            # value - the value (capacity) of the maximum flow between the given vertices\n",
    "            # flow - the flow values on each edge. For directed graphs, this is simply a list where element i corresponds to the flow on edge i.\n",
    "            # cut - edge IDs in the minimal cut corresponding to the flow.\n",
    "            # partition - vertex IDs in the parts created after removing edges in the cut\n",
    "            # es - an edge selector restricted to the edges in the cut.\n",
    "\n",
    "            # we get a flow value for each edge contributing to the flow.\n",
    "            # source\n",
    "            mfres_value = mfres.value\n",
    "            if vidi in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                mfres_value /= len(core_indexes)\n",
    "            else:\n",
    "                mfres_value /= len(other_indexes)\n",
    "            cell_total_capacity[vidi].append(mfres_value)\n",
    "            # target\n",
    "            mfres_value = mfres.value\n",
    "            if vidj in np.array(ophys_cell_ids)[core_indexes]:\n",
    "                mfres_value /= len(core_indexes)\n",
    "            else:\n",
    "                mfres_value /= len(other_indexes)\n",
    "            cell_total_capacity[vidj].append(mfres_value)\n",
    "            \n",
    "            # Iterate over the edges identified by the flow.\n",
    "            # count the edges sourcing from cores, and those targeting cores. Which is more?\n",
    "            for edge in mfres.es:\n",
    "                sourceid = int(dgraph.vs[edge.source]['ophys_cell_id'])\n",
    "                targetid = int(dgraph.vs[edge.target]['ophys_cell_id'])\n",
    "                if sourceid in cell_total_capacity.keys():\n",
    "                    edges_sourcing[sourceid] +=1 # just count\n",
    "                if targetid in cell_total_capacity.keys():\n",
    "                    edges_targeting[targetid] +=1 # just count\n",
    "\n",
    "# Flow\n",
    "# print(cell_total_capacity)\n",
    "flowvalue_cores = []\n",
    "for cid in np.array(ophys_cell_ids)[core_indexes]:\n",
    "    flowvalue_cores.extend(cell_total_capacity[cid])\n",
    "flowvalue_others = []\n",
    "for cid in np.array(ophys_cell_ids)[other_indexes]:\n",
    "    flowvalue_others.extend(cell_total_capacity[cid])\n",
    "\n",
    "# description\n",
    "print(\"    Flow cores: \"+str(stats.describe(flowvalue_cores)) )\n",
    "print(\"    Flow others: \"+str(stats.describe(flowvalue_others)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowvalue_cores, flowvalue_others, equal_var=False))\n",
    "d,_ = stats.ks_2samp(flowvalue_cores, flowvalue_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(flowvalue_cores))\n",
    "plt.scatter(xs, flowvalue_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(flowvalue_others))\n",
    "plt.scatter(xs, flowvalue_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([flowvalue_cores,flowvalue_others], widths=0.15, showextrema=False, showmeans=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmeans'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized flow value')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(flowvalue_cores)), \"other\\n(n={:d})\".format(len(flowvalue_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_flowvalue.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n",
    "print()\n",
    "# Cuts\n",
    "# print(edges_sourcing)\n",
    "# print(edges_targeting)\n",
    "flowcuts_core_sources = []\n",
    "flowcuts_core_targets = []\n",
    "for cid in np.array(ophys_cell_ids)[core_indexes]:\n",
    "    flowcuts_core_sources.append(edges_sourcing[cid]/len(core_indexes))\n",
    "    flowcuts_core_targets.append(edges_targeting[cid]/len(core_indexes))\n",
    "flowcuts_other_sources = []\n",
    "flowcuts_other_targets = []\n",
    "for cid in np.array(ophys_cell_ids)[other_indexes]:\n",
    "    flowcuts_other_sources.append(edges_sourcing[cid]/len(other_indexes))\n",
    "    flowcuts_other_targets.append(edges_targeting[cid]/len(other_indexes))\n",
    "\n",
    "# description\n",
    "print(\"    Cut edges sourcing from cores: \"+str(stats.describe(flowcuts_core_sources)) )\n",
    "print(\"    Cut edges targeting cores: \"+str(stats.describe(flowcuts_core_targets)) )\n",
    "print(\"    Cut edges sourcing from others: \"+str(stats.describe(flowcuts_other_sources)) )\n",
    "print(\"    Cut edges targeting others: \"+str(stats.describe(flowcuts_other_targets)) )\n",
    "# significativity\n",
    "print(\"    Core targets vs sources Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowcuts_core_targets, flowcuts_core_sources, equal_var=False))\n",
    "d,_ = stats.ks_2samp(flowcuts_core_targets, flowcuts_core_sources) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "print(\"    Core targets vs Other targets Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(flowcuts_core_targets, flowcuts_other_targets, equal_var=False))\n",
    "d,_ = stats.ks_2samp(flowcuts_core_targets, flowcuts_other_targets) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(flowcuts_core_sources))\n",
    "plt.scatter(xs, flowcuts_core_sources, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(flowcuts_core_targets))\n",
    "plt.scatter(xs, flowcuts_core_targets, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(3, 0.04, len(flowcuts_other_sources))\n",
    "plt.scatter(xs, flowcuts_other_sources, alpha=0.3, c='silver')\n",
    "xs = np.random.normal(4, 0.04, len(flowcuts_other_targets))\n",
    "plt.scatter(xs, flowcuts_other_targets, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([flowcuts_core_sources,flowcuts_core_targets,flowcuts_other_sources,flowcuts_other_targets], widths=0.15, showextrema=False, showmeans=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc in vp['bodies'][0:2]:\n",
    "    pc.set_facecolor('#228B224d')\n",
    "for pc in vp['bodies'][2:]:\n",
    "    pc.set_facecolor('#D3D3D34d')\n",
    "vp['cmeans'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized edges in the cut')\n",
    "plt.xticks([1, 2, 3, 4], [\"core as\\nsource\", \"core as\\ntarget\", \"other as\\nsource\", \"other as\\ntarget\"])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_cutvalue.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cores are targets of multiple paths\n",
    "\n",
    "If cores are more often than others part of the paths, it means that they might not be central by virtue of their degree, but by how many event trajectory path (not just any path as in the betweenness, or hubness) pass through them.     \n",
    "More in detail, cores are important because they are more often the target of cut flow edges. The **pagerank** - where a node rank is proportional to the total rank of the other nodes pointing to it - is a way to measure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... PageRank centrality\n",
      "    cores: DescribeResult(nobs=35, minmax=(0.006725200919659086, 0.013983399707699865), mean=0.008086067919655509, variance=2.2740236820600384e-06, skewness=2.139231064286641, kurtosis=5.142654286263989)\n",
      "    others: DescribeResult(nobs=77, minmax=(7.176666085246508e-06, 0.008494477755171185), mean=0.0011266826080292174, variance=1.4417684587524277e-06, skewness=3.5408949693202096, kurtosis=17.3788598833623)\n",
      "    Kruskal-Wallis test:  68.815 p= 0.000\n",
      "    Kolmogorov-Smirnov Effect Size: 0.987\n"
     ]
    }
   ],
   "source": [
    "print('... PageRank centrality')\n",
    "pagerank_cores = np.array(dgraph.personalized_pagerank(vertices=core_indexes, directed=True, damping=0.85, reset=\"is_core\"))\n",
    "pagerank_others = np.array(dgraph.personalized_pagerank(vertices=other_indexes, directed=True, damping=0.85, reset=\"is_core\"))\n",
    "# description\n",
    "print(\"    cores: \"+str(stats.describe(pagerank_cores)) )\n",
    "print(\"    others: \"+str(stats.describe(pagerank_others)) )\n",
    "# significativity\n",
    "print(\"    Kruskal-Wallis test:  %.3f p= %.3f\" % stats.kruskal(pagerank_cores, pagerank_others))\n",
    "d,_ = stats.ks_2samp(pagerank_cores, pagerank_others) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(pagerank_cores))\n",
    "plt.scatter(xs, pagerank_cores, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(pagerank_others))\n",
    "plt.scatter(xs, pagerank_others, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([pagerank_cores,pagerank_others], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('PageRank')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(pagerank_cores)), \"other\\n(n={:d})\".format(len(pagerank_others))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_pagerank.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between structural and dynamical cores\n",
    "\n",
    "### Hierarchical modularity\n",
    "The log-log linear relationship between Clustering coefficient and degree is plotted in the file `hierarchical modularity.png` (and .svg) produced by the `structural_analysis.py` and already stored in the `results` folder.\n",
    "\n",
    "### Bow-tie structure of modules\n",
    "Local bow-ties analysis as in Fujita et al. 2019.\n",
    "Identify communities based on (multiple trials) random walks as information flows.    \n",
    "With very sparsely connected networks, as MICrONS, the library igraph finds only one module (see [here](https://stackoverflow.com/questions/20364939/community-detection-with-infomap-algorithm-producing-one-massive-module)). Using the same algorithm (with teleportation) from the InfoMap authors (see [here](https://mapequation.github.io/infomap/python/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    starting infomap analysis\n",
      "    found 44 modules with codelength: 7.8376  entropy: 1.4132\n",
      ". Found 2 levels with codelength 7.837594297\n",
      "\n",
      "=> Trial 1/1 finished in 0.014791879s with codelength 7.8375943\n",
      "\n",
      "\n",
      "================================================\n",
      "Summary after 1 trial\n",
      "================================================\n",
      "Best end modular solution in 2 levels:\n",
      "Per level number of modules:         [         44,           0] (sum: 44)\n",
      "Per level number of leaf nodes:      [          0,         334] (sum: 334)\n",
      "Per level average child degree:      [         44,     7.59091] (average: 11.829)\n",
      "Per level codelength for modules:    [3.506240903, 0.000000000] (sum: 3.506240903)\n",
      "Per level codelength for leaf nodes: [0.000000000, 4.331353394] (sum: 4.331353394)\n",
      "Per level codelength total:          [3.506240903, 4.331353394] (sum: 7.837594297)\n",
      "\n",
      "===================================================\n",
      "  Infomap ends at 2022-12-18 09:40:33\n",
      "  (Elapsed time: 0.02138768s)\n",
      "===================================================\n",
      "=======================================================\n",
      "  Infomap v2.6.1 starts at 2022-12-18 09:43:26\n",
      "  -> Input network: \n",
      "  -> No file output!\n",
      "  -> Configuration: no-self-links\n",
      "                    flow-model = directed\n",
      "                    seed = 2147483647\n",
      "                    prefer-modular-solution\n",
      "=======================================================\n",
      "  OpenMP 201511 detected with 12 threads...\n",
      "  -> Ordinary network input, using the Map Equation for first order network flows\n",
      "Calculating global network flow using flow model 'directed'... \n",
      "  -> Using unrecorded teleportation to links. \n",
      "  -> PageRank calculation done in 67 iterations.\n",
      "\n",
      "  => Sum node flow: 1, sum link flow: 1\n",
      "Build internal network with 334 nodes and 1734 links...\n",
      "  -> One-level codelength: 8.02217523\n",
      "\n",
      "================================================\n",
      "Trial 1/1 starting at 2022-12-18 09:43:26\n",
      "================================================\n",
      "Two-level compression: 1.6% 0.52% 0.195865874% 0.0061436123% 0.00300578023% \n",
      "Partitioned to codelength 3.5062409 + 4.33135339 = 7.837594297 in 44 (32 non-trivial) modules.\n",
      "Super-level compression: to codelength 7.837594297 in 44 top modules.\n",
      "\n",
      "Recursive sub-structure compression: 0%     bow-tie score: 0.13953488372093023\n",
      "    communities lens: DescribeResult(nobs=6, minmax=(14, 35), mean=19.833333333333332, variance=69.36666666666666, skewness=1.1484550230058315, kurtosis=-0.19983785010764077)\n"
     ]
    }
   ],
   "source": [
    "from infomap import Infomap # with teleportation to ensure no local solution\n",
    "im = Infomap(no_self_links=True, flow_model=\"directed\", seed=2**32-2, prefer_modular_solution=True)\n",
    "im.add_networkx_graph( dgraph.to_networkx() ) # infomap accepts only networkx format\n",
    "print(\"    starting infomap analysis\")\n",
    "im.run()\n",
    "print(f\"    found {im.num_top_modules} modules with codelength: {im.codelength:.4f}  entropy: {im.entropy_rate:.4f}\")\n",
    "previous_id = 1\n",
    "communities_tot = []\n",
    "communities_lens = []\n",
    "community = []\n",
    "structural_cores = []\n",
    "for node_id, module_id in sorted(im.modules, key=lambda x: x[1]):\n",
    "    if module_id>previous_id: # simple module handling\n",
    "        community_graph = dgraph.subgraph(community) # community contains the indexes in dgraph\n",
    "        imcommunity = Infomap(no_self_links=True, flow_model=\"directed\", seed=2**32-2, prefer_modular_solution=True, silent=True)\n",
    "        imcommunity.add_networkx_graph( community_graph.to_networkx() )\n",
    "        imcommunity.run()\n",
    "        # unspecific submodule specification\n",
    "        if imcommunity.num_non_trivial_top_modules > 2:\n",
    "            communities_lens.append(len(community))\n",
    "        communities_tot.append(len(community))\n",
    "        # get central community cores\n",
    "        community_cores = []\n",
    "        for imnode, immodules in imcommunity.get_multilevel_modules().items():\n",
    "            if immodules[0]==1: # only the center\n",
    "                community_cores.append(imnode)\n",
    "        structural_cores.append(community_cores)\n",
    "        # simple module handling\n",
    "        previous_id=module_id\n",
    "        community = []\n",
    "    # print(node_id, module_id)\n",
    "    community.append(node_id)\n",
    "print(\"    bow-tie score:\", len(communities_lens)/len(communities_tot))\n",
    "print(\"    communities lens:\",stats.describe(communities_lens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL) Random rewiring to test bow-tie score\n",
    "\n",
    "The EM data gives a certain bow-tie score.    \n",
    "To see whether the score can be improved or worsen by different connectivities, we can rewire at random (easy), and study their statistics.   \n",
    "**Uncomment the lines below to run the (rather long) rewiring tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewired_bowtie_score = {}\n",
    "# for rewireprob in np.linspace(0.01, 0.05, num=10):\n",
    "#     print(\"    \\nrewiring probability:\",rewireprob)\n",
    "#     rewired_bowtie_score[rewireprob] = []\n",
    "#\n",
    "#     for trial in range(0,10):\n",
    "#         rewired_graph = dgraph.copy()\n",
    "#\n",
    "#         rewired_graph.rewire_edges(prob=rewireprob, loops=False, multiple=True) # in place!\n",
    "#\n",
    "#         # Clustering Coefficient of only excitatory cells\n",
    "#         local_clustering_coefficients = np.array(rewired_graph.transitivity_local_undirected(vertices=None, mode=\"zero\"))\n",
    "#         # plot\n",
    "#         paramsfit = [2, 1.05] # 334 EM-only all proofread\n",
    "#         pfit = powerlaw(degrees, *paramsfit)\n",
    "#         fig = plt.figure()\n",
    "#         summer = mpcm.summer\n",
    "#         plt.scatter( degrees,local_clustering_coefficients, marker='o', facecolor='#111111', s=50, edgecolors='none', alpha=0.5) #\n",
    "#         plt.plot(degrees,pfit,c='k')\n",
    "#         plt.yscale('log')\n",
    "#         plt.xscale('log')\n",
    "#         ax = plt.gca()\n",
    "#         ax.spines['top'].set_visible(False)\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         plt.ylabel('LCC')\n",
    "#         plt.xlabel('degree')\n",
    "#         plt.tick_params(axis='both', bottom='on', top='on', left='off', right='off')\n",
    "#         plt.tight_layout()\n",
    "#         #fig.savefig(exp_path+'/results/rewiring/hierarchical_modularity'+str(rewireprob)+str(trial)+'.png', transparent=True, dpi=900)\n",
    "#         fig.savefig(exp_path+'/results/rewiring/hierarchical_modularity'+str(rewireprob)+\"_\"+str(trial)+'.svg', transparent=True)\n",
    "#         plt.close()\n",
    "#         fig.clf()\n",
    "#\n",
    "#         # Local bow-ties analysis as in FujitaKichikawaFujiwaraSoumaIyetomi2019\n",
    "#         from infomap import Infomap\n",
    "#         im = Infomap(silent=True, no_self_links=True, flow_model=\"directed\", seed=2**32-1, core_loop_limit=10, prefer_modular_solution=True, inner_parallelization=True, num_trials=10)\n",
    "#         im.add_networkx_graph( rewired_graph.to_networkx() ) # infomap accepts only networkx format\n",
    "#         im.run()\n",
    "#         previous_id = 1\n",
    "#         communities_tot = []\n",
    "#         communities_lens = []\n",
    "#         community = []\n",
    "#         for node_id, module_id in sorted(im.modules, key=lambda x: x[1]):\n",
    "#             if module_id>previous_id: # simple module handling\n",
    "#                 community_graph = rewired_graph.subgraph(community) # community contains the indexes in rewired_graph\n",
    "#                 imcommunity = Infomap(no_self_links=True, flow_model=\"directed\", seed=2**32-1, core_loop_limit=10, prefer_modular_solution=True, silent=True, num_trials=10)\n",
    "#                 imcommunity.add_networkx_graph( community_graph.to_networkx() )\n",
    "#                 imcommunity.run()\n",
    "#                 if imcommunity.num_non_trivial_top_modules > 2:\n",
    "#                     communities_lens.append(len(community))\n",
    "#                 communities_tot.append(len(community))\n",
    "#                 previous_id=module_id\n",
    "#                 community = []\n",
    "#             community.append(node_id)\n",
    "#         if len(communities_tot)<5:\n",
    "#             continue\n",
    "#         bowtie_score = len(communities_lens)/len(communities_tot)\n",
    "#         print(\"    trial:\", trial, \"score:\",bowtie_score)\n",
    "#         rewired_bowtie_score[rewireprob].append( bowtie_score )\n",
    "#\n",
    "# for rwiredk, rewiredv in rewired_bowtie_score.items():\n",
    "#     print(\"dgraph rewired with prob:\",rwiredk)\n",
    "#     print(\"    bow-tie score avg:\", stats.describe(rewiredv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap between structural and dynamical clusters\n",
    "\n",
    "Check the consistency of our hypothesis chain by looking at the overlap between dynamically-identified core neurons – those reliably participating in multiple clustered population events – and structurally-identified core neurons – those repetitively found in bow-tie modules.    \n",
    "(it requires the file with dynamical cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading clusters\n",
      "... loaded: (11,)\n",
      "    # cores: 35\n",
      "    # non-cores: 77\n",
      "... overlap between structural cores and dynamical cores\n",
      "0.0 0/2\n",
      "0.375 3/8\n",
      "0.14285714285714285 1/7\n",
      "0.25 1/4\n",
      "0.6666666666666666 2/3\n",
      "0.25 1/4\n",
      "0.2 1/5\n",
      "0.2 1/5\n",
      "0.0 0/2\n",
      "0.0 0/7\n",
      "0.25 1/4\n",
      "DescribeResult(nobs=11, minmax=(0.0, 0.6666666666666666), mean=0.21222943722943724, variance=0.03775229334157905, skewness=0.9909598007383054, kurtosis=0.8177289227244207)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./results/clusters_cores.npy'):\n",
    "    clusters_cores = np.load('./results/clusters_cores.npy', allow_pickle=True)\n",
    "    print(\"... loaded:\", clusters_cores.shape)\n",
    "    # print(clusters_cores)\n",
    "    # print(clusters_cores.shape)\n",
    "\n",
    "    core_indexes = []\n",
    "    other_indexes = []\n",
    "    clusters_cores_indexes = []\n",
    "    for dyn_core in clusters_cores:\n",
    "        core_indexes.extend( [ophys_cell_ids.index(strid) for strid in dyn_core] )\n",
    "        clusters_cores_indexes.append( [ophys_cell_ids.index(strid) for strid in dyn_core] )\n",
    "    core_indexes = np.unique(core_indexes)\n",
    "    print(\"    # cores:\",len(core_indexes))\n",
    "    other_indexes = [i for i in range(len(ophys_cell_ids)) if i not in core_indexes]\n",
    "    print(\"    # non-cores:\",len(other_indexes))\n",
    "\n",
    "    print(\"... overlap between structural cores and dynamical cores\")\n",
    "    # for each set of dynamical cores from a cluster\n",
    "    # we compare it with all the sets of structral core nodes identified by the InfoMap (and take the max to avoid duplicates)\n",
    "    overlapSD = {}\n",
    "    overlap_ratio = []\n",
    "    ccs_len = []\n",
    "    scs_len = []\n",
    "    for iccs,ccs in enumerate(clusters_cores_indexes):\n",
    "        ccs_len.append(len(ccs))\n",
    "        kccs = \"{}_{}_\".format(iccs,len(ccs))\n",
    "        overlapSD[kccs+\"lens\"] = []\n",
    "        overlapSD[kccs+\"ratio\"] = []\n",
    "        for scs in structural_cores:\n",
    "            scs_len.append(len(scs))\n",
    "            overlapSD[kccs+\"lens\"].append( \"{}/{}\".format( len(set(ccs)&set(scs)), len(ccs) ) )\n",
    "            overlapSD[kccs+\"ratio\"].append( len(set(ccs)&set(scs))/len(ccs) )\n",
    "        # print(len(scs),len(ccs))\n",
    "        # choose which structural_cores better matches\n",
    "        index_max = max(range(len(overlapSD[kccs+\"ratio\"])), key=overlapSD[kccs+\"ratio\"].__getitem__)\n",
    "        print(overlapSD[kccs+\"ratio\"][index_max], overlapSD[kccs+\"lens\"][index_max])\n",
    "        overlap_ratio.append(overlapSD[kccs+\"ratio\"][index_max])\n",
    "    print(stats.describe(overlap_ratio))\n",
    "\n",
    "    # plot all ratio\n",
    "    fig, ax = plt.subplots()\n",
    "    xs = np.random.normal(1, 0.04, len(overlap_ratio))\n",
    "    plt.scatter(xs, overlap_ratio, alpha=0.3, c='gray', edgecolors='none')\n",
    "    vp = ax.violinplot([overlap_ratio], widths=0.15, showextrema=False, showmeans=True)\n",
    "    for pc in vp['bodies']:\n",
    "        pc.set_edgecolor('black')\n",
    "    for pc,cb in zip(vp['bodies'],['black']):\n",
    "        pc.set_facecolor(cb)\n",
    "    vp['cmeans'].set_color('orange')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.ylim([0,0.7])\n",
    "    plt.ylabel('Overlap ratio')\n",
    "    fig.set_figwidth(1.5)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('./results/dynamical_structural_cores.svg', transparent=True)\n",
    "    plt.close()\n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Supplementary figure 3\n",
    "   \n",
    "To have keep cores within the attractor framework, cores activity could be sustained by indirect synaptic feedback, through highly connected secondary paths.   \n",
    "To back up the attractor idea, one would expect that core neurons would have shorter paths or cycles, compared to others. \n",
    "\n",
    "### Shortest paths of cores and others (panel S3A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... number of paths in a complete graph of the same size: 4.317298994652368e+178\n",
      "... number of shortest paths between cores\n",
      "    cores shortest paths: DescribeResult(nobs=110, minmax=(0, 5), mean=0.9727272727272728, variance=2.485487906588825, skewness=1.131712439974105, kurtosis=-0.412811249534613)\n",
      "    others shortest paths: DescribeResult(nobs=10100, minmax=(0, 8), mean=0.6076237623762376, variance=2.021983658807508, skewness=2.163248016247377, kurtosis=3.4165268418042007)\n",
      "    equal variances? LeveneResult(statistic=7.156175120461955, pvalue=0.007482545093017935)\n",
      "    Welch t test:  2.418 p= 0.017\n",
      "    Kolmogorov-Smirnov Effect Size: 0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_582/2252254418.py:7: RuntimeWarning: Couldn't reach some vertices at src/paths/unweighted.c:368\n",
      "  shrtpth = dgraph.get_shortest_paths(coreidx, to=othercores, weights=None, mode='out', output='vpath')\n",
      "/tmp/ipykernel_582/2252254418.py:14: RuntimeWarning: Couldn't reach some vertices at src/paths/unweighted.c:368\n",
      "  shrtpth = dgraph.get_shortest_paths(otheridx, to=otherothers, weights=None, mode='out', output='vpath')\n"
     ]
    }
   ],
   "source": [
    "print(\"... number of paths in a complete graph of the same size:\", (np.math.factorial(112-2)*np.e))\n",
    "print('... number of shortest paths between cores')\n",
    "core_shortestpaths = []\n",
    "for coreidx in core_indexes:\n",
    "    othercores = list(core_indexes)\n",
    "    othercores.remove(coreidx)\n",
    "    shrtpth = dgraph.get_shortest_paths(coreidx, to=othercores, weights=None, mode='out', output='vpath')\n",
    "    for strp in shrtpth:\n",
    "        core_shortestpaths.append(len(strp))\n",
    "other_shortestpaths = []\n",
    "for otheridx in other_indexes:\n",
    "    otherothers = list(other_indexes)\n",
    "    otherothers.remove(otheridx)\n",
    "    shrtpth = dgraph.get_shortest_paths(otheridx, to=otherothers, weights=None, mode='out', output='vpath')\n",
    "    for strp in shrtpth:\n",
    "        other_shortestpaths.append(len(strp))\n",
    "print(\"    cores shortest paths: \"+str(stats.describe(core_shortestpaths)) )\n",
    "print(\"    others shortest paths: \"+str(stats.describe(other_shortestpaths)) )\n",
    "print(\"    equal variances? \"+str(stats.levene(core_shortestpaths, other_shortestpaths)) )\n",
    "# significativity\n",
    "print(\"    Welch t test:  %.3f p= %.3f\" % stats.ttest_ind(core_shortestpaths, other_shortestpaths, equal_var=False))\n",
    "d,_ = stats.ks_2samp(core_shortestpaths, other_shortestpaths) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core_shortestpaths))\n",
    "plt.scatter(xs, core_shortestpaths, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(other_shortestpaths))\n",
    "plt.scatter(xs, other_shortestpaths, alpha=0.3, c='silver')\n",
    "vp = ax.violinplot([core_shortestpaths,other_shortestpaths], widths=0.15, showextrema=False, showmedians=True)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_edgecolor('black')\n",
    "for pc,cb in zip(vp['bodies'],['#228B224d','#D3D3D34d']):\n",
    "    pc.set_facecolor(cb)\n",
    "vp['cmedians'].set_color('orange')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Shortest path length')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(core_shortestpaths)), \"other\\n(n={:d})\".format(len(other_shortestpaths))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_shortestpath.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycles between cores or others (panel S3B)\n",
    "\n",
    "Cycles are built starting from a core (or other) and iterating neighbors of different lenghts, where the last vertex is the starting one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... cycles\n"
     ]
    }
   ],
   "source": [
    "print('... cycles')\n",
    "# breadth first search of paths and unique cycles\n",
    "def get_cycles(adj, paths, maxlen):\n",
    "    # tracking the actual path length:\n",
    "    maxlen -= 1\n",
    "    nxt_paths = []\n",
    "    # iterating over all paths:\n",
    "    for path in paths['paths']:\n",
    "        # iterating neighbors of the last vertex in the path:\n",
    "        for nxt in adj[path[-1]]:\n",
    "            # attaching the next vertex to the path:\n",
    "            nxt_path = path + [nxt]\n",
    "            if path[0] == nxt and min(path) == nxt:\n",
    "                # the next vertex is the starting vertex, we found a cycle\n",
    "                # we keep the cycle only if the starting vertex has the\n",
    "                # lowest vertex id, to avoid having the same cycles\n",
    "                # more than once\n",
    "                paths['cycles'].append(nxt_path)\n",
    "                # if you don't need the starting vertex\n",
    "                # included at the end:\n",
    "                # paths$cycles <- c(paths$cycles, list(path))\n",
    "            elif nxt not in path:\n",
    "                # keep the path only if we don't create\n",
    "                # an internal cycle in the path\n",
    "                nxt_paths.append(nxt_path)\n",
    "    # paths grown by one step:\n",
    "    paths['paths'] = nxt_paths\n",
    "    if maxlen == 0:\n",
    "        # the final return when maximum search length reached\n",
    "        return paths\n",
    "    else:\n",
    "        # recursive return, to grow paths further\n",
    "        return get_cycles(adj, paths, maxlen)\n",
    "# Comparison of core based cycles vs other based cycles\n",
    "maxlen = 10 # the maximum length to limit computation time\n",
    "# creating an adjacency list\n",
    "adj = [[n.index for n in v.neighbors()] for v in dgraph.vs]\n",
    "# recursive search of cycles\n",
    "# for each core vertex as candidate starting point\n",
    "core_cycles = []\n",
    "for start in core_indexes:\n",
    "    core_cycles += get_cycles(adj,{'paths': [[start]], 'cycles': []}, maxlen)['cycles']\n",
    "print(\"    # core-based cycles:\", len(core_cycles) )\n",
    "# count the length of loops involving 1 core\n",
    "core_cycles_lens = [len(cycle) for cycle in core_cycles]\n",
    "print(\"    core-based cycles length: \"+str(stats.describe(core_cycles_lens)) )\n",
    "\n",
    "other_cycles = []\n",
    "for start in other_indexes:\n",
    "    other_cycles += get_cycles(adj,{'paths': [[start]], 'cycles': []}, maxlen)['cycles']\n",
    "print(\"    # other-based cycles:\", len(other_cycles) )\n",
    "# count the length of loops involving 1 core\n",
    "other_cycles_lens = [len(cycle) for cycle in other_cycles]\n",
    "print(\"    other-based cycles length: \"+str(stats.describe(other_cycles_lens)) )\n",
    "\n",
    "d,_ = stats.ks_2samp(core_cycles_lens, other_cycles_lens) # non-parametric measure of effect size [0,1]\n",
    "print('    Kolmogorov-Smirnov Effect Size: %.3f' % d)\n",
    "# all cycles by type\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.random.normal(1, 0.04, len(core_cycles_lens))\n",
    "plt.scatter(xs, core_cycles_lens, alpha=0.3, c='forestgreen')\n",
    "xs = np.random.normal(2, 0.04, len(other_cycles_lens))\n",
    "plt.scatter(xs, other_cycles_lens, alpha=0.3, c='silver')\n",
    "bp = ax.boxplot([core_cycles_lens,other_cycles_lens], notch=0, sym='', showcaps=False, zorder=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Cycles length')\n",
    "plt.xticks([1, 2], [\"core\\n(n={:d})\".format(len(core_cycles_lens)), \"other\\n(n={:d})\".format(len(other_cycles_lens))])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_cyclelens.png', transparent=True, dpi=1500)\n",
    "plt.close()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Do the cores of each cluster form more cliques than others? (Panel S3C)\n",
    "\n",
    "If the cores of each cluster are pattern completion units, they should participate in more cliques (set of vertices where an edge is present between any two of them) than other non-core neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_582/1704612098.py:1: RuntimeWarning: Edge directions are ignored for clique calculations at src/cliques/cliquer_wrapper.c:57\n",
      "  cliques = dgraph.cliques(min=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 24), (24, 30), (5, 21), (24, 30), (24, 30), (5, 21), (5, 21), (5, 21), (5, 21), (21, 24), (5, 21), (24, 30), (21, 24), (21, 24), (5, 21), (21, 24), (24, 30), (21, 24), (24, 30), (5, 21), (21, 24), (21, 24), (5, 21)]\n",
      "    cliques made by cores: 2.090909090909091\n",
      "    cliques made by others: 2.8613861386138613\n"
     ]
    }
   ],
   "source": [
    "cliques = dgraph.cliques(min=2)\n",
    "\n",
    "cliques_cores = []\n",
    "cliques_others = []\n",
    "\n",
    "for cluster_cids in clustered_spectrums:\n",
    "    cluster_core_indices = []\n",
    "    # we take the index of the cell participating in this cluster\n",
    "    cluster_indices = [ophys_cell_ids.index(strid) for strid in cluster_cids]\n",
    "    # we take the cores of this cluster\n",
    "    cluster_core_indices = list(set(core_indexes).intersection(cluster_indices))\n",
    "    cluster_other_indices = list(set(other_indexes).intersection(cluster_indices))\n",
    "    # we take the edges between the cores\n",
    "    for clique in cliques:\n",
    "        if set(clique).issubset(cluster_core_indices):\n",
    "            cliques_cores.append(clique)\n",
    "        if set(clique).issubset(cluster_other_indices):\n",
    "            cliques_others.append(clique)\n",
    "print(cliques_cores)\n",
    "cores_cliques_count = len(cliques_cores)/len(core_indexes)\n",
    "others_cliques_count = len(cliques_others)/len(other_indexes)\n",
    "\n",
    "print(\"    cliques made by cores:\",cores_cliques_count)\n",
    "print(\"    cliques made by others:\",others_cliques_count)\n",
    "\n",
    "# print(core_edges)\n",
    "x = np.array([\"cores\", \"others\"])\n",
    "y = np.array([cores_cliques_count, others_cliques_count])\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x, y, color=['forestgreen','silver'])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.ylabel('Normalized count of cliques')\n",
    "plt.xticks([0, 1], [\"core\\n(n={:.3f})\".format(cores_cliques_count), \"other\\n(n={:.3f})\".format(others_cliques_count)])\n",
    "fig.savefig(exp_path+'/results/global_cores_others_cliques.svg', transparent=True)\n",
    "plt.close()\n",
    "fig.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters of events are not reproducible trajectories of the population dynamics\n",
    "\n",
    "Clusters of population events are found by correlating population vectors, which only retain the cell IDs while ignoring the time of firing.    \n",
    "We can consider also time.\n",
    "\n",
    "Each recorded frame (~67ms) is an instantaneous population state defined by all its cells (112 of them are known for their firing, the others are unkown).    \n",
    "A sequence of population states is a trajectory in the population dynamical state space.    \n",
    "In this space, clusters of reproducible population events are represented by reproducible trajectories. \n",
    "\n",
    "We can compare the event trajectories visited within a cluster by comparing their patterns.    \n",
    "Events are made by cells firing (often multiple times) during the event interval, so each sequence is a 2D submatrix of the population rasterplot. This gives a measure of trajectory reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... sequence internal consistency\n",
      "\n",
      "    common trajectory pattern with n cells: 6  and m intervals: 674\n",
      "    correlation across all trajectories: 0.310±0.02\n",
      "... searching for repeating sequences in the ordered firing of cell IDs\n",
      "    (92, 93) 2\n",
      "[ 5  8 21 24 30 36 47 50 81 92 93]\n",
      "\n",
      "    common trajectory pattern with n cells: 9  and m intervals: 809\n",
      "    correlation across all trajectories: 0.172±0.04\n",
      "... searching for repeating sequences in the ordered firing of cell IDs\n",
      "    (47, 81) 2\n",
      "[ 5  8 21 24 30 36 47 50 81 92 93]\n",
      "\n",
      "    common trajectory pattern with n cells: 14  and m intervals: 877\n",
      "    correlation across all trajectories: 0.216±0.03\n",
      "... searching for repeating sequences in the ordered firing of cell IDs\n",
      "    (24, 5) 2\n",
      "[ 5  8 21 24 30 36 47 50 81 92 93]\n",
      "\n",
      "    common trajectory pattern with n cells: 7  and m intervals: 337\n",
      "    correlation across all trajectories: 0.271±0.03\n",
      "... searching for repeating sequences in the ordered firing of cell IDs\n",
      "    (5, 21) 2\n",
      "[ 5  8 21 24 30 36 47 50 81 92 93]\n",
      "    (21, 24) 2\n",
      "[ 5  8 21 24 30 36 47 50 81 92 93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# print(cluster_events_spiketrains) # already expressed in integer (ms)\n",
    "\n",
    "print(\"... sequence internal consistency\")\n",
    "\n",
    "# cycle over clusters\n",
    "for cluster_k, events_cellindexes in sorted_events_indexes.items():\n",
    "    if cluster_k == 'gray':\n",
    "        continue\n",
    "    print()\n",
    "\n",
    "    # We want to compare the trajectories of this cluster.\n",
    "    # Trajectories should have same shape. We will subract them to get the difference (/num of events).\n",
    "    \n",
    "    # Finding the common-shape trajectory\n",
    "    # n is the maximal number of cells participating to events in this cluster\n",
    "    maxcells = max(events_cellindexes, key = lambda i: len(i))\n",
    "    # m is the largest interval between the first min spiketrain and the last max spiketrain of all events in the cluster\n",
    "    events_spiketrains = cluster_events_spiketrains[cluster_k]\n",
    "    # print(events_spiketrains)\n",
    "    maxinterval = 0 # \n",
    "    for evt_spktrains in events_spiketrains:\n",
    "        mint = np.amin([x for xs in evt_spktrains for x in xs]) # for cases of just one spiketinme in list\n",
    "        if isinstance(mint, list): mint = mint[0] # for cases of list\n",
    "        maxt = np.amax([x for xs in evt_spktrains for x in xs])\n",
    "        if isinstance(maxt, list): maxt = maxt[-1]\n",
    "        if maxt-mint > maxinterval:\n",
    "            maxinterval = maxt-mint\n",
    "    print(\"    common trajectory pattern with n cells:\", len(maxcells), \" and m intervals:\", maxinterval)\n",
    "    \n",
    "    # cluster trajectories, one per event, all same shape\n",
    "    cluster_trajectories = []\n",
    "    for evt_indexes,evt_spktrains in zip(events_cellindexes,events_spiketrains):\n",
    "        # create empty trajectory of shape n cell, m interval\n",
    "        trajectory = np.zeros((len(maxcells),maxinterval+1))\n",
    "        mint = np.amin([x for xs in evt_spktrains for x in xs]) # take local mintime to find the trajectory m index\n",
    "        if isinstance(mint, list): mint = mint[0] # for cases of just one spiketinme in list\n",
    "        for ncell,spktrain in enumerate(evt_spktrains):\n",
    "            trajectory[ncell][spktrain-mint] = 1\n",
    "        cluster_trajectories.append(trajectory)\n",
    "    \n",
    "    # correlation between trajectories\n",
    "    # very simple (probably too much) measure of trajectory correspondence\n",
    "    trajR = []\n",
    "    for itr,itrajectory in enumerate(cluster_trajectories):\n",
    "        for jtr,jtrajectory in enumerate(cluster_trajectories):\n",
    "            if itr!=jtr:\n",
    "                trajR.append( np.nanmean(np.corrcoef(itrajectory,jtrajectory)) )\n",
    "    print(\"    correlation across all trajectories: {:1.3f}±{:1.2f}\".format(np.mean(trajR),np.std(trajR)))\n",
    "\n",
    "    print(\"... searching for repeating sequences in the ordered firing of cell IDs\")\n",
    "    size = 2\n",
    "    # size = 3\n",
    "    cluster_sequences = [x for xs in events_cellindexes for x in xs]\n",
    "    # print(cluster_sequences)\n",
    "    windows = [\n",
    "        tuple(window)\n",
    "        for window in more_itertools.windowed(cluster_sequences, size)\n",
    "    ]\n",
    "    counter = collections.Counter(windows)\n",
    "    for window, count in counter.items():\n",
    "        if count > 1:\n",
    "            print(\"   \",window, count)\n",
    "            print(core_indexes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
